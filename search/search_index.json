{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Headroom","text":"<p>The Context Optimization Layer for LLM Applications</p> <p>Tool outputs are 70-95% redundant. Headroom compresses that away\u2014without losing information.</p>"},{"location":"#quick-install","title":"Quick Install","text":"<pre><code>pip install headroom-ai[all]\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#option-1-proxy-zero-code-changes","title":"Option 1: Proxy (Zero Code Changes)","text":"<p>Start the proxy:</p> <pre><code>headroom proxy\n</code></pre> <p>Point your tools at it:</p> <pre><code>ANTHROPIC_BASE_URL=http://localhost:8787 claude\n</code></pre> <p>That's it. Your existing code works unchanged, with 40-90% fewer tokens.</p>"},{"location":"#option-2-python-sdk","title":"Option 2: Python SDK","text":"<pre><code>from headroom import Headroom\n\nhr = Headroom()\n\n# Compress tool output before sending to LLM\ncompressed = hr.compress(large_tool_output)\n\n# If LLM needs the full data, retrieve it\noriginal = hr.retrieve(compressed)\n</code></pre>"},{"location":"#why-headroom","title":"Why Headroom?","text":"Problem Solution Tool outputs bloat context with repetitive JSON Statistical compression removes redundancy Dynamic content breaks provider caching Cache alignment stabilizes prefixes Long conversations exceed context limits Intelligent scoring drops low-value messages Compressed data might be needed later CCR stores originals for on-demand retrieval"},{"location":"#results","title":"Results","text":"<p>100 log entries. One critical error buried at position 67.</p> Metric Baseline Headroom Input tokens 10,144 1,260 Correct answers 4/4 4/4 <p>87.6% fewer tokens. Same answer.</p> <p>The FATAL error was automatically preserved\u2014no configuration needed.</p>"},{"location":"#how-it-works","title":"How It Works","text":"<pre><code>Your App \u2192 Headroom \u2192 LLM Provider\n              \u2193\n         Compression\n         Caching\n         Retrieval\n</code></pre> <ol> <li>Intercepts context \u2014 Tool outputs, logs, search results</li> <li>Compresses intelligently \u2014 Keeps errors, outliers, boundaries</li> <li>Stores originals \u2014 Full data available if LLM requests it</li> <li>Aligns for caching \u2014 Provider caches actually hit</li> </ol>"},{"location":"#integrations","title":"Integrations","text":"LangChainAgnoAWS Bedrock <pre><code>from langchain_openai import ChatOpenAI\nfrom headroom.integrations import HeadroomChatModel\n\nllm = HeadroomChatModel(ChatOpenAI(model=\"gpt-4o\"))\nresponse = llm.invoke(\"Hello!\")\n</code></pre> <pre><code>from agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom headroom.integrations.agno import HeadroomAgnoModel\n\nmodel = HeadroomAgnoModel(OpenAIChat(id=\"gpt-4o\"))\nagent = Agent(model=model)\n</code></pre> <pre><code># Start proxy with Bedrock backend\nheadroom proxy --backend bedrock --region us-east-1\n\n# Point Claude Code at it\nANTHROPIC_API_KEY=\"sk-ant-dummy\" \\\nANTHROPIC_BASE_URL=http://localhost:8787 \\\nclaude\n</code></pre>"},{"location":"#features","title":"Features","text":"<p>Compression</p> <ul> <li>Statistical JSON array compression (no hardcoded rules)</li> <li>ML-based text compression via LLMLingua</li> <li>AST-aware code compression</li> <li>Image optimization (40-90% reduction)</li> </ul> <p>Context Management</p> <ul> <li>Intelligent message scoring and dropping</li> <li>Compress-Cache-Retrieve (CCR) for lossless compression</li> <li>Provider cache alignment for better hit rates</li> </ul> <p>Operations</p> <ul> <li>Prometheus metrics endpoint</li> <li>Request logging and cost tracking</li> <li>Budget limits and rate limiting</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart Guide \u2014 Get running in 5 minutes</li> <li>Proxy Documentation \u2014 Configure the optimization proxy</li> <li>Architecture \u2014 Deep dive into how it works</li> </ul>"},{"location":"#license","title":"License","text":"<p>Apache 2.0 \u2014 Free for commercial use.</p>"},{"location":"ARCHITECTURE/","title":"Headroom SDK: A Complete Explanation","text":""},{"location":"ARCHITECTURE/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart TB\n    subgraph Entry[\"Entry Points\"]\n        Proxy[\"Proxy Mode&lt;br/&gt;&lt;i&gt;Zero code changes&lt;/i&gt;\"]\n        SDK[\"SDK Mode&lt;br/&gt;&lt;i&gt;HeadroomClient&lt;/i&gt;\"]\n        Integrations[\"Integrations&lt;br/&gt;&lt;i&gt;LangChain / Agno&lt;/i&gt;\"]\n    end\n\n    subgraph Pipeline[\"Transform Pipeline\"]\n        direction TB\n\n        CA[\"Cache Aligner&lt;br/&gt;\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501&lt;br/&gt;Extracts dynamic content&lt;br/&gt;(dates, UUIDs, tokens)&lt;br/&gt;Stable prefix for caching\"]\n\n        SC[\"Smart Crusher&lt;br/&gt;\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501&lt;br/&gt;Analyzes tool outputs&lt;br/&gt;Keeps: first, last, errors, outliers&lt;br/&gt;70-95% reduction\"]\n\n        CM[\"Context Manager&lt;br/&gt;\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501&lt;br/&gt;Enforces token limits&lt;br/&gt;Scores by recency and relevance&lt;br/&gt;Fits context window\"]\n\n        CA --&gt; SC --&gt; CM\n    end\n\n    subgraph Cache[\"Provider Cache Optimization\"]\n        direction LR\n        Anthropic[\"Anthropic&lt;br/&gt;&lt;i&gt;cache_control blocks&lt;/i&gt;&lt;br/&gt;90% savings\"]\n        OpenAI[\"OpenAI&lt;br/&gt;&lt;i&gt;Prefix alignment&lt;/i&gt;&lt;br/&gt;50% savings\"]\n        Google[\"Google&lt;br/&gt;&lt;i&gt;CachedContent API&lt;/i&gt;&lt;br/&gt;75% savings\"]\n    end\n\n    subgraph CCR[\"CCR: Compress-Cache-Retrieve\"]\n        Store[(\"Compression&lt;br/&gt;Store\")]\n        Tool[\"Retrieve Tool&lt;br/&gt;&lt;i&gt;LLM requests original&lt;/i&gt;\"]\n        Store &lt;--&gt; Tool\n    end\n\n    LLM[\"LLM API&lt;br/&gt;&lt;i&gt;OpenAI / Anthropic / Google&lt;/i&gt;\"]\n\n    Entry --&gt; Pipeline\n    Pipeline --&gt; Cache\n    Cache --&gt; LLM\n    SC -.-&gt;|\"Stores original\"| Store\n    LLM -.-&gt;|\"If needed\"| Tool\n</code></pre>"},{"location":"ARCHITECTURE/#what-problem-does-headroom-solve","title":"What Problem Does Headroom Solve?","text":"<p>When you use AI models like GPT-4 or Claude, you pay for tokens - the pieces of text you send (input) and receive (output). The problem is:</p> <ol> <li>Tool outputs are HUGE: When an AI agent calls tools (search, database queries, APIs), the responses are often massive JSON blobs with thousands of tokens</li> <li>Most of that data is REDUNDANT: 60 metric data points showing <code>cpu: 45%</code> repeated, or 50 log entries with the same error message</li> <li>You're paying for waste: Every token costs money and adds latency</li> <li>Context windows fill up: Models have limits (128K tokens), and bloated tool outputs eat into your available space</li> </ol> <p>Headroom creates \"headroom\" - it intelligently compresses your input tokens so you have more room (and budget) for what matters.</p>"},{"location":"ARCHITECTURE/#how-headroom-works-the-big-picture","title":"How Headroom Works: The Big Picture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        YOUR APPLICATION                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      HEADROOM CLIENT                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502  \u2502   ANALYZE   \u2502\u2192 \u2502  TRANSFORM  \u2502\u2192 \u2502    CALL     \u2502             \u2502\n\u2502  \u2502  (Parser)   \u2502  \u2502  (Pipeline) \u2502  \u2502   (API)     \u2502             \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502         \u2502                \u2502                \u2502                     \u2502\n\u2502         \u25bc                \u25bc                \u25bc                     \u2502\n\u2502   Count tokens     Apply compressions   Send to OpenAI/Claude  \u2502\n\u2502   Detect waste     Preserve meaning     Log metrics            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    OPENAI / ANTHROPIC API                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ARCHITECTURE/#the-core-components-in-simple-terms","title":"The Core Components (In Simple Terms)","text":""},{"location":"ARCHITECTURE/#1-headroomclient-clientpy-the-wrapper","title":"1. HeadroomClient (<code>client.py</code>) - The Wrapper","text":"<p>This is what you interact with. It wraps your existing OpenAI or Anthropic client:</p> <pre><code># Before (normal OpenAI)\nclient = OpenAI(api_key=\"...\")\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=[...])\n\n# After (with Headroom)\nbase = OpenAI(api_key=\"...\")\nclient = HeadroomClient(original_client=base, provider=OpenAIProvider())\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=[...])\n</code></pre> <p>What it does: - Intercepts your API calls - Runs messages through the transform pipeline - Calls the real API with optimized messages - Logs metrics to a database - Returns the response unchanged</p> <p>Two modes: - <code>audit</code>: Just observe and log (no changes) - <code>optimize</code>: Apply transforms to reduce tokens</p>"},{"location":"ARCHITECTURE/#2-providers-providers-model-specific-knowledge","title":"2. Providers (<code>providers/</code>) - Model-Specific Knowledge","text":"<p>Different AI providers have different rules:</p> <pre><code>class OpenAIProvider:\n    # Knows GPT-4o has 128K context\n    # Knows how to count tokens (tiktoken)\n    # Knows pricing ($2.50 per million input tokens)\n\nclass AnthropicProvider:\n    # Knows Claude has 200K context\n    # Uses different tokenization (~4 chars per token)\n    # Different pricing structure\n</code></pre> <p>Why this matters: Token counting is model-specific. GPT-4 uses different tokenization than Claude. Headroom needs accurate counts to know how much to compress.</p>"},{"location":"ARCHITECTURE/#3-parser-parserpy-understanding-your-messages","title":"3. Parser (<code>parser.py</code>) - Understanding Your Messages","text":"<p>Before optimizing, Headroom needs to understand what's in your messages:</p> <pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are helpful...\"},\n    {\"role\": \"user\", \"content\": \"Search for X\"},\n    {\"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"role\": \"tool\", \"content\": \"{huge JSON}\"},\n]\n\n# Parser breaks this into \"blocks\":\nblocks = [\n    Block(kind=\"system\", tokens=50, ...),\n    Block(kind=\"user\", tokens=10, ...),\n    Block(kind=\"tool_call\", tokens=20, ...),\n    Block(kind=\"tool_result\", tokens=5000, ...),  # \u2190 This is the problem!\n]\n</code></pre> <p>It also detects waste signals: - Large JSON blobs (&gt;500 tokens) - HTML tags and comments - Base64 encoded data - Excessive whitespace</p>"},{"location":"ARCHITECTURE/#4-transforms-transforms-the-compression-magic","title":"4. Transforms (<code>transforms/</code>) - The Compression Magic","text":"<p>This is where the real work happens. Headroom has 4 transforms that run in sequence:</p>"},{"location":"ARCHITECTURE/#transform-1-cache-aligner","title":"Transform 1: Cache Aligner","text":"<p>Problem: LLM providers cache your prompts, but only if they're byte-identical. If your system prompt has today's date, every day is a cache miss.</p> <pre><code># Before:\n\"You are helpful. Current Date: 2024-12-15\"  # Changes daily = no cache\n\n# After:\n\"You are helpful.\"  # Static = cacheable\n\"[Context: Current Date: 2024-12-15]\"  # Dynamic part moved to end\n</code></pre> <p>How it works: 1. Find date patterns in system prompt 2. Extract them 3. Move to end of message 4. Now the PREFIX is stable \u2192 cache hits!</p>"},{"location":"ARCHITECTURE/#transform-2-tool-crusher-naive-disabled-by-default","title":"Transform 2: Tool Crusher (Naive) - DISABLED BY DEFAULT","text":"<p>This was our first approach - simple but limited:</p> <pre><code># Before: 60 items\n[{\"ts\": 1, \"cpu\": 45}, {\"ts\": 2, \"cpu\": 45}, ..., {\"ts\": 60, \"cpu\": 95}]\n\n# After: First 10 items only\n[{\"ts\": 1, \"cpu\": 45}, ..., {\"ts\": 10, \"cpu\": 45}, {\"__truncated\": 50}]\n</code></pre> <p>Problem: If the important data (CPU spike) is at position 45, it gets thrown away!</p>"},{"location":"ARCHITECTURE/#transform-3-smart-crusher-new-default","title":"Transform 3: Smart Crusher (NEW DEFAULT)","text":"<p>This is the intelligent approach using statistical analysis:</p> <pre><code># Analyzes the data first:\nanalysis = {\n    \"ts\": {\"type\": \"sequential\", \"unique_ratio\": 1.0},\n    \"host\": {\"type\": \"constant\", \"value\": \"prod-1\"},  # \u2190 Same everywhere!\n    \"cpu\": {\"variance\": 892, \"change_points\": [45]},  # \u2190 Spike detected!\n}\n\n# Smart compression:\n{\n    \"__headroom_constants\": {\"host\": \"prod-1\"},  # Factor out\n    \"__headroom_summary\": \"items 0-44: cpu stable at ~45\",  # Summarize boring part\n    \"data\": [\n        {\"ts\": 45, \"cpu\": 92},  # Keep the spike!\n        {\"ts\": 46, \"cpu\": 95},\n        ...\n    ]\n}\n</code></pre> <p>Strategies it uses: 1. TIME_SERIES: Detect variance spikes, keep change points 2. CLUSTER: Group similar log messages, keep 1-2 per cluster 3. TOP_N: For search results, keep highest scored 4. SMART_SAMPLE: Statistical sampling with constant extraction</p>"},{"location":"ARCHITECTURE/#transform-4-llmlingua-compressor-optional","title":"Transform 4: LLMLingua Compressor (Optional)","text":"<p>When to use: Maximum compression needed and latency is acceptable.</p> <pre><code># Opt-in ML-based compression using Microsoft's LLMLingua-2\n# BERT-based token classifier trained via GPT-4 distillation\n\n# Before: Long tool output text\n\"The function processUserData takes a user object and validates all fields...\"\n\n# After: Compressed while preserving semantic meaning\n\"function processUserData validates user fields...\"\n</code></pre> <p>Key characteristics: - Uses <code>microsoft/llmlingua-2-xlm-roberta-large-meetingbank</code> model - Auto-detects content type (code, JSON, text) for optimal compression rates - Stores original in CCR for retrieval if needed - Adds 50-200ms latency per request - Requires ~1GB RAM when loaded</p> <p>Proxy integration (opt-in): <pre><code>headroom proxy --llmlingua --llmlingua-device cuda\n</code></pre></p>"},{"location":"ARCHITECTURE/#transform-5-rolling-window","title":"Transform 5: Rolling Window","text":"<p>Problem: Even after compression, you might exceed the model's context limit.</p> <pre><code># Model limit: 128K tokens\n# Your messages: 150K tokens\n# Need to drop 22K tokens\n\n# Rolling Window drops OLDEST messages first:\n# - Keeps system prompt (always)\n# - Keeps last 2 turns (always)\n# - Drops old tool calls + their responses as atomic units\n</code></pre> <p>Safety rule: If we drop a tool CALL, we MUST drop its RESPONSE too (or vice versa). Otherwise the model sees orphaned data.</p>"},{"location":"ARCHITECTURE/#transform-6-intelligent-context-manager-advanced","title":"Transform 6: Intelligent Context Manager (Advanced)","text":"<p>Problem: Rolling Window drops by position (oldest first), but position doesn't equal importance.</p> <pre><code># Scenario: Error at turn 3, verbose success at turn 10\n# Rolling Window: Drops turn 3 error (oldest first)\n# Intelligent Context: Keeps turn 3 error (high TOIN error score)\n</code></pre> <p>The Solution: Multi-factor importance scoring using TOIN-learned patterns:</p> <pre><code># Message scores (all learned, no hardcodes):\nscores = {\n    \"recency\": 0.20,           # Exponential decay from end\n    \"semantic_similarity\": 0.20,  # Embedding similarity to recent context\n    \"toin_importance\": 0.25,   # TOIN retrieval_rate (high = important)\n    \"error_indicator\": 0.15,   # TOIN field_semantics.inferred_type\n    \"forward_reference\": 0.15, # Referenced by later messages\n    \"token_density\": 0.05,     # Unique tokens / total tokens\n}\n\n# Drops lowest-scored messages first\n# Preserves critical errors even if old\n</code></pre> <p>Key principle: No hardcoded patterns. Error detection uses TOIN's learned <code>field_semantics.inferred_type == \"error_indicator\"</code>, not keyword matching like \"error\" or \"fail\".</p> <p>TOIN + CCR Integration:</p> <p>IntelligentContext is a message-level compressor \u2014 just like SmartCrusher compresses items in an array, IntelligentContext \"compresses\" messages in a conversation. This means full CCR integration:</p> <pre><code># When messages are dropped:\n# 1. Store dropped messages in CCR for potential retrieval\nccr_ref = store.store(\n    original=json.dumps(dropped_messages),\n    compressed=\"[60 messages dropped]\",\n    tool_name=\"intelligent_context_drop\",\n)\n\n# 2. Record drop to TOIN for cross-user learning\ntoin.record_compression(\n    tool_signature=message_signature,  # Pattern of roles, tools, errors\n    original_count=len(dropped_messages),\n    compressed_count=1,  # The marker\n    strategy=\"intelligent_context_drop\",\n)\n\n# 3. Insert marker with CCR reference\nmarker = f\"[Earlier context compressed: 60 messages dropped. Retrieve: {ccr_ref}]\"\n</code></pre> <p>The feedback loop: - If users retrieve dropped messages via CCR, TOIN learns those patterns are important - Future drops of similar message patterns get higher importance scores - The system gets smarter across all users, not just within one session</p>"},{"location":"ARCHITECTURE/#5-storage-storage-metrics-database","title":"5. Storage (<code>storage/</code>) - Metrics Database","text":"<p>Every request is logged:</p> <pre><code>CREATE TABLE requests (\n    id TEXT PRIMARY KEY,\n    timestamp TEXT,\n    model TEXT,\n    mode TEXT,  -- audit or optimize\n    tokens_input_before INTEGER,  -- Before Headroom\n    tokens_input_after INTEGER,   -- After Headroom\n    tokens_saved INTEGER,         -- The win!\n    transforms_applied TEXT,      -- What we did\n    ...\n);\n</code></pre> <p>This lets you: - See how much you're saving - Generate reports - Track trends over time</p>"},{"location":"ARCHITECTURE/#the-data-flow-step-by-step","title":"The Data Flow (Step by Step)","text":"<p>Let's trace a real request:</p>"},{"location":"ARCHITECTURE/#step-1-you-call-the-api","title":"Step 1: You call the API","text":"<pre><code>response = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are an SRE. Date: 2024-12-15\"},\n        {\"role\": \"user\", \"content\": \"Check the metrics\"},\n        {\"role\": \"assistant\", \"tool_calls\": [...]},\n        {\"role\": \"tool\", \"content\": \"{60 metric points...}\"},  # 5000 tokens!\n        {\"role\": \"user\", \"content\": \"What's wrong?\"},\n    ],\n    headroom_mode=\"optimize\",\n)\n</code></pre>"},{"location":"ARCHITECTURE/#step-2-headroomclient-intercepts","title":"Step 2: HeadroomClient intercepts","text":"<pre><code># In client.py:\ndef _create(self, messages, ...):\n    # 1. Parse messages into blocks\n    blocks, breakdown, waste = parse_messages(messages, tokenizer)\n    # breakdown = {\"system\": 50, \"user\": 20, \"tool_result\": 5000, ...}\n\n    # 2. Count original tokens\n    tokens_before = 5100\n</code></pre>"},{"location":"ARCHITECTURE/#step-3-transform-pipeline-runs","title":"Step 3: Transform Pipeline runs","text":"<pre><code># In pipeline.py:\ndef apply(self, messages, ...):\n    # Transform 1: Cache Aligner\n    # - Extracts \"Date: 2024-12-15\" from system prompt\n    # - Moves to end\n\n    # Transform 2: Smart Crusher\n    # - Analyzes 60 metric points\n    # - Detects CPU spike at point 45\n    # - Compresses to 17 points (preserving spike)\n    # - Factors out constant \"host\" field\n\n    # Transform 3: LLMLingua (if enabled via --llmlingua)\n    # - ML-based compression on remaining long text\n    # - Auto-detects content type for optimal rate\n    # - Stores original in CCR for retrieval\n\n    # Transform 4: Rolling Window\n    # - Checks if we're under limit (we are)\n    # - No drops needed\n\n    return TransformResult(\n        messages=optimized,\n        tokens_before=5100,\n        tokens_after=1200,  # 76% reduction!\n        transforms=[\"cache_align\", \"smart_crush:1\"]\n    )\n</code></pre>"},{"location":"ARCHITECTURE/#step-4-call-real-api","title":"Step 4: Call real API","text":"<pre><code># In client.py:\nresponse = self._original.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=optimized_messages,  # Only 1200 tokens now!\n)\n</code></pre>"},{"location":"ARCHITECTURE/#step-5-log-metrics-and-return","title":"Step 5: Log metrics and return","text":"<pre><code># Save to database\nmetrics = RequestMetrics(\n    tokens_input_before=5100,\n    tokens_input_after=1200,\n    tokens_saved=3900,  # 76%!\n    ...\n)\nstorage.save(metrics)\n\nreturn response  # Unchanged from API\n</code></pre>"},{"location":"ARCHITECTURE/#the-smart-crusher-deep-dive","title":"The Smart Crusher Deep Dive","text":"<p>This is the most sophisticated part. Here's how it analyzes data:</p>"},{"location":"ARCHITECTURE/#field-analysis","title":"Field Analysis","text":"<pre><code>def analyze_field(key, items):\n    values = [item[key] for item in items]\n\n    return {\n        \"unique_ratio\": len(set(values)) / len(values),\n        # 0.0 = all same (constant)\n        # 1.0 = all different (unique IDs)\n\n        \"variance\": statistics.variance(values),  # For numbers\n        # Low = stable\n        # High = changing\n\n        \"change_points\": detect_spikes(values),\n        # Indices where value jumps significantly\n    }\n</code></pre>"},{"location":"ARCHITECTURE/#pattern-detection","title":"Pattern Detection","text":"<pre><code>def detect_pattern(field_stats):\n    # Has timestamp + numeric variance? \u2192 TIME_SERIES\n    if has_timestamp and has_numeric_variance:\n        return \"time_series\"\n\n    # Has message field + level field? \u2192 LOGS\n    if has_message_field and has_level_field:\n        return \"logs\"\n\n    # Has score/rank field? \u2192 SEARCH_RESULTS\n    if has_score_field:\n        return \"search_results\"\n\n    return \"generic\"\n</code></pre>"},{"location":"ARCHITECTURE/#compression-strategy","title":"Compression Strategy","text":"<pre><code>def compress(items, analysis):\n    if analysis.pattern == \"time_series\":\n        # Keep points around change points\n        # Summarize stable regions\n        return time_series_compress(items, analysis.change_points)\n\n    elif analysis.pattern == \"logs\":\n        # Cluster similar messages\n        # Keep 1-2 per cluster\n        return cluster_compress(items, analysis.clusters)\n\n    elif analysis.pattern == \"search_results\":\n        # Sort by score\n        # Keep top N\n        return top_n_compress(items, analysis.score_field)\n</code></pre>"},{"location":"ARCHITECTURE/#ccr-architecture-compress-cache-retrieve","title":"CCR Architecture: Compress-Cache-Retrieve","text":""},{"location":"ARCHITECTURE/#the-key-insight","title":"The Key Insight","text":"<p>\"Prefer raw &gt; Compaction &gt; Summarization only when compaction no longer yields enough space. Compaction (Reversible) strips out information that is redundant because it exists in the environment\u2014if the agent needs to read the data later, it can use a tool to retrieve it.\" \u2014 Phil Schmid, Context Engineering</p> <p>The problem with traditional compression: If we guess wrong about what's important, we've permanently lost data. The LLM might need something we threw away.</p> <p>CCR's solution: Make compression reversible. When SmartCrusher compresses, the original data is cached. If the LLM needs more, it can retrieve instantly.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TOOL OUTPUT (1000 items)                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  HEADROOM CCR LAYER                                               \u2502\n\u2502                                                                   \u2502\n\u2502  1. COMPRESS: Keep 20 items (errors, anomalies, relevant)        \u2502\n\u2502  2. CACHE: Store full 1000 items in fast local cache             \u2502\n\u2502  3. INJECT: Add retrieval capability to LLM context              \u2502\n\u2502                                                                   \u2502\n\u2502  \"20 items shown. Use /v1/retrieve?hash=xxx for more.\"           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LLM PROCESSING                                                   \u2502\n\u2502                                                                   \u2502\n\u2502  Option A: LLM solves task with 20 items \u2192 Done                  \u2502\n\u2502  Option B: LLM needs more \u2192 retrieves via API                    \u2502\n\u2502            \u2192 We fetch from cache \u2192 Return instantly              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  FEEDBACK LOOP                                                    \u2502\n\u2502                                                                   \u2502\n\u2502  Track: What did the LLM retrieve? What queries?                 \u2502\n\u2502  Learn: \"For this tool, keep items matching common queries\"      \u2502\n\u2502  Improve: Next compression uses learned patterns                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ARCHITECTURE/#ccr-phase-1-compression-store","title":"CCR Phase 1: Compression Store","text":"<p>Location: <code>headroom/cache/compression_store.py</code></p> <p>When SmartCrusher compresses, the original content is stored for on-demand retrieval:</p> <pre><code>@dataclass\nclass CompressionEntry:\n    hash: str                    # 16-char SHA256 for retrieval\n    original_content: str        # Full JSON before compression\n    compressed_content: str      # Compressed JSON\n    original_item_count: int\n    compressed_item_count: int\n    tool_name: str | None        # For feedback tracking\n    created_at: float\n    ttl: int = 300               # 5 minute default\n</code></pre> <p>Features: - Thread-safe in-memory storage - TTL-based expiration (default 5 minutes) - LRU-style eviction when capacity reached - Built-in BM25 search within cached content</p> <p>Usage: <pre><code>store = get_compression_store()\n\n# Store compressed content\nhash_key = store.store(\n    original=original_json,\n    compressed=compressed_json,\n    original_item_count=1000,\n    compressed_item_count=20,\n    tool_name=\"search_api\",\n)\n\n# Retrieve later\nentry = store.retrieve(hash_key)\n\n# Or search within cached content\nresults = store.search(hash_key, \"user query\")\n</code></pre></p>"},{"location":"ARCHITECTURE/#ccr-phase-2-retrieval-api","title":"CCR Phase 2: Retrieval API","text":"<p>Endpoints:</p> Endpoint Method Description <code>/v1/retrieve</code> POST Retrieve original content by hash <code>/v1/retrieve?query=X</code> POST Search within cached content <p>Retrieval Request: <pre><code>{\n    \"hash\": \"abc123def456...\",\n    \"query\": \"find errors\"     // Optional: search within\n}\n</code></pre></p> <p>Response (full retrieval): <pre><code>{\n    \"hash\": \"abc123def456...\",\n    \"original_content\": \"[{...}, {...}, ...]\",\n    \"original_item_count\": 1000,\n    \"tool_name\": \"search_api\"\n}\n</code></pre></p> <p>Response (search): <pre><code>{\n    \"hash\": \"abc123def456...\",\n    \"query\": \"find errors\",\n    \"results\": [{...}, {...}, ...],\n    \"count\": 15\n}\n</code></pre></p>"},{"location":"ARCHITECTURE/#ccr-phase-3-tool-injection","title":"CCR Phase 3: Tool Injection","text":"<p>When compression happens, Headroom injects retrieval instructions into the LLM context.</p> <p>Method A: System Message Injection <pre><code>## Compressed Context Available\nThe following tool outputs have been compressed. If you need more detail,\ncall the retrieve_compressed tool with the hash.\n\nAvailable: hash=abc123 (1000\u219220 items from search_api)\n</code></pre></p> <p>Method B: MCP Tool Registration (Hybrid) When running as MCP server, Headroom exposes retrieval as a tool:</p> <pre><code>{\n    \"name\": \"headroom_retrieve\",\n    \"description\": \"Retrieve more items from compressed tool output\",\n    \"inputSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"hash\": {\"type\": \"string\"},\n            \"query\": {\"type\": \"string\"}\n        }\n    }\n}\n</code></pre> <p>Marker Injection: Compressed content includes retrieval markers: <pre><code>{\n    \"__headroom_compressed\": true,\n    \"__headroom_hash\": \"abc123def456\",\n    \"__headroom_stats\": {\n        \"original_items\": 1000,\n        \"kept_items\": 20,\n        \"errors_preserved\": 5\n    },\n    \"data\": [...]\n}\n</code></pre></p>"},{"location":"ARCHITECTURE/#ccr-phase-4-feedback-loop","title":"CCR Phase 4: Feedback Loop","text":"<p>Location: <code>headroom/cache/compression_feedback.py</code></p> <p>The feedback system learns from retrieval patterns to improve future compression.</p> <p>Tracked Patterns per Tool: <pre><code>@dataclass\nclass ToolPattern:\n    tool_name: str\n    total_compressions: int      # Times we compressed this tool\n    total_retrievals: int        # Times LLM asked for more\n    full_retrievals: int         # Retrieved everything\n    search_retrievals: int       # Used search query\n    common_queries: dict[str, int]   # Query frequency\n    queried_fields: dict[str, int]   # Fields mentioned in queries\n</code></pre></p> <p>Key Metrics: - Retrieval Rate: <code>total_retrievals / total_compressions</code>   - High (&gt;50%) \u2192 Compressing too aggressively   - Low (&lt;20%) \u2192 Compression is effective - Full Retrieval Rate: <code>full_retrievals / total_retrievals</code>   - High (&gt;80%) \u2192 Data is unique, consider skipping compression</p> <p>Compression Hints: <pre><code>@dataclass\nclass CompressionHints:\n    max_items: int = 15          # Target item count\n    suggested_items: int | None  # Calculated optimal\n    skip_compression: bool       # Don't compress at all\n    preserve_fields: list[str]   # Always keep these fields\n    aggressiveness: float        # 0.0 = aggressive, 1.0 = conservative\n    reason: str                  # Explanation\n</code></pre></p> <p>Feedback-Driven Adjustment: <pre><code># In SmartCrusher._crush_array()\nif self.config.use_feedback_hints and tool_name:\n    feedback = get_compression_feedback()\n    hints = feedback.get_compression_hints(tool_name)\n\n    if hints.skip_compression:\n        return items, f\"skip:feedback({hints.reason})\", None\n\n    if hints.suggested_items is not None:\n        self.config.max_items_after_crush = hints.suggested_items\n</code></pre></p> <p>Feedback Endpoints:</p> Endpoint Method Description <code>/v1/feedback</code> GET Get all learned patterns <code>/v1/feedback/{tool_name}</code> GET Get hints for specific tool <p>Example Response: <pre><code>{\n    \"total_compressions\": 150,\n    \"total_retrievals\": 23,\n    \"global_retrieval_rate\": 0.15,\n    \"tools_tracked\": 5,\n    \"tool_patterns\": {\n        \"search_api\": {\n            \"compressions\": 50,\n            \"retrievals\": 5,\n            \"retrieval_rate\": 0.10,\n            \"full_rate\": 0.20,\n            \"search_rate\": 0.80,\n            \"common_queries\": [\"status:error\", \"level:critical\"],\n            \"queried_fields\": [\"status\", \"level\", \"message\"]\n        }\n    }\n}\n</code></pre></p>"},{"location":"ARCHITECTURE/#ccr-phase-5-response-handler-automatic-tool-call-handling","title":"CCR Phase 5: Response Handler (Automatic Tool Call Handling)","text":"<p>Location: <code>headroom/ccr/response_handler.py</code></p> <p>The Problem: When the proxy injects the <code>headroom_retrieve</code> tool, the LLM might call it. But who handles that tool call? Without response handling, the tool call would go back to the client unhandled.</p> <p>The Solution: The Response Handler intercepts LLM responses, detects CCR tool calls, executes retrievals automatically, and continues the conversation until the LLM produces a final response.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  RESPONSE HANDLER FLOW                                           \u2502\n\u2502                                                                   \u2502\n\u2502  1. LLM Response arrives                                         \u2502\n\u2502     \u2514\u2500 Contains: tool_use(headroom_retrieve, hash=abc123)       \u2502\n\u2502                                                                   \u2502\n\u2502  2. Handler detects CCR tool call                                \u2502\n\u2502     \u2514\u2500 Extracts hash and optional query                         \u2502\n\u2502                                                                   \u2502\n\u2502  3. Handler executes retrieval                                   \u2502\n\u2502     \u2514\u2500 Full retrieval: store.retrieve(hash)                     \u2502\n\u2502     \u2514\u2500 Search: store.search(hash, query)                        \u2502\n\u2502                                                                   \u2502\n\u2502  4. Handler continues conversation                               \u2502\n\u2502     \u2514\u2500 Adds tool result to messages                             \u2502\n\u2502     \u2514\u2500 Makes another API call                                   \u2502\n\u2502                                                                   \u2502\n\u2502  5. Repeat until no CCR tool calls                              \u2502\n\u2502     \u2514\u2500 Max 3 rounds (configurable)                              \u2502\n\u2502                                                                   \u2502\n\u2502  6. Return final response to client                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Classes:</p> <pre><code>@dataclass\nclass CCRToolCall:\n    tool_call_id: str      # For matching response\n    hash_key: str          # CCR hash to retrieve\n    query: str | None      # Optional search query\n\n@dataclass\nclass CCRToolResult:\n    tool_call_id: str\n    content: str           # Retrieved data as JSON\n    success: bool\n    items_retrieved: int\n    was_search: bool       # True if search, False if full retrieval\n\nclass CCRResponseHandler:\n    async def handle_response(\n        self,\n        response: dict,           # Initial LLM response\n        messages: list,           # Conversation history\n        tools: list,              # Tool definitions\n        api_call_fn: Callable,    # Function to make API calls\n        provider: str,            # \"anthropic\" or \"openai\"\n    ) -&gt; dict:\n        \"\"\"Handle CCR tool calls until final response.\"\"\"\n</code></pre> <p>Streaming Support:</p> <p>The handler also supports streaming responses via <code>StreamingCCRHandler</code>:</p> <pre><code>class StreamingCCRBuffer:\n    \"\"\"Buffers streaming chunks to detect CCR tool calls.\"\"\"\n    chunks: list[bytes]\n    detected_ccr: bool\n\nclass StreamingCCRHandler:\n    \"\"\"Handles CCR in streaming responses.\"\"\"\n    async def process_stream(self, stream, messages, tools, api_call_fn):\n        \"\"\"Yields chunks, switching to buffered mode if CCR detected.\"\"\"\n</code></pre>"},{"location":"ARCHITECTURE/#ccr-phase-6-context-tracker-multi-turn-awareness","title":"CCR Phase 6: Context Tracker (Multi-Turn Awareness)","text":"<p>Location: <code>headroom/ccr/context_tracker.py</code></p> <p>The Problem: In multi-turn conversations, earlier compressed data might become relevant later. Without tracking, the LLM has \"context amnesia\" - it can't reference data that was compressed in turn 1 when answering a question in turn 5.</p> <p>The Solution: The Context Tracker maintains awareness of all compressed content across the conversation and can proactively expand relevant data when a new query might need it.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  CONTEXT TRACKER FLOW                                            \u2502\n\u2502                                                                   \u2502\n\u2502  Turn 1: Search returns 100 files \u2192 compressed to 10            \u2502\n\u2502          Tracker stores: hash=abc123, sample=\"auth.py, db.py\"   \u2502\n\u2502                                                                   \u2502\n\u2502  Turn 5: User asks \"What about the authentication middleware?\"  \u2502\n\u2502          Tracker analyzes query:                                 \u2502\n\u2502          - \"authentication\" matches \"auth.py\" in sample         \u2502\n\u2502          - Relevance score: 0.7 (above threshold)               \u2502\n\u2502                                                                   \u2502\n\u2502  Proactive Expansion:                                           \u2502\n\u2502          - Retrieves abc123 before LLM responds                 \u2502\n\u2502          - Adds expanded context to request                     \u2502\n\u2502                                                                   \u2502\n\u2502  Result: LLM sees full file list, can answer accurately         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Classes:</p> <pre><code>@dataclass\nclass CompressedContext:\n    hash_key: str              # CCR hash\n    turn_number: int           # When compression happened\n    timestamp: float           # For age-based filtering\n    tool_name: str | None      # Which tool was compressed\n    original_item_count: int\n    compressed_item_count: int\n    query_context: str         # User query at compression time\n    sample_content: str        # Preview for relevance matching\n\n@dataclass\nclass ExpansionRecommendation:\n    hash_key: str\n    reason: str                # Human-readable explanation\n    relevance_score: float     # 0-1, higher = more relevant\n    expand_full: bool          # True = full retrieval\n    search_query: str | None   # If expand_full=False\n\nclass ContextTracker:\n    def track_compression(self, hash_key, turn_number, ...):\n        \"\"\"Track a compression event.\"\"\"\n\n    def analyze_query(self, query: str) -&gt; list[ExpansionRecommendation]:\n        \"\"\"Find relevant compressed contexts for a query.\"\"\"\n\n    def execute_expansions(self, recommendations) -&gt; list[dict]:\n        \"\"\"Execute recommended expansions.\"\"\"\n</code></pre> <p>Relevance Calculation:</p> <p>The tracker uses simple but effective heuristics:</p> <ol> <li>Keyword overlap with sample content - Extract keywords from query, match against compressed content preview</li> <li>Keyword overlap with original query - Match against the query that triggered compression</li> <li>Tool name relevance - File operations more likely to need expansion for \"file\", \"where\", \"find\" queries</li> <li>Age discount - Older contexts get lower scores</li> </ol> <p>Configuration:</p> <pre><code>@dataclass\nclass ContextTrackerConfig:\n    enabled: bool = True\n    max_tracked_contexts: int = 100      # LRU eviction\n    relevance_threshold: float = 0.3     # Min score to recommend\n    max_context_age_seconds: float = 300 # 5 minutes\n    proactive_expansion: bool = True\n    max_proactive_expansions: int = 2    # Per query\n</code></pre>"},{"location":"ARCHITECTURE/#why-ccr-is-a-moat","title":"Why CCR is a Moat","text":"<ol> <li>Reversible: No permanent information loss. Worst case = retrieve everything.</li> <li>Transparent: LLM knows it can ask for more data.</li> <li>Automatic: Response Handler executes retrievals without client intervention.</li> <li>Context-Aware: Context Tracker prevents multi-turn amnesia.</li> <li>Feedback Loop: Learn from actual needs, not guesses.</li> <li>Network Effect: Retrieval patterns across users improve compression for everyone.</li> <li>Zero-Risk: If compression fails, instant fallback to original data.</li> </ol>"},{"location":"ARCHITECTURE/#image-compression-architecture","title":"Image Compression Architecture","text":"<p>Vision models charge by the token, and images are expensive (765-2900 tokens for a typical image). Headroom's image compression uses a trained ML router to automatically select the optimal compression technique.</p>"},{"location":"ARCHITECTURE/#the-key-insight_1","title":"The Key Insight","text":"<p>Not all image queries need full resolution: - \"What is this?\" \u2192 Low detail is fine (87% savings) - \"Count the whiskers\" \u2192 Need full detail (0% savings) - \"Read the sign\" \u2192 Could convert to text (99% savings)</p>"},{"location":"ARCHITECTURE/#how-it-works","title":"How It Works","text":"<pre><code>User: [image] + \"What animal is this?\"\n           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. Query Analysis              \u2502\n\u2502  TrainedRouter (MiniLM)         \u2502\n\u2502  Classifies \u2192 full_low          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  2. Image Analysis (Optional)   \u2502\n\u2502  SigLIP checks:                 \u2502\n\u2502  - Has text? Is complex?        \u2502\n\u2502  - Fine details needed?         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  3. Apply Compression           \u2502\n\u2502  OpenAI: detail=\"low\"           \u2502\n\u2502  Anthropic: Resize to 512px     \u2502\n\u2502  Google: Resize to 768px        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2193\nCompressed request \u2192 LLM \u2192 Response\n</code></pre>"},{"location":"ARCHITECTURE/#the-trained-router","title":"The Trained Router","text":"<p>A fine-tuned MiniLM classifier hosted on HuggingFace:</p> <ul> <li>Model: <code>chopratejas/technique-router</code></li> <li>Size: ~128MB (downloaded once, cached)</li> <li>Accuracy: 93.7% on 1,157 training examples</li> <li>Latency: ~10ms CPU, ~2ms GPU</li> </ul> <p>The router learns from examples like: | Query | Technique | |-------|-----------| | \"What is this?\" | <code>full_low</code> | | \"Count the items\" | <code>preserve</code> | | \"Read the text\" | <code>transcode</code> | | \"What's in the corner?\" | <code>crop</code> |</p>"},{"location":"ARCHITECTURE/#provider-specific-compression","title":"Provider-Specific Compression","text":"<p>Each provider handles images differently:</p> Provider Method Savings OpenAI <code>detail=\"low\"</code> parameter ~87% Anthropic PIL resize to 512px ~75% Google PIL resize to 768px (tile-optimized) ~75%"},{"location":"ARCHITECTURE/#integration-points","title":"Integration Points","text":"<p>Image compression runs in the proxy before text compression:</p> <pre><code>Request arrives\n      \u2193\n[Image Compression] \u2190 NEW\n      \u2193\n[Transform Pipeline: Cache Aligner \u2192 Smart Crusher \u2192 ...]\n      \u2193\nForward to LLM\n</code></pre> <p>This ensures images are compressed first, then text compression (CCR, SmartCrusher) handles the rest.</p>"},{"location":"ARCHITECTURE/#code-location","title":"Code Location","text":"<pre><code>headroom/\n\u251c\u2500\u2500 image/\n\u2502   \u251c\u2500\u2500 __init__.py         # Public API\n\u2502   \u251c\u2500\u2500 compressor.py       # ImageCompressor class\n\u2502   \u2514\u2500\u2500 trained_router.py   # TrainedRouter (HuggingFace model)\n\u251c\u2500\u2500 proxy/\n\u2502   \u2514\u2500\u2500 server.py           # Integration point\n</code></pre>"},{"location":"ARCHITECTURE/#file-structure-explained","title":"File Structure Explained","text":"<pre><code>headroom/\n\u251c\u2500\u2500 __init__.py          # Public exports\n\u251c\u2500\u2500 client.py            # HeadroomClient - the main wrapper\n\u251c\u2500\u2500 config.py            # All configuration dataclasses\n\u251c\u2500\u2500 parser.py            # Message \u2192 Block decomposition\n\u251c\u2500\u2500 tokenizer.py         # Token counting abstraction\n\u251c\u2500\u2500 utils.py             # Hashing, markers, helpers\n\u2502\n\u251c\u2500\u2500 providers/\n\u2502   \u251c\u2500\u2500 base.py          # Provider/TokenCounter protocols\n\u2502   \u251c\u2500\u2500 openai.py        # OpenAI-specific (tiktoken)\n\u2502   \u2514\u2500\u2500 anthropic.py     # Anthropic-specific\n\u2502\n\u251c\u2500\u2500 transforms/\n\u2502   \u251c\u2500\u2500 base.py              # Transform protocol\n\u2502   \u251c\u2500\u2500 pipeline.py          # Orchestrates all transforms\n\u2502   \u251c\u2500\u2500 cache_aligner.py     # Date extraction for caching\n\u2502   \u251c\u2500\u2500 tool_crusher.py      # Naive compression (disabled)\n\u2502   \u251c\u2500\u2500 smart_crusher.py     # Statistical compression (default)\n\u2502   \u251c\u2500\u2500 rolling_window.py    # Token limit enforcement (position-based)\n\u2502   \u251c\u2500\u2500 intelligent_context.py  # Semantic context management (score-based)\n\u2502   \u251c\u2500\u2500 scoring.py           # Message importance scoring\n\u2502   \u2514\u2500\u2500 llmlingua_compressor.py  # ML-based compression (opt-in)\n\u2502\n\u251c\u2500\u2500 cache/               # CCR Architecture - Caching &amp; Storage\n\u2502   \u251c\u2500\u2500 compression_store.py    # Phase 1: Store original content\n\u2502   \u251c\u2500\u2500 compression_feedback.py # Phase 4: Learn from retrievals\n\u2502   \u251c\u2500\u2500 anthropic.py     # Anthropic cache optimizer\n\u2502   \u251c\u2500\u2500 openai.py        # OpenAI cache optimizer\n\u2502   \u251c\u2500\u2500 google.py        # Google cache optimizer\n\u2502   \u2514\u2500\u2500 dynamic_detector.py # Dynamic content detection\n\u2502\n\u251c\u2500\u2500 ccr/                 # CCR Architecture - Tool Injection &amp; Response Handling\n\u2502   \u251c\u2500\u2500 __init__.py             # CCR module exports\n\u2502   \u251c\u2500\u2500 tool_injection.py       # Phase 3: Inject retrieval tool\n\u2502   \u251c\u2500\u2500 response_handler.py     # Phase 5: Handle CCR tool calls\n\u2502   \u251c\u2500\u2500 context_tracker.py      # Phase 6: Multi-turn context tracking\n\u2502   \u2514\u2500\u2500 mcp_server.py           # MCP server for retrieval tool\n\u2502\n\u251c\u2500\u2500 relevance/           # Relevance scoring for compression\n\u2502   \u251c\u2500\u2500 bm25.py          # BM25 keyword scorer\n\u2502   \u251c\u2500\u2500 embedding.py     # Semantic embedding scorer\n\u2502   \u2514\u2500\u2500 hybrid.py        # Adaptive fusion scorer\n\u2502\n\u251c\u2500\u2500 storage/\n\u2502   \u251c\u2500\u2500 base.py          # Storage protocol\n\u2502   \u251c\u2500\u2500 sqlite.py        # SQLite implementation\n\u2502   \u2514\u2500\u2500 jsonl.py         # JSON Lines implementation\n\u2502\n\u251c\u2500\u2500 proxy/\n\u2502   \u2514\u2500\u2500 server.py        # Production HTTP proxy (CCR endpoints)\n\u2502\n\u2514\u2500\u2500 reporting/\n    \u2514\u2500\u2500 generator.py     # HTML report generation\n</code></pre>"},{"location":"ARCHITECTURE/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"ARCHITECTURE/#1-provider-agnostic","title":"1. Provider-Agnostic","text":"<p>Works with ANY OpenAI-compatible API: - OpenAI - Azure OpenAI - Anthropic - Groq - Together - Local models (Ollama)</p>"},{"location":"ARCHITECTURE/#2-deterministic-transforms","title":"2. Deterministic Transforms","text":"<p>No LLM calls for compression. Everything is: - Statistical analysis - Pattern matching - Rule-based</p> <p>This means: - Predictable results - Fast (&lt;10ms overhead) - No added API costs</p>"},{"location":"ARCHITECTURE/#3-safety-first","title":"3. Safety First","text":"<ul> <li>Never modify user/assistant TEXT content</li> <li>Tool call + response are atomic (drop both or neither)</li> <li>Parse failures = no-op (return unchanged)</li> <li>Audit mode for testing before optimizing</li> </ul>"},{"location":"ARCHITECTURE/#4-smart-by-default","title":"4. Smart by Default","text":"<ul> <li>SmartCrusher enabled (statistical analysis)</li> <li>ToolCrusher disabled (naive rules)</li> <li>Conservative settings that preserve important data</li> </ul>"},{"location":"ARCHITECTURE/#what-makes-this-different","title":"What Makes This Different?","text":""},{"location":"ARCHITECTURE/#vs-summarization-llm-based-compression","title":"vs. Summarization (LLM-based compression)","text":"Headroom Summarization Deterministic Non-deterministic ~10ms overhead ~2-5 seconds overhead No extra API cost Costs money to summarize Preserves structure Loses structure Can't hallucinate Can hallucinate"},{"location":"ARCHITECTURE/#vs-simple-truncation","title":"vs. Simple Truncation","text":"Headroom Truncation Keeps important data Loses end of data Statistical analysis No analysis Detects spikes Misses spikes Factors out constants Keeps redundancy"},{"location":"ARCHITECTURE/#the-numbers-from-our-tests","title":"The Numbers (From Our Tests)","text":"<p>Real-world SRE incident investigation: - 5 tool calls: Metrics, logs, status, deployments, runbook - Original: 22,048 tokens - After SmartCrusher: 2,190 tokens - Reduction: 90% - Quality Score: 5.0/5 (no information loss)</p> <p>The model could still: - Identify the CPU spike (preserved by change point detection) - Reference specific error rates (kept in compressed data) - Provide correct remediation commands</p>"},{"location":"ARCHITECTURE/#summary","title":"Summary","text":"<p>Headroom is a Context Budget Controller that:</p> <ol> <li>Wraps your existing LLM client</li> <li>Analyzes your messages to find waste</li> <li>Compresses tool outputs intelligently (not blindly)</li> <li>Preserves important information (spikes, anomalies, unique data)</li> <li>Logs everything for observability</li> <li>Saves 70-90% of tokens on tool-heavy workloads</li> </ol> <p>The key insight: Most tool output redundancy is statistical (repeated values, constant fields, similar messages). By analyzing the data first, we can compress intelligently without losing the information that matters.</p>"},{"location":"LATENCY_BENCHMARKS/","title":"Headroom Latency Benchmarks","text":"<p>Measured compression overhead across content types and sizes to answer: does the token savings outweigh the processing time?</p> <p>Generated: 2026-02-24 01:11 UTC</p>"},{"location":"LATENCY_BENCHMARKS/#environment","title":"Environment","text":"<ul> <li>Platform: macOS-26.1-arm64-arm-64bit</li> <li>Processor: arm</li> <li>Python: 3.11.11</li> <li>Headroom: v0.3.7</li> </ul>"},{"location":"LATENCY_BENCHMARKS/#tldr","title":"TL;DR","text":"<ul> <li>Average compression: 93% token reduction</li> <li>Maximum compression overhead: 12213ms (p50)</li> <li>Net latency win: 11/12 scenarios against Claude Sonnet 4.5</li> </ul>"},{"location":"LATENCY_BENCHMARKS/#compression-overhead-by-scenario","title":"Compression Overhead by Scenario","text":"Scenario Tokens In Tokens Out Saved Ratio p50 (ms) p95 (ms) Mean (ms) JSON: Search Results (100 items) 10.2K 1.5K 8.7K 86% 189 231 196 JSON: Search Results (500 items) 50.2K 1.5K 48.7K 97% 943 955 943 JSON: Search Results (1K items) 100.5K 1.5K 99.0K 99% 2012 2198 2032 JSON: Search Results (5K items) 502.6K 1.5K 501.2K 100% 12213 12804 12223 JSON: API Responses (500 items) 38.9K 1.1K 37.8K 97% 743 776 744 JSON: Database Rows (1K rows) 43.7K 605 43.1K 99% 961 1104 986 JSON: String Array (100 strings) 1.1K 231 820 78% 15.0 15.4 15.0 JSON: String Array (500 strings) 4.9K 233 4.6K 95% 71.9 80.3 72.7 JSON: String Array (1K strings) 9.6K 242 9.4K 97% 146 160 147 JSON: Number Array (200 numbers) 1.2K 192 1.1K 85% 30.9 61.9 33.8 JSON: Number Array (1K numbers) 6.1K 243 5.8K 96% 301 307 300 JSON: Mixed Array (250 items) 2.3K 368 1.9K 84% 38.4 39.8 38.4"},{"location":"LATENCY_BENCHMARKS/#per-transform-latency-breakdown","title":"Per-Transform Latency Breakdown","text":"Scenario Transform p50 (ms) % of Total JSON: Search Results (100 items) cache_aligner 2.2 1% JSON: Search Results (100 items) content_router 186 98% JSON: Search Results (100 items) rolling_window &lt;0.01 0% JSON: Search Results (500 items) cache_aligner 10.7 1% JSON: Search Results (500 items) content_router 927 98% JSON: Search Results (500 items) rolling_window &lt;0.01 0% JSON: Search Results (1K items) cache_aligner 21.0 1% JSON: Search Results (1K items) content_router 1980 98% JSON: Search Results (1K items) rolling_window &lt;0.01 0% JSON: Search Results (5K items) cache_aligner 105 1% JSON: Search Results (5K items) content_router 11985 98% JSON: Search Results (5K items) rolling_window &lt;0.01 0% JSON: API Responses (500 items) cache_aligner 8.8 1% JSON: API Responses (500 items) content_router 729 98% JSON: API Responses (500 items) rolling_window &lt;0.01 0% JSON: Database Rows (1K rows) cache_aligner 9.3 1% JSON: Database Rows (1K rows) content_router 946 99% JSON: Database Rows (1K rows) rolling_window &lt;0.01 0% JSON: String Array (100 strings) cache_aligner 0.27 2% JSON: String Array (100 strings) content_router 14.5 97% JSON: String Array (100 strings) rolling_window &lt;0.01 0% JSON: String Array (500 strings) cache_aligner 0.95 1% JSON: String Array (500 strings) content_router 70.2 98% JSON: String Array (500 strings) rolling_window &lt;0.01 0% JSON: String Array (1K strings) cache_aligner 1.9 1% JSON: String Array (1K strings) content_router 143 98% JSON: String Array (1K strings) rolling_window &lt;0.01 0% JSON: Number Array (200 numbers) cache_aligner 0.66 2% JSON: Number Array (200 numbers) content_router 29.6 96% JSON: Number Array (200 numbers) rolling_window &lt;0.01 0% JSON: Number Array (1K numbers) cache_aligner 2.5 1% JSON: Number Array (1K numbers) content_router 297 99% JSON: Number Array (1K numbers) rolling_window &lt;0.01 0% JSON: Mixed Array (250 items) cache_aligner 0.58 1% JSON: Mixed Array (250 items) content_router 37.4 97% JSON: Mixed Array (250 items) rolling_window &lt;0.01 0%"},{"location":"LATENCY_BENCHMARKS/#cost-benefit-analysis","title":"Cost-Benefit Analysis","text":"<p>Net latency benefit = LLM time saved from fewer tokens - compression overhead.</p> Scenario Compress (ms) LLM Saved (ms)* Net Benefit $/1K Requests** JSON: Search Results (100 items) 189 261 +71.8ms $26.13 JSON: Search Results (500 items) 943 1461 +517.5ms $146.06 JSON: Search Results (1K items) 2012 2969 +956.9ms $296.91 JSON: Search Results (5K items) 12213 15035 +2822.2ms $1503.53 JSON: API Responses (500 items) 743 1134 +390.7ms $113.38 JSON: Database Rows (1K rows) 961 1292 +330.7ms $129.16 JSON: String Array (100 strings) 15.0 24.6 +9.6ms $2.46 JSON: String Array (500 strings) 71.9 139 +67.1ms $13.90 JSON: String Array (1K strings) 146 282 +135.9ms $28.16 JSON: Number Array (200 numbers) 30.9 31.6 +0.7ms $3.16 JSON: Number Array (1K numbers) 301 175 -126.3ms $17.45 JSON: Mixed Array (250 items) 38.4 56.6 +18.2ms $5.66 <p>* LLM time saved based on Claude Sonnet 4.5 prefill rate (0.03ms/token) ** Cost savings at $3.0/MTok input pricing</p>"},{"location":"LATENCY_BENCHMARKS/#break-even-across-models","title":"Break-Even Across Models","text":"<p>Compression overhead (p50) vs. LLM time saved for different model speed tiers:</p> Scenario Compress (ms) GPT-4o Mini GPT-4o Claude Sonnet 4.5 Claude Opus 4 JSON: Search Results (100 items) 189 -102ms +71.8ms +71.8ms +507ms JSON: Search Results (500 items) 943 -456ms +518ms +518ms +2952ms JSON: Search Results (1K items) 2012 -1022ms +957ms +957ms +5905ms JSON: Search Results (5K items) 12213 -7201ms +2822ms +2822ms +27881ms JSON: API Responses (500 items) 743 -365ms +391ms +391ms +2280ms JSON: Database Rows (1K rows) 961 -530ms +331ms +331ms +2483ms JSON: String Array (100 strings) 15.0 -6.8ms +9.6ms +9.6ms +50.6ms JSON: String Array (500 strings) 71.9 -25.6ms +67.1ms +67.1ms +299ms JSON: String Array (1K strings) 146 -51.9ms +136ms +136ms +605ms JSON: Number Array (200 numbers) 30.9 -20.4ms +0.68ms +0.68ms +53.3ms JSON: Number Array (1K numbers) 301 -243ms -126ms -126ms +165ms JSON: Mixed Array (250 items) 38.4 -19.5ms +18.2ms +18.2ms +113ms"},{"location":"LATENCY_BENCHMARKS/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Compression pays for itself in latency for 11/12 compressing scenarios (json). For these, the LLM prefill time saved exceeds compression overhead.</li> <li>ContentRouter is 98% of pipeline cost on average \u2014 it does the actual compression work. CacheAligner and context management are &lt;2% of total time.</li> <li>Cost savings are substantial regardless of latency. The highest-compression scenario (JSON: Search Results (5K items)) saves $1504/1K requests at Claude Sonnet 4.5 pricing.</li> <li>Slower/pricier models benefit most. Claude Opus shows a net latency win in 12/12 scenarios vs 11 for Claude Sonnet 4.5, with 0.08ms/token prefill.</li> </ol> <p>Benchmarks run with <code>python benchmarks/bench_latency.py</code>. Results vary based on hardware, Python version, and content characteristics.</p>"},{"location":"LIMITATIONS/","title":"Headroom Limitations &amp; Known Behavior","text":"<p>Honest documentation of when Headroom helps, when it doesn't, and what to watch out for.</p>"},{"location":"LIMITATIONS/#when-headroom-helps-and-when-it-doesnt","title":"When Headroom Helps (and When It Doesn't)","text":"Content Type Compression Latency Impact Best For JSON: Arrays of dicts (search results, API responses, DB rows) 86-100% Net latency win on Sonnet/Opus Primary use case \u2014 always use JSON: Arrays of strings (file paths, log lines, tags) 60-90% Net latency win New \u2014 works with all string arrays JSON: Arrays of numbers (metrics, time series) 70-85% Net latency win New \u2014 includes statistical summary JSON: Mixed-type arrays 50-70% Net latency win New \u2014 groups by type, compresses each Structured logs (as JSON) 82-95% Net latency win Log entries in tool outputs Agentic conversations (25-50 turns) 56-81% Break-even to net win Multi-tool agent sessions Plain text (documentation, articles) 43-46% Adds latency (cost savings only) Cost optimization, not speed Code Passthrough Minimal overhead See Code Compression RAG document contexts Passthrough Minimal overhead Not compressed (plain text in user messages) <p>See LATENCY_BENCHMARKS.md for full data with per-scenario timing.</p>"},{"location":"LIMITATIONS/#code-compression","title":"Code Compression","text":"<p>Headroom includes an AST-aware CodeCompressor (tree-sitter, 8 languages) but it's gated behind safety protections that prevent it from firing in most real-world scenarios. This is intentional.</p> <p>Why code mostly passes through:</p> <ol> <li>Word count gate: Content under 50 words is silently skipped</li> <li>Recent code protection (<code>protect_recent_code=4</code>): Code in the last 4 messages is never compressed. In typical tool-call patterns, the tool result is always \"recent\"</li> <li>Analysis intent protection (<code>protect_analysis_context=True</code>): If the most recent user message contains keywords like \"analyze\", \"review\", \"explain\", \"fix\", \"debug\", \"optimize\", \"error\", \"bug\" \u2014 ALL code in the conversation is protected</li> </ol> <p>Why this is the right default: Code is almost always fetched because the user wants to work with it. Compressing function bodies would remove exactly what they need. LLMs like Claude are excellent at navigating large code files without compression.</p> <p>Where code savings come from: The IntelligentContextManager drops old code messages that are no longer relevant (scoring-based), which is a better strategy than stripping function bodies from active code.</p> <p>Override: Set <code>protect_analysis_context=False</code> in <code>ContentRouterConfig</code> for aggressive code compression. Requires <code>headroom-ai[code]</code> for tree-sitter.</p>"},{"location":"LIMITATIONS/#json-compression-constraints","title":"JSON Compression Constraints","text":""},{"location":"LIMITATIONS/#what-gets-compressed","title":"What gets compressed","text":"<ul> <li>Arrays of dicts: Full statistical analysis with adaptive K (Kneedle algorithm)</li> <li>Arrays of strings: Dedup + adaptive sampling + error preservation</li> <li>Arrays of numbers: Statistical summary + outlier/change-point preservation</li> <li>Mixed-type arrays: Grouped by type, each group compressed independently</li> <li>Nested objects: Recursed into, arrays within are compressed (up to depth 5)</li> </ul>"},{"location":"LIMITATIONS/#what-passes-through","title":"What passes through","text":"<ul> <li>Arrays below 5 items (<code>min_items_to_analyze</code>)</li> <li>Content below 200 tokens (<code>min_tokens_to_crush</code>)</li> <li>Bool-only arrays (not useful to compress)</li> <li>JSON objects without array values</li> <li>Malformed JSON (silently passes through, no error)</li> <li>Non-JSON content (handled by other pipeline stages)</li> </ul>"},{"location":"LIMITATIONS/#edge-cases","title":"Edge cases","text":"<ul> <li>NaN/Infinity in numeric fields: Filtered out before statistics are computed</li> <li>Nesting depth &gt; 5: Inner arrays not examined for compression</li> <li>Mixed-type arrays with small groups: Groups below <code>min_items_to_analyze</code> are kept as-is</li> </ul>"},{"location":"LIMITATIONS/#adaptive-k-how-item-retention-works","title":"Adaptive K: How Item Retention Works","text":"<p>SmartCrusher doesn't use fixed K values. It uses information-theoretic sizing:</p> <ol> <li>Kneedle algorithm on bigram coverage curves finds the point where adding more items stops providing new information</li> <li>SimHash fingerprinting detects near-duplicate items</li> <li>zlib validation ensures the subset captures the full set's diversity</li> <li>The resulting K is split: 30% from array start, 15% from end, 55% for importance-scored items</li> </ol> <p>Safety guarantees (additive, never dropped): - Error items (containing \"error\", \"exception\", \"failed\", \"critical\", etc.) \u2014 across ALL array types - Numeric anomalies (&gt; 2\u03c3 from mean) - String length anomalies (&gt; 2\u03c3 from mean length) - Change points (sudden shifts in running values)</p> <p>These are kept even if they exceed the K budget.</p>"},{"location":"LIMITATIONS/#text-compression-llmlingua","title":"Text Compression (LLMLingua)","text":"<ul> <li>Requires: <code>headroom-ai[llmlingua]</code> \u2014 downloads ~2GB model, needs ~1GB RAM</li> <li>First call: 10-30s model load latency (cached globally after)</li> <li>Sequence length: Content chunked at 512 tokens (model limit)</li> <li>Content &lt; 100 tokens: Skipped</li> <li>Latency: Adds overhead that doesn't break even on fast models (GPT-4o Mini, Sonnet). Use for cost savings, not speed</li> <li>Thread safety: Single global model instance with lock \u2014 sequential access under concurrency</li> </ul>"},{"location":"LIMITATIONS/#error-handling","title":"Error Handling","text":"<p>All compressors follow the same principle: fail gracefully, return original content unchanged.</p> <ul> <li>Invalid JSON \u2192 passthrough (no error raised)</li> <li>AST parse failure in CodeCompressor \u2192 falls back to original or LLMLingua</li> <li>Compression makes output larger \u2192 original returned</li> <li>Missing optional dependencies (tree-sitter, LLMLingua) \u2192 passthrough with warning log</li> <li>One exception: LLMLingua out-of-memory during model loading raises <code>RuntimeError</code></li> </ul> <p>Errors are logged at WARNING level and never propagated to callers.</p>"},{"location":"LIMITATIONS/#toin-cold-start","title":"TOIN Cold Start","text":"<p>The Tool Output Intelligence Network (TOIN) learns compression patterns from usage. For new tool types:</p> <ul> <li>No learned patterns exist \u2192 falls back to statistical heuristics</li> <li>Confidence below <code>toin_confidence_threshold</code> (default 0.3) \u2192 TOIN hints ignored</li> <li>Patterns build up over time as tools are used repeatedly</li> <li>Cross-session learning requires persistence (<code>TelemetryConfig.storage_path</code>)</li> </ul>"},{"location":"LIMITATIONS/#cachealigner-behavior","title":"CacheAligner Behavior","text":"<ul> <li>Only processes system messages for dynamic content extraction</li> <li>Dynamic content in user/assistant/tool messages is not extracted</li> <li>May add small markers (<code>[Dynamic Context]</code> separator) that slightly increase token count</li> <li>Whitespace normalization may affect content with significant indentation (code blocks, ASCII art)</li> </ul>"},{"location":"LIMITATIONS/#provider-interactions","title":"Provider Interactions","text":"<ul> <li>CacheAligner is designed to maximize Anthropic/OpenAI prefix cache hit rates</li> <li>Token counting uses model-specific tokenizers (tiktoken for OpenAI, calibrated estimation for Anthropic)</li> <li>Compression works with all providers \u2014 no provider-specific limitations</li> <li>Compressed content is valid JSON \u2014 downstream tools and parsers work unchanged</li> </ul>"},{"location":"LIMITATIONS/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>ContentRouter accounts for 91-98% of pipeline cost \u2014 it does the actual compression work</li> <li>CacheAligner and RollingWindow are sub-millisecond</li> <li>Scaling is roughly linear with input size</li> <li>Full benchmark data: LATENCY_BENCHMARKS.md</li> </ul>"},{"location":"LIMITATIONS/#configuration-tuning","title":"Configuration Tuning","text":"Parameter Default Effect <code>min_items_to_analyze</code> 5 Arrays below this pass through <code>min_tokens_to_crush</code> 200 Content below this passes through <code>max_items_after_crush</code> 15 Upper bound on retained items <code>variance_threshold</code> 2.0 Std devs for anomaly detection (lower = more preserved) <code>first_fraction</code> 0.3 Fraction of K allocated to array start <code>last_fraction</code> 0.15 Fraction of K allocated to array end <code>protect_analysis_context</code> True Protect code when user asks about it <code>protect_recent_code</code> 4 Messages from end to protect code <code>skip_user_messages</code> True Never compress user messages <code>toin_confidence_threshold</code> 0.3 Minimum TOIN confidence to apply hints"},{"location":"agno/","title":"Agno Integration","text":"<p>Headroom integrates with Agno (formerly Phidata) to provide automatic context optimization for AI agents. This guide covers model wrapping, observability hooks, and multi-provider support.</p>"},{"location":"agno/#installation","title":"Installation","text":"<pre><code>pip install \"headroom-ai[agno]\"\n</code></pre> <p>This installs Headroom with Agno support. You'll also need Agno itself:</p> <pre><code>pip install agno\n</code></pre>"},{"location":"agno/#quick-start","title":"Quick Start","text":"<pre><code>from agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom headroom.integrations.agno import HeadroomAgnoModel\n\n# Wrap your model\nmodel = HeadroomAgnoModel(OpenAIChat(id=\"gpt-4o\"))\n\n# Create agent as usual\nagent = Agent(model=model)\n\n# Use exactly like before\nresponse = agent.run(\"What's the capital of France?\")\n\n# Check savings\nprint(f\"Tokens saved: {model.total_tokens_saved}\")\nprint(model.get_savings_summary())\n# {'total_requests': 1, 'total_tokens_saved': 245, 'average_savings_percent': 12.3}\n</code></pre>"},{"location":"agno/#integration-patterns","title":"Integration Patterns","text":""},{"location":"agno/#1-basic-model-wrapping","title":"1. Basic Model Wrapping","text":"<p>The simplest integration - wrap any Agno model with <code>HeadroomAgnoModel</code>:</p> <pre><code>from agno.models.openai import OpenAIChat\nfrom agno.models.anthropic import Claude\nfrom agno.models.google import Gemini\nfrom headroom.integrations.agno import HeadroomAgnoModel\n\n# Works with any Agno model\nopenai_model = HeadroomAgnoModel(OpenAIChat(id=\"gpt-4o\"))\nclaude_model = HeadroomAgnoModel(Claude(id=\"claude-3-5-sonnet-20241022\"))\ngemini_model = HeadroomAgnoModel(Gemini(id=\"gemini-2.0-flash\"))\n\n# Each automatically uses the correct provider for accurate token counting\n</code></pre> <p>Why this matters: Headroom automatically detects the underlying provider and applies the correct tokenizer for accurate optimization metrics.</p>"},{"location":"agno/#2-agent-with-observability-hooks","title":"2. Agent with Observability Hooks","text":"<p>Use hooks for detailed tracking without modifying your model:</p> <pre><code>from agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom headroom.integrations.agno import (\n    HeadroomAgnoModel,\n    HeadroomPreHook,\n    HeadroomPostHook,\n)\n\n# Model wrapper for optimization\nmodel = HeadroomAgnoModel(OpenAIChat(id=\"gpt-4o\"))\n\n# Hooks for observability\npre_hook = HeadroomPreHook()\npost_hook = HeadroomPostHook(token_alert_threshold=10000)\n\nagent = Agent(\n    model=model,\n    pre_hooks=[pre_hook],\n    post_hooks=[post_hook],\n)\n\n# Run agent\nresponse = agent.run(\"Analyze this large dataset...\")\n\n# Check metrics from model\nprint(f\"Tokens saved: {model.total_tokens_saved}\")\n\n# Check observability from hooks\nprint(f\"Post-hook summary: {post_hook.get_summary()}\")\nprint(f\"Alerts triggered: {post_hook.alerts}\")\n</code></pre> <p>Why this matters: Hooks provide observability into agent behavior and can alert when token usage exceeds thresholds.</p>"},{"location":"agno/#3-convenience-hook-factory","title":"3. Convenience Hook Factory","text":"<p>Use <code>create_headroom_hooks()</code> to create matched hook pairs:</p> <pre><code>from headroom.integrations.agno import create_headroom_hooks\n\npre_hook, post_hook = create_headroom_hooks(\n    token_alert_threshold=5000,\n    log_level=\"DEBUG\",\n)\n\nagent = Agent(\n    model=model,\n    pre_hooks=[pre_hook],\n    post_hooks=[post_hook],\n)\n</code></pre>"},{"location":"agno/#4-custom-configuration","title":"4. Custom Configuration","text":"<p>Pass a <code>HeadroomConfig</code> for fine-grained control:</p> <pre><code>from headroom import HeadroomConfig, HeadroomMode\nfrom headroom.integrations.agno import HeadroomAgnoModel\n\nconfig = HeadroomConfig(\n    default_mode=HeadroomMode.OPTIMIZE,\n    # Add other configuration options as needed\n)\n\nmodel = HeadroomAgnoModel(\n    wrapped_model=OpenAIChat(id=\"gpt-4o\"),\n    config=config,\n)\n</code></pre>"},{"location":"agno/#5-standalone-message-optimization","title":"5. Standalone Message Optimization","text":"<p>Optimize messages without wrapping a model:</p> <pre><code>from headroom.integrations.agno import optimize_messages\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Analyze this large JSON: \" + large_json},\n]\n\noptimized_messages, metrics = optimize_messages(messages, model=\"gpt-4o\")\n\nprint(f\"Tokens saved: {metrics['tokens_saved']}\")\nprint(f\"Transforms applied: {metrics['transforms_applied']}\")\n</code></pre>"},{"location":"agno/#6-async-operations","title":"6. Async Operations","text":"<p>Full async support for high-throughput applications:</p> <pre><code>import asyncio\nfrom headroom.integrations.agno import HeadroomAgnoModel\n\nasync def process_async():\n    model = HeadroomAgnoModel(OpenAIChat(id=\"gpt-4o\"))\n\n    # Async response\n    response = await model.aresponse(messages)\n\n    # Async streaming\n    async for chunk in model.aresponse_stream(messages):\n        print(chunk, end=\"\", flush=True)\n\n    print(f\"\\nTokens saved: {model.total_tokens_saved}\")\n\nasyncio.run(process_async())\n</code></pre>"},{"location":"agno/#real-world-examples","title":"Real-World Examples","text":""},{"location":"agno/#example-1-tool-heavy-agent","title":"Example 1: Tool-Heavy Agent","text":"<pre><code>from agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom agno.tools.duckduckgo import DuckDuckGoTools\nfrom headroom.integrations.agno import HeadroomAgnoModel\n\n# Wrap model for optimization\nmodel = HeadroomAgnoModel(OpenAIChat(id=\"gpt-4o\"))\n\n# Agent with search tools\nagent = Agent(\n    model=model,\n    tools=[DuckDuckGoTools()],\n    show_tool_calls=True,\n)\n\n# Tool outputs get compressed automatically\nresponse = agent.run(\"Research the latest AI developments and summarize\")\n\n# Impact: Tool outputs (often 10K+ tokens) compressed by 70-90%\nprint(f\"Tokens saved: {model.total_tokens_saved}\")\nprint(model.get_savings_summary())\n</code></pre>"},{"location":"agno/#example-2-multi-model-routing","title":"Example 2: Multi-Model Routing","text":"<pre><code>from agno.models.openai import OpenAIChat\nfrom agno.models.anthropic import Claude\nfrom headroom.integrations.agno import HeadroomAgnoModel\n\n# Different models for different tasks\nfast_model = HeadroomAgnoModel(OpenAIChat(id=\"gpt-4o-mini\"))\npowerful_model = HeadroomAgnoModel(Claude(id=\"claude-3-5-sonnet-20241022\"))\n\n# Use fast model for simple tasks\nsimple_agent = Agent(model=fast_model)\n\n# Use powerful model for complex reasoning\ncomplex_agent = Agent(model=powerful_model)\n\n# Each tracks its own metrics\nprint(f\"Fast model saved: {fast_model.total_tokens_saved}\")\nprint(f\"Powerful model saved: {powerful_model.total_tokens_saved}\")\n</code></pre>"},{"location":"agno/#example-3-production-monitoring","title":"Example 3: Production Monitoring","text":"<pre><code>from agno.agent import Agent\nfrom headroom.integrations.agno import (\n    HeadroomAgnoModel,\n    create_headroom_hooks,\n)\n\nmodel = HeadroomAgnoModel(OpenAIChat(id=\"gpt-4o\"))\npre_hook, post_hook = create_headroom_hooks(\n    token_alert_threshold=50000,  # Alert on large requests\n    log_level=\"WARNING\",\n)\n\nagent = Agent(\n    model=model,\n    pre_hooks=[pre_hook],\n    post_hooks=[post_hook],\n)\n\n# Run multiple requests\nfor query in user_queries:\n    response = agent.run(query)\n\n# Check for alerts\nif post_hook.alerts:\n    print(f\"WARNING: {len(post_hook.alerts)} requests exceeded threshold\")\n    for alert in post_hook.alerts:\n        print(f\"  - {alert}\")\n\n# Summary stats\nsummary = post_hook.get_summary()\nprint(f\"Total requests: {summary['total_requests']}\")\nprint(f\"Average tokens: {summary['average_tokens']}\")\n</code></pre>"},{"location":"agno/#example-4-reset-for-new-sessions","title":"Example 4: Reset for New Sessions","text":"<pre><code>model = HeadroomAgnoModel(OpenAIChat(id=\"gpt-4o\"))\n\n# Session 1\nagent.run(\"First conversation...\")\nprint(f\"Session 1 savings: {model.get_savings_summary()}\")\n\n# Reset for new session\nmodel.reset()\n\n# Session 2 - metrics start fresh\nagent.run(\"Second conversation...\")\nprint(f\"Session 2 savings: {model.get_savings_summary()}\")\n</code></pre>"},{"location":"agno/#supported-providers","title":"Supported Providers","text":"<p>HeadroomAgnoModel automatically detects the provider from the wrapped model:</p> Provider Agno Models Auto-Detected OpenAI <code>OpenAIChat</code>, <code>OpenAILike</code> Yes Anthropic <code>Claude</code>, <code>AwsBedrock</code> Yes Google <code>Gemini</code>, <code>VertexAI</code> Yes Cohere <code>Cohere</code>, <code>CohereChat</code> Yes Groq <code>Groq</code> Yes (OpenAI-compatible) Mistral <code>Mistral</code> Yes (OpenAI-compatible) Together <code>Together</code> Yes (OpenAI-compatible) Ollama <code>Ollama</code> Yes (OpenAI-compatible) <p>To disable auto-detection:</p> <pre><code>model = HeadroomAgnoModel(\n    wrapped_model=some_model,\n    auto_detect_provider=False,  # Falls back to OpenAI tokenizer\n)\n</code></pre>"},{"location":"agno/#feature-coverage","title":"Feature Coverage","text":""},{"location":"agno/#whats-optimized","title":"What's Optimized","text":"<p>HeadroomAgnoModel optimizes messages at the LLM call boundary. This covers:</p> Feature Optimized Notes User/Assistant Messages \u2705 Yes Full message history compressed Tool Calls \u2705 Yes Tool call arguments optimized Tool Results \u2705 Yes JSON responses compressed 70-90% via SmartCrusher System Prompts \u2705 Yes Included in message optimization Streaming Responses \u2705 Yes Both sync and async Multi-turn Conversations \u2705 Yes Full history available for optimization"},{"location":"agno/#known-limitations","title":"Known Limitations","text":"<p>The integration operates at the model layer, not the agent layer. Some Agno features operate outside this boundary:</p> Agno Feature Status Explanation Agent Memory \u26a0\ufe0f Partial Memory content is optimized when it enters messages, but the persistent memory store itself is not compressed. If you're storing large amounts of data in agent memory, consider summarizing before storage. Knowledge Bases \u26a0\ufe0f Partial KB retrieval happens before messages reach the model. Retrieved context is optimized as part of the message, but we can't influence KB retrieval itself. Agent Teams \u274c Not supported Each agent's model is wrapped independently. No cross-agent optimization or team-level coordination. Tool Definitions \u26a0\ufe0f Not deduplicated Tool schemas are sent with every request. Future versions may deduplicate repeated tool definitions. Structured Outputs \u2705 Supported <code>response_model</code> works normally; optimization doesn't affect output parsing. Reasoning Models \u2705 Supported Extended thinking works; we don't compress reasoning traces."},{"location":"agno/#best-practices-for-maximum-savings","title":"Best Practices for Maximum Savings","text":"<ol> <li>Tool-heavy agents see the biggest wins \u2014 Tool results (JSON, logs, search results) compress 70-90%</li> <li>Long conversations benefit from RollingWindow \u2014 Configure context limits to avoid hitting provider maximums</li> <li>Wrap at the model level, not agent level \u2014 This ensures all LLM calls go through optimization</li> <li>Use hooks for observability \u2014 Track token usage patterns to identify optimization opportunities</li> </ol>"},{"location":"agno/#future-improvements","title":"Future Improvements","text":"<p>We're tracking these potential enhancements:</p> <ul> <li>Memory optimization hooks \u2014 Compress data before it enters agent memory</li> <li>Knowledge base integration \u2014 Optimize retrieved context at the KB layer</li> <li>Tool schema deduplication \u2014 Cache and reference repeated tool definitions</li> <li>Team-level optimization \u2014 Shared context compression across agent teams</li> </ul> <p>Contributions welcome! See CONTRIBUTING.md.</p>"},{"location":"agno/#configuration-reference","title":"Configuration Reference","text":""},{"location":"agno/#headroomagnomodel","title":"HeadroomAgnoModel","text":"Parameter Type Default Description <code>wrapped_model</code> Any Required The Agno model to wrap <code>config</code> <code>HeadroomConfig</code> <code>None</code> Custom configuration <code>auto_detect_provider</code> <code>bool</code> <code>True</code> Auto-detect provider for token counting <p>Properties: - <code>wrapped_model</code> - Access the underlying Agno model - <code>total_tokens_saved</code> - Running total of tokens saved - <code>metrics_history</code> - List of last 100 <code>OptimizationMetrics</code></p> <p>Methods: - <code>response(messages, **kwargs)</code> - Sync response with optimization - <code>response_stream(messages, **kwargs)</code> - Sync streaming response - <code>aresponse(messages, **kwargs)</code> - Async response - <code>aresponse_stream(messages, **kwargs)</code> - Async streaming - <code>get_savings_summary()</code> - Returns dict with stats - <code>reset()</code> - Clear all metrics</p>"},{"location":"agno/#headroomprehook","title":"HeadroomPreHook","text":"Parameter Type Default Description <code>config</code> <code>HeadroomConfig</code> <code>None</code> Configuration (for future use) <code>model</code> <code>str</code> <code>\"gpt-4o\"</code> Model name for estimation"},{"location":"agno/#headroomposthook","title":"HeadroomPostHook","text":"Parameter Type Default Description <code>log_level</code> <code>str</code> <code>\"INFO\"</code> Logging level <code>token_alert_threshold</code> <code>int</code> <code>None</code> Alert if tokens exceed this <p>Properties: - <code>total_requests</code> - Number of requests tracked - <code>alerts</code> - List of alert messages</p> <p>Methods: - <code>get_summary()</code> - Returns dict with request stats - <code>reset()</code> - Clear history and alerts</p>"},{"location":"agno/#create_headroom_hooks","title":"create_headroom_hooks()","text":"Parameter Type Default Description <code>config</code> <code>HeadroomConfig</code> <code>None</code> Config for pre-hook <code>model</code> <code>str</code> <code>\"gpt-4o\"</code> Model for pre-hook <code>log_level</code> <code>str</code> <code>\"INFO\"</code> Log level for post-hook <code>token_alert_threshold</code> <code>int</code> <code>None</code> Alert threshold for post-hook <p>Returns: <code>tuple[HeadroomPreHook, HeadroomPostHook]</code></p>"},{"location":"agno/#import-reference","title":"Import Reference","text":"<pre><code># Main integration\nfrom headroom.integrations.agno import HeadroomAgnoModel\n\n# Hooks\nfrom headroom.integrations.agno import HeadroomPreHook\nfrom headroom.integrations.agno import HeadroomPostHook\nfrom headroom.integrations.agno import create_headroom_hooks\n\n# Utilities\nfrom headroom.integrations.agno import optimize_messages\nfrom headroom.integrations.agno import agno_available\nfrom headroom.integrations.agno import get_headroom_provider\nfrom headroom.integrations.agno import get_model_name_from_agno\n\n# Or import everything from parent\nfrom headroom.integrations import (\n    HeadroomAgnoModel,\n    HeadroomPreHook,\n    HeadroomPostHook,\n    create_headroom_hooks,\n)\n</code></pre>"},{"location":"agno/#troubleshooting","title":"Troubleshooting","text":""},{"location":"agno/#check-if-agno-is-available","title":"Check if Agno is Available","text":"<pre><code>from headroom.integrations.agno import agno_available\n\nif agno_available():\n    from headroom.integrations.agno import HeadroomAgnoModel\nelse:\n    print(\"Install agno: pip install agno\")\n</code></pre>"},{"location":"agno/#provider-detection-issues","title":"Provider Detection Issues","text":"<p>If auto-detection fails, check the detected provider:</p> <pre><code>from headroom.integrations.agno import get_headroom_provider, get_model_name_from_agno\n\nmodel = OpenAIChat(id=\"gpt-4o\")\nprovider = get_headroom_provider(model)\nmodel_name = get_model_name_from_agno(model)\n\nprint(f\"Detected provider: {type(provider).__name__}\")\nprint(f\"Model name: {model_name}\")\n</code></pre>"},{"location":"agno/#metrics-not-updating","title":"Metrics Not Updating","text":"<p>Ensure you're checking the correct object:</p> <pre><code># Model metrics (optimization)\nprint(model.total_tokens_saved)  # Actual savings\n\n# Hook metrics (observability)\nprint(post_hook.get_summary())  # Request tracking\n</code></pre> <p>Note: Hooks track request counts, not token savings. Use the model wrapper for optimization metrics.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#headroomclient","title":"HeadroomClient","text":"<p>The main entry point for Headroom SDK.</p> <pre><code>from headroom import HeadroomClient\nfrom openai import OpenAI\n\nclient = HeadroomClient(\n    original_client=OpenAI(),\n    default_mode=\"optimize\",\n)\n</code></pre>"},{"location":"api/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>original_client</code> <code>OpenAI \\| Anthropic</code> Required The underlying LLM client <code>provider</code> <code>Provider</code> Auto-detected Token counting provider <code>default_mode</code> <code>str</code> <code>\"audit\"</code> Default mode: \"audit\", \"optimize\", \"off\" <code>store_url</code> <code>str</code> <code>None</code> Storage URL for metrics <code>smart_crusher_config</code> <code>SmartCrusherConfig</code> Default Compression settings <code>cache_aligner_config</code> <code>CacheAlignerConfig</code> Default Cache alignment settings <code>rolling_window_config</code> <code>RollingWindowConfig</code> Default Context window settings"},{"location":"api/#methods","title":"Methods","text":""},{"location":"api/#chatcompletionscreatekwargs","title":"<code>chat.completions.create(**kwargs)</code>","text":"<p>Create a chat completion with optional optimization.</p> <pre><code>response = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[...],\n    headroom_mode=\"optimize\",  # Override default mode\n)\n</code></pre> <p>Additional Parameters:</p> Parameter Type Description <code>headroom_mode</code> <code>str</code> Override mode for this request <code>headroom_query</code> <code>str</code> Query for relevance scoring"},{"location":"api/#chatcompletionssimulatekwargs","title":"<code>chat.completions.simulate(**kwargs)</code>","text":"<p>Preview optimization without making an API call.</p> <pre><code>plan = client.chat.completions.simulate(\n    model=\"gpt-4o\",\n    messages=[...],\n)\n\nprint(f\"Tokens before: {plan.tokens_before}\")\nprint(f\"Tokens after: {plan.tokens_after}\")\nprint(f\"Savings: {plan.savings_percent:.1f}%\")\n</code></pre> <p>Returns: <code>SimulationResult</code></p>"},{"location":"api/#configuration-classes","title":"Configuration Classes","text":""},{"location":"api/#smartcrusherconfig","title":"SmartCrusherConfig","text":"<pre><code>from headroom import SmartCrusherConfig\n\nconfig = SmartCrusherConfig(\n    min_tokens_to_crush=200,\n    max_items_after_crush=50,\n    keep_first=3,\n    keep_last=2,\n    relevance_threshold=0.3,\n    anomaly_std_threshold=2.0,\n    preserve_errors=True,\n)\n</code></pre>"},{"location":"api/#cachealignerconfig","title":"CacheAlignerConfig","text":"<pre><code>from headroom import CacheAlignerConfig\n\nconfig = CacheAlignerConfig(\n    extract_dates=True,\n    normalize_whitespace=True,\n    stable_prefix_min_tokens=100,\n)\n</code></pre>"},{"location":"api/#rollingwindowconfig","title":"RollingWindowConfig","text":"<pre><code>from headroom import RollingWindowConfig\n\nconfig = RollingWindowConfig(\n    max_tokens=100000,\n    preserve_system=True,\n    preserve_recent_turns=5,\n    drop_oldest_first=True,\n)\n</code></pre>"},{"location":"api/#intelligentcontextconfig","title":"IntelligentContextConfig","text":"<pre><code>from headroom.config import IntelligentContextConfig, ScoringWeights\n\nweights = ScoringWeights(\n    recency=0.20,\n    semantic_similarity=0.20,\n    toin_importance=0.25,\n    error_indicator=0.15,\n    forward_reference=0.15,\n    token_density=0.05,\n)\n\nconfig = IntelligentContextConfig(\n    enabled=True,\n    keep_system=True,\n    keep_last_turns=2,\n    output_buffer_tokens=4000,\n    use_importance_scoring=True,\n    scoring_weights=weights,\n    toin_integration=True,\n    recency_decay_rate=0.1,\n    compress_threshold=0.1,\n)\n</code></pre>"},{"location":"api/#scoringweights","title":"ScoringWeights","text":"<pre><code>from headroom.config import ScoringWeights\n\nweights = ScoringWeights(\n    recency=0.20,              # Exponential decay from end\n    semantic_similarity=0.20,  # Embedding similarity to recent context\n    toin_importance=0.25,      # TOIN retrieval_rate\n    error_indicator=0.15,      # TOIN field_semantics error detection\n    forward_reference=0.15,    # Messages referenced by later messages\n    token_density=0.05,        # Unique/total token ratio\n)\n\n# Weights are auto-normalized to sum to 1.0\nnormalized = weights.normalized()\n</code></pre>"},{"location":"api/#relevancescorerconfig","title":"RelevanceScorerConfig","text":"<pre><code>from headroom import RelevanceScorerConfig\n\nconfig = RelevanceScorerConfig(\n    scorer_type=\"bm25\",      # \"bm25\", \"embedding\", or \"hybrid\"\n    embedding_model=None,    # Model name for embedding scorer\n    hybrid_alpha=0.5,        # Weight for hybrid scoring\n)\n</code></pre>"},{"location":"api/#data-models","title":"Data Models","text":""},{"location":"api/#simulationresult","title":"SimulationResult","text":"<p>Returned by <code>simulate()</code>.</p> <pre><code>@dataclass\nclass SimulationResult:\n    tokens_before: int\n    tokens_after: int\n    tokens_saved: int\n    savings_percent: float\n    transforms_applied: list[str]\n    waste_signals: WasteSignals\n</code></pre>"},{"location":"api/#requestmetrics","title":"RequestMetrics","text":"<p>Metrics for a single request.</p> <pre><code>@dataclass\nclass RequestMetrics:\n    request_id: str\n    timestamp: datetime\n    model: str\n    tokens_input_before: int\n    tokens_input_after: int\n    tokens_output: int\n    cost_before: float\n    cost_after: float\n    transforms_applied: list[str]\n</code></pre>"},{"location":"api/#wastesignals","title":"WasteSignals","text":"<p>Detected waste in the request.</p> <pre><code>@dataclass\nclass WasteSignals:\n    json_bloat_tokens: int\n    html_noise_tokens: int\n    whitespace_tokens: int\n    dynamic_date_tokens: int\n    repetition_tokens: int\n</code></pre>"},{"location":"api/#providers","title":"Providers","text":""},{"location":"api/#openaiprovider","title":"OpenAIProvider","text":"<pre><code>from headroom import OpenAIProvider\n\nprovider = OpenAIProvider()\n\n# Get token counter\ncounter = provider.get_token_counter(\"gpt-4o\")\ntokens = counter.count_text(\"Hello, world!\")\n\n# Get context limit\nlimit = provider.get_context_limit(\"gpt-4o\")  # 128000\n\n# Estimate cost\ncost = provider.estimate_cost(\n    input_tokens=1000,\n    output_tokens=500,\n    model=\"gpt-4o\",\n)\n</code></pre>"},{"location":"api/#anthropicprovider","title":"AnthropicProvider","text":"<pre><code>from headroom import AnthropicProvider\nfrom anthropic import Anthropic\n\nprovider = AnthropicProvider(client=Anthropic())\n\ncounter = provider.get_token_counter(\"claude-3-5-sonnet-latest\")\ntokens = counter.count_messages(messages)  # Accurate count via API\n</code></pre>"},{"location":"api/#relevance-scoring","title":"Relevance Scoring","text":""},{"location":"api/#bm25scorer","title":"BM25Scorer","text":"<p>Fast keyword-based scoring (zero dependencies).</p> <pre><code>from headroom import BM25Scorer\n\nscorer = BM25Scorer()\nscores = scorer.score_items(\n    items=[\"item 1\", \"item 2\", ...],\n    query=\"search query\",\n)\n</code></pre>"},{"location":"api/#embeddingscorer","title":"EmbeddingScorer","text":"<p>Semantic similarity scoring (requires <code>sentence-transformers</code>).</p> <pre><code>from headroom import EmbeddingScorer, embedding_available\n\nif embedding_available():\n    scorer = EmbeddingScorer(model=\"all-MiniLM-L6-v2\")\n    scores = scorer.score_items(items, query)\n</code></pre>"},{"location":"api/#hybridscorer","title":"HybridScorer","text":"<p>Combines BM25 and embeddings.</p> <pre><code>from headroom import HybridScorer\n\nscorer = HybridScorer(alpha=0.5)  # 50% BM25, 50% embedding\nscores = scorer.score_items(items, query)\n</code></pre>"},{"location":"api/#create_scorer","title":"create_scorer()","text":"<p>Factory function to create scorers.</p> <pre><code>from headroom import create_scorer\n\n# Auto-select best available scorer\nscorer = create_scorer()\n\n# Explicitly choose type\nscorer = create_scorer(scorer_type=\"hybrid\", alpha=0.7)\n</code></pre>"},{"location":"api/#transforms-direct-use","title":"Transforms (Direct Use)","text":""},{"location":"api/#smartcrusher","title":"SmartCrusher","text":"<pre><code>from headroom import SmartCrusher\n\ncrusher = SmartCrusher()\nresult = crusher.crush(\n    data={\"results\": [...]},\n    query=\"user query\",\n)\n</code></pre>"},{"location":"api/#cachealigner","title":"CacheAligner","text":"<pre><code>from headroom import CacheAligner\n\naligner = CacheAligner()\nresult = aligner.align(messages)\n</code></pre>"},{"location":"api/#rollingwindow","title":"RollingWindow","text":"<pre><code>from headroom import RollingWindow\n\nwindow = RollingWindow(config)\nresult = window.apply(messages, max_tokens=100000)\n</code></pre>"},{"location":"api/#intelligentcontextmanager","title":"IntelligentContextManager","text":"<pre><code>from headroom.transforms import IntelligentContextManager\nfrom headroom.config import IntelligentContextConfig\nfrom headroom.telemetry import get_toin\n\n# With TOIN integration for learned patterns\ntoin = get_toin()\nconfig = IntelligentContextConfig(\n    keep_system=True,\n    keep_last_turns=2,\n    use_importance_scoring=True,\n)\n\nmanager = IntelligentContextManager(config, toin=toin)\nresult = manager.apply(messages, tokenizer, model_limit=128000)\n\n# Access scoring details\nprint(result.transforms_applied)  # [\"intelligent_cap:3\"]\nprint(result.tokens_before, result.tokens_after)\n</code></pre>"},{"location":"api/#messagescorer","title":"MessageScorer","text":"<pre><code>from headroom.transforms import MessageScorer, MessageScore\nfrom headroom.config import ScoringWeights\n\nscorer = MessageScorer(\n    weights=ScoringWeights(),\n    toin=None,  # Optional TOIN for learned patterns\n    embedding_provider=None,  # Optional for semantic similarity\n    recency_decay_rate=0.1,\n)\n\n# Score messages\nscores: list[MessageScore] = scorer.score_messages(\n    messages=messages,\n    protected_indices={0},  # System message\n    tool_unit_indices={2, 3},  # Tool call + response\n)\n\nfor score in scores:\n    print(f\"Message {score.message_index}: {score.total_score:.2f}\")\n    print(f\"  Recency: {score.recency_score:.2f}\")\n    print(f\"  TOIN: {score.toin_score:.2f}\")\n    print(f\"  Protected: {score.is_protected}\")\n</code></pre>"},{"location":"api/#transformpipeline","title":"TransformPipeline","text":"<pre><code>from headroom import TransformPipeline\n\npipeline = TransformPipeline([\n    SmartCrusher(),\n    CacheAligner(),\n    RollingWindow(),\n])\n\nresult = pipeline.transform(messages)\n</code></pre>"},{"location":"api/#utilities","title":"Utilities","text":""},{"location":"api/#tokenizer","title":"Tokenizer","text":"<pre><code>from headroom import Tokenizer, count_tokens_text, count_tokens_messages\n\n# Quick counting\ntokens = count_tokens_text(\"Hello, world!\", model=\"gpt-4o\")\n\n# With tokenizer instance\ntokenizer = Tokenizer(model=\"gpt-4o\")\ntokens = tokenizer.count_text(\"Hello\")\ntokens = tokenizer.count_messages(messages)\n</code></pre>"},{"location":"api/#generate_report","title":"generate_report()","text":"<p>Generate HTML/Markdown reports from stored metrics.</p> <pre><code>from headroom import generate_report\n\nreport = generate_report(\n    store_url=\"sqlite:///headroom.db\",\n    format=\"html\",\n    period=\"day\",\n)\n</code></pre>"},{"location":"benchmarks/","title":"Accuracy Benchmarks","text":"<p>Headroom's core promise: compress context without losing accuracy. This page shows our latest benchmark results against established open-source datasets.</p> <p>Key Result</p> <p>98.2% recall on article extraction with 94.9% compression \u2014 we preserve nearly all information while dramatically reducing tokens.</p>"},{"location":"benchmarks/#summary","title":"Summary","text":"Benchmark Metric Headroom Baseline Status Scrapinghub Article Extraction F1 Score 0.919 0.958 :white_check_mark: Scrapinghub Article Extraction Recall 98.2% \u2014 :white_check_mark: Scrapinghub Article Extraction Compression 94.9% \u2014 :white_check_mark: SmartCrusher (JSON) Accuracy 100% \u2014 :white_check_mark: SmartCrusher (JSON) Compression 87.6% \u2014 :white_check_mark:"},{"location":"benchmarks/#html-extraction","title":"HTML Extraction","text":"<p>Dataset: Scrapinghub Article Extraction Benchmark Samples: 181 HTML pages with ground truth article bodies Baseline: trafilatura (0.958 F1)</p> <p>HTMLExtractor removes scripts, styles, navigation, ads, and boilerplate while preserving article content.</p>"},{"location":"benchmarks/#results","title":"Results","text":"Metric Value Description F1 Score 0.919 Token-level overlap with ground truth Precision 0.879 Proportion of extracted content that's relevant Recall 0.982 Proportion of ground truth content captured Compression 94.9% Average size reduction"},{"location":"benchmarks/#why-recall-matters-most","title":"Why Recall Matters Most","text":"<p>For LLM applications, recall is critical \u2014 we must capture all relevant information. A 98.2% recall means:</p> <ul> <li>Nearly all article content is preserved</li> <li>LLMs can answer questions accurately from extracted content</li> <li>The slight precision drop (some extra content) doesn't hurt LLM accuracy</li> </ul>"},{"location":"benchmarks/#run-it-yourself","title":"Run It Yourself","text":"<pre><code># Install dependencies\npip install \"headroom-ai[html]\" datasets\n\n# Run the benchmark\npytest tests/test_evals/test_html_oss_benchmarks.py::TestExtractionBenchmark -v -s\n</code></pre>"},{"location":"benchmarks/#json-compression-smartcrusher","title":"JSON Compression (SmartCrusher)","text":"<p>Test: 100 production log entries with critical error at position 67 Task: Find the error, error code, resolution, and affected count</p>"},{"location":"benchmarks/#results_1","title":"Results","text":"Metric Baseline Headroom Input tokens 10,144 1,260 Correct answers 4/4 4/4 Compression \u2014 87.6% <p>SmartCrusher preserves:</p> <ul> <li>First N items (schema examples)</li> <li>Last N items (recency)</li> <li>All anomalies (errors, warnings, outliers)</li> <li>Statistical distribution</li> </ul>"},{"location":"benchmarks/#run-it-yourself_1","title":"Run It Yourself","text":"<pre><code>python examples/needle_in_haystack_test.py\n</code></pre>"},{"location":"benchmarks/#qa-accuracy-preservation","title":"QA Accuracy Preservation","text":"<p>We verify that LLMs can answer questions equally well from compressed content.</p> <p>Method: 1. Take original HTML content 2. Extract with HTMLExtractor 3. Ask LLM same question on both 4. Compare answers against ground truth</p> <p>Datasets: SQuAD v2, HotpotQA</p>"},{"location":"benchmarks/#results_2","title":"Results","text":"Metric Original HTML Extracted Delta F1 Score 0.85 0.87 +0.02 Exact Match 60% 62% +2% <p>Extraction Can Improve Accuracy</p> <p>Removing HTML noise sometimes helps LLMs focus on relevant content.</p>"},{"location":"benchmarks/#run-it-yourself_2","title":"Run It Yourself","text":"<pre><code># Requires OPENAI_API_KEY\npytest tests/test_evals/test_html_oss_benchmarks.py::TestQAAccuracyPreservation -v -s\n</code></pre>"},{"location":"benchmarks/#multi-tool-agent-test","title":"Multi-Tool Agent Test","text":"<p>Setup: Agno agent with 4 tools investigating a memory leak Total tool output: 62,323 chars (~15,580 tokens)</p>"},{"location":"benchmarks/#results_3","title":"Results","text":"Metric Baseline Headroom Tokens sent 15,662 6,100 Tool calls 4 4 Correct findings All All Compression \u2014 76.3% <p>Both found: Issue #42, <code>cleanup_worker()</code> fix, OutOfMemoryError logs, relevant papers.</p>"},{"location":"benchmarks/#run-it-yourself_3","title":"Run It Yourself","text":"<pre><code>python examples/multi_tool_agent_test.py\n</code></pre>"},{"location":"benchmarks/#methodology","title":"Methodology","text":""},{"location":"benchmarks/#token-level-f1","title":"Token-Level F1","text":"<p>We use the standard NLP metric for text overlap:</p> <pre><code>Precision = |predicted \u2229 ground_truth| / |predicted|\nRecall = |predicted \u2229 ground_truth| / |ground_truth|\nF1 = 2 * (Precision * Recall) / (Precision + Recall)\n</code></pre>"},{"location":"benchmarks/#qa-accuracy","title":"QA Accuracy","text":"<p>For question-answering, we measure:</p> <ul> <li>Exact Match: Normalized answer strings match exactly</li> <li>F1 Score: Token overlap between predicted and ground truth answers</li> </ul>"},{"location":"benchmarks/#compression-ratio","title":"Compression Ratio","text":"<pre><code>Compression = 1 - (compressed_size / original_size)\n</code></pre> <p>A 94.9% compression means the output is 5.1% of the original size.</p>"},{"location":"benchmarks/#reproducing-results","title":"Reproducing Results","text":"<p>All benchmarks are reproducible:</p> <pre><code># Clone the repo\ngit clone https://github.com/chopratejas/headroom.git\ncd headroom\n\n# Install with eval dependencies\npip install -e \".[evals,html]\"\n\n# Run all benchmarks\npytest tests/test_evals/ -v -s\n\n# Run specific benchmark\npytest tests/test_evals/test_html_oss_benchmarks.py -v -s\n</code></pre>"},{"location":"benchmarks/#ci-integration","title":"CI Integration","text":"<p>Benchmarks run on every PR. See .github/workflows/ci.yml.</p>"},{"location":"benchmarks/#adding-new-benchmarks","title":"Adding New Benchmarks","text":"<p>We welcome contributions! See CONTRIBUTING.md for guidelines.</p> <p>Benchmarks should:</p> <ol> <li>Use established open-source datasets</li> <li>Include reproducible evaluation code</li> <li>Test accuracy preservation, not just compression</li> <li>Run in CI without API keys (or skip gracefully)</li> </ol>"},{"location":"ccr/","title":"CCR: Compress-Cache-Retrieve","text":"<p>Headroom's CCR architecture makes compression reversible. When content is compressed, the original data is cached. If the LLM needs more data, it can retrieve it instantly.</p>"},{"location":"ccr/#the-problem-with-traditional-compression","title":"The Problem with Traditional Compression","text":"<p>Traditional compression is lossy \u2014 if you guess wrong about what's important, data is lost forever. This creates a difficult tradeoff:</p> <ul> <li>Aggressive compression: Risk losing data the LLM needs</li> <li>Conservative compression: Miss out on token savings</li> </ul> <p>CCR eliminates this tradeoff.</p>"},{"location":"ccr/#ccr-enabled-components","title":"CCR-Enabled Components","text":"Component What it compresses CCR integration SmartCrusher JSON arrays (tool outputs) Stores original array, marker includes hash ContentRouter Code, logs, search results, text Stores original content by strategy IntelligentContextManager Messages (conversation turns) Stores dropped messages, marker includes hash"},{"location":"ccr/#how-ccr-works","title":"How CCR Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TOOL OUTPUT (1000 items)                                        \u2502\n\u2502  \u2514\u2500 SmartCrusher compresses to 20 items                         \u2502\n\u2502  \u2514\u2500 Original cached with hash=abc123                            \u2502\n\u2502  \u2514\u2500 Retrieval tool injected into context                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LLM PROCESSING                                                  \u2502\n\u2502  Option A: LLM solves task with 20 items \u2192 Done (90% savings)   \u2502\n\u2502  Option B: LLM calls headroom_retrieve(hash=abc123)             \u2502\n\u2502            \u2192 Response Handler executes retrieval automatically  \u2502\n\u2502            \u2192 LLM receives full data, responds accurately        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ccr/#phase-1-compression-store","title":"Phase 1: Compression Store","text":"<p>When SmartCrusher compresses tool output: 1. Original content is stored in an LRU cache 2. A hash key is generated for retrieval 3. A marker is added to the compressed output: <code>[1000 items compressed to 20. Retrieve more: hash=abc123]</code></p>"},{"location":"ccr/#phase-2-tool-injection","title":"Phase 2: Tool Injection","text":"<p>Headroom injects a <code>headroom_retrieve</code> tool into the LLM's available tools:</p> <pre><code>{\n  \"name\": \"headroom_retrieve\",\n  \"description\": \"Retrieve original uncompressed data from Headroom cache\",\n  \"parameters\": {\n    \"hash\": \"The hash key from the compression marker\",\n    \"query\": \"Optional: search within the cached data\"\n  }\n}\n</code></pre>"},{"location":"ccr/#phase-3-response-handler","title":"Phase 3: Response Handler","text":"<p>When the LLM calls <code>headroom_retrieve</code>: 1. Response Handler intercepts the tool call 2. Retrieves data from the local cache (~1ms) 3. Adds the result to the conversation 4. Continues the API call automatically</p> <p>The client never sees CCR tool calls \u2014 they're handled transparently.</p>"},{"location":"ccr/#phase-4-context-tracker","title":"Phase 4: Context Tracker","text":"<p>Across multiple turns, the Context Tracker: 1. Remembers what was compressed in earlier turns 2. Analyzes new queries for relevance to compressed content 3. Proactively expands relevant data before the LLM asks</p> <p>Example: <pre><code>Turn 1: User searches for files\n        \u2192 Tool returns 500 files\n        \u2192 SmartCrusher compresses to 15, caches original (hash=abc123)\n        \u2192 LLM sees 15 files, answers question\n\nTurn 5: User asks \"What about the auth middleware?\"\n        \u2192 Context Tracker detects \"auth\" might be in abc123\n        \u2192 Proactively expands compressed content\n        \u2192 LLM sees full file list, finds auth_middleware.py\n</code></pre></p>"},{"location":"ccr/#message-level-ccr-intelligentcontext","title":"Message-Level CCR (IntelligentContext)","text":"<p>IntelligentContextManager is a message-level compressor. When it drops low-importance messages to fit the context budget, those messages are stored in CCR:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LONG CONVERSATION (100 messages, 50K tokens)                    \u2502\n\u2502  \u2514\u2500 IntelligentContext scores messages by importance            \u2502\n\u2502  \u2514\u2500 Drops 60 low-scoring messages                               \u2502\n\u2502  \u2514\u2500 Dropped messages cached with hash=def456                    \u2502\n\u2502  \u2514\u2500 Marker inserted: \"60 messages dropped, retrieve: def456\"    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LLM PROCESSING                                                  \u2502\n\u2502  Option A: LLM solves task with remaining messages \u2192 Done       \u2502\n\u2502  Option B: LLM needs earlier context                            \u2502\n\u2502            \u2192 Calls headroom_retrieve(hash=def456)               \u2502\n\u2502            \u2192 Full conversation restored                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The marker includes the CCR reference: <pre><code>[Earlier context compressed: 60 message(s) dropped by importance scoring.\nFull content available via ccr_retrieve tool with reference 'def456'.]\n</code></pre></p> <p>TOIN integration: When users retrieve dropped messages, TOIN learns to score those message patterns higher next time, improving future drop decisions across all users.</p>"},{"location":"ccr/#features","title":"Features","text":"Feature Description Automatic Response Handling When LLM calls <code>headroom_retrieve</code>, the proxy handles it automatically Multi-Turn Context Tracking Tracks compressed content across turns, proactively expands when relevant BM25 Search LLM can search within compressed data: <code>headroom_retrieve(hash, query=\"errors\")</code> Feedback Learning Learns from retrieval patterns to improve future compression"},{"location":"ccr/#configuration","title":"Configuration","text":"<pre><code># Proxy with CCR enabled (default)\nheadroom proxy --port 8787\n\n# Disable CCR response handling\nheadroom proxy --no-ccr-responses\n\n# Disable proactive expansion\nheadroom proxy --no-ccr-expansion\n</code></pre>"},{"location":"ccr/#why-this-matters","title":"Why This Matters","text":"Approach Risk Savings No compression None 0% Traditional compression Data loss 70-90% CCR compression None (reversible) 70-90% <p>CCR gives you the savings of aggressive compression with zero risk \u2014 the LLM can always retrieve the original data if needed.</p>"},{"location":"ccr/#demo","title":"Demo","text":"<p>Run the CCR demonstration to see it in action:</p> <pre><code>python examples/ccr_demo.py\n</code></pre> <p>Output: <pre><code>1. COMPRESSION STORE\n   Original: 100 items (7,059 chars)\n   Compressed: 8 items (633 chars)\n   Reduction: 91.0%\n\n3. RESPONSE HANDLER\n   Detected CCR tool call: True\n   Retrieved 100 items automatically\n\n4. CONTEXT TRACKER\n   Turn 5: User asks \"show authentication middleware\"\n   Tracker found 1 relevant context\n   \u2192 relevance=0.73\n   Proactively expanded: 100 items\n</code></pre></p>"},{"location":"ccr/#architecture","title":"Architecture","text":"<p>For implementation details, see ARCHITECTURE.md.</p>"},{"location":"compression/","title":"Universal Compression","text":"<p>Headroom's Universal Compression module provides intelligent, automatic compression with ML-based content detection and structure preservation.</p>"},{"location":"compression/#overview","title":"Overview","text":"<p>Universal Compression combines several techniques:</p> <ol> <li>ML-based Detection - Automatically detects content type (JSON, code, logs, text) using Magika</li> <li>Structure Preservation - Keeps keys, signatures, and templates intact via structure masks</li> <li>Intelligent Compression - Compresses content while preserving meaning with LLMLingua</li> <li>Reversible via CCR - Stores originals for retrieval when LLM needs full context</li> </ol>"},{"location":"compression/#quick-start","title":"Quick Start","text":""},{"location":"compression/#one-liner","title":"One-Liner","text":"<pre><code>from headroom.compression import compress\n\nresult = compress(content)\nprint(result.compressed)\nprint(f\"Saved {result.savings_percentage:.0f}% tokens\")\n</code></pre>"},{"location":"compression/#with-configuration","title":"With Configuration","text":"<pre><code>from headroom.compression import UniversalCompressor, UniversalCompressorConfig\n\nconfig = UniversalCompressorConfig(\n    compression_ratio_target=0.5,  # Keep 50% of content\n    use_entropy_preservation=True,  # Preserve UUIDs, hashes\n)\n\ncompressor = UniversalCompressor(config=config)\nresult = compressor.compress(content)\n</code></pre>"},{"location":"compression/#how-it-works","title":"How It Works","text":""},{"location":"compression/#detection-flow","title":"Detection Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Content   \u2502\u2500\u2500\u2500&gt;\u2502   Detect    \u2502\u2500\u2500\u2500&gt;\u2502   Extract   \u2502\u2500\u2500\u2500&gt;\u2502  Compress   \u2502\n\u2502   Input     \u2502    \u2502   Type      \u2502    \u2502   Structure \u2502    \u2502  Content    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502                   \u2502                   \u2502\n                         \u25bc                   \u25bc                   \u25bc\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502   Magika    \u2502    \u2502   Handler   \u2502    \u2502  LLMLingua  \u2502\n                   \u2502   (ML)      \u2502    \u2502   (JSON,    \u2502    \u2502  (optional) \u2502\n                   \u2502             \u2502    \u2502   Code...)  \u2502    \u2502             \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"compression/#structure-masks","title":"Structure Masks","text":"<p>Structure masks identify what to preserve:</p> Content Type What's Preserved What's Compressed JSON Keys, brackets, booleans, nulls, short values, UUIDs Long string values, whitespace Code Imports, function signatures, class definitions, types Function bodies, comments Logs Timestamps, log levels, error messages Repeated patterns, verbose details Text High-entropy tokens (IDs, hashes) Low-information content"},{"location":"compression/#configuration","title":"Configuration","text":""},{"location":"compression/#universalcompressorconfig","title":"UniversalCompressorConfig","text":"<pre><code>from headroom.compression import UniversalCompressorConfig\n\nconfig = UniversalCompressorConfig(\n    # Detection\n    use_magika=True,               # Use ML-based detection (requires magika)\n\n    # Compression\n    use_llmlingua=True,            # Use LLMLingua for compression\n    compression_ratio_target=0.3,  # Keep 30% of content (70% reduction)\n    min_content_length=100,        # Skip content shorter than this\n\n    # Structure preservation\n    use_entropy_preservation=True, # Preserve high-entropy tokens\n    entropy_threshold=0.85,        # Entropy threshold for preservation\n\n    # CCR\n    ccr_enabled=True,              # Store originals for retrieval\n)\n</code></pre>"},{"location":"compression/#configuration-options","title":"Configuration Options","text":"Option Default Description <code>use_magika</code> <code>True</code> Use ML-based content detection <code>use_llmlingua</code> <code>True</code> Use LLMLingua for compression <code>compression_ratio_target</code> <code>0.3</code> Target ratio (0.3 = keep 30%) <code>min_content_length</code> <code>100</code> Minimum chars to compress <code>use_entropy_preservation</code> <code>True</code> Preserve high-entropy tokens <code>entropy_threshold</code> <code>0.85</code> Entropy threshold (0.0-1.0) <code>ccr_enabled</code> <code>True</code> Enable CCR storage"},{"location":"compression/#content-handlers","title":"Content Handlers","text":""},{"location":"compression/#json-handler","title":"JSON Handler","text":"<p>Preserves JSON structure while compressing values:</p> <pre><code>from headroom.compression.handlers.json_handler import JSONStructureHandler\n\nhandler = JSONStructureHandler(\n    preserve_short_values=True,     # Keep values &lt; 20 chars\n    short_value_threshold=20,       # Threshold for \"short\"\n    preserve_high_entropy=True,     # Keep UUIDs, hashes\n    entropy_threshold=0.85,         # Entropy threshold\n    max_array_items_full=3,         # Keep first N array items full\n    max_number_digits=10,           # Preserve numbers up to N digits\n)\n</code></pre> <p>What's Preserved: - All keys (navigational - LLM sees schema) - Structural syntax (<code>{</code>, <code>}</code>, <code>[</code>, <code>]</code>, <code>:</code>, <code>,</code>) - Booleans and nulls (semantically important) - High-entropy strings (UUIDs, hashes - identifiers) - Short numbers (often IDs)</p> <p>Example:</p> <pre><code># Before\n{\n    \"id\": \"usr_abc123\",\n    \"name\": \"Alice Johnson\",\n    \"bio\": \"A long description that goes on and on...\"\n}\n\n# After (structure preserved, long values compressed)\n{\n    \"id\": \"usr_abc123\",\n    \"name\": \"Alice Johnson\",\n    \"bio\": \"A long...[compressed]...\"\n}\n</code></pre>"},{"location":"compression/#code-handler","title":"Code Handler","text":"<p>Preserves code structure using AST parsing (tree-sitter) or regex fallback:</p> <pre><code>from headroom.compression.handlers.code_handler import CodeStructureHandler\n\nhandler = CodeStructureHandler(\n    preserve_comments=False,        # Preserve comments as structural\n    use_tree_sitter=True,           # Use tree-sitter for parsing\n    default_language=\"python\",      # Default when detection fails\n)\n</code></pre> <p>What's Preserved: - Import statements - Function/method signatures - Class definitions - Type annotations - Decorators</p> <p>What's Compressed: - Function bodies (implementations) - Comments (unless <code>preserve_comments=True</code>)</p> <p>Example:</p> <pre><code># Before\ndef process_data(items: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Process items and count occurrences.\"\"\"\n    result = {}\n    for item in items:\n        item = item.strip().lower()\n        if item in result:\n            result[item] += 1\n        else:\n            result[item] = 1\n    return result\n\n# After (signature preserved, body compressed)\ndef process_data(items: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Process items and count occurrences.\"\"\"\n    result = {}\n    for item in items:\n    ...[compressed]...\n</code></pre>"},{"location":"compression/#supported-languages","title":"Supported Languages","text":"Language Parser Support Level Python tree-sitter Full AST JavaScript tree-sitter Full AST TypeScript tree-sitter Full AST Go tree-sitter Full AST Rust tree-sitter Full AST Java tree-sitter Full AST C tree-sitter Full AST C++ tree-sitter Full AST"},{"location":"compression/#compression-result","title":"Compression Result","text":"<pre><code>from headroom.compression import compress\n\nresult = compress(content)\n\n# Access result fields\nprint(result.compressed)           # Compressed content\nprint(result.original)             # Original content\nprint(result.compression_ratio)    # e.g., 0.35 (35% of original size)\nprint(result.tokens_before)        # Estimated tokens before\nprint(result.tokens_after)         # Estimated tokens after\nprint(result.tokens_saved)         # tokens_before - tokens_after\nprint(result.savings_percentage)   # e.g., 65.0 (65% savings)\n\n# Detection info\nprint(result.content_type)         # ContentType.JSON, CODE, etc.\nprint(result.detection_confidence) # 0.0-1.0\n\n# Structure info\nprint(result.handler_used)         # \"json\", \"code\", etc.\nprint(result.preservation_ratio)   # Fraction preserved as structure\n\n# CCR info\nprint(result.ccr_key)              # Key for retrieval (if CCR enabled)\n</code></pre>"},{"location":"compression/#batch-compression","title":"Batch Compression","text":"<p>For multiple contents, batch compression is more efficient:</p> <pre><code>from headroom.compression import UniversalCompressor\n\ncompressor = UniversalCompressor()\n\ncontents = [\n    '{\"users\": [...]}',\n    'def hello(): pass',\n    'Plain text content',\n]\n\nresults = compressor.compress_batch(contents)\n\nfor result in results:\n    print(f\"{result.content_type}: {result.savings_percentage:.0f}% saved\")\n</code></pre>"},{"location":"compression/#custom-handlers","title":"Custom Handlers","text":"<p>Register custom handlers for specific content types:</p> <pre><code>from headroom.compression import UniversalCompressor\nfrom headroom.compression.detector import ContentType\nfrom headroom.compression.handlers.base import BaseStructureHandler, HandlerResult\nfrom headroom.compression.masks import StructureMask\n\n\nclass LogStructureHandler(BaseStructureHandler):\n    \"\"\"Custom handler for log content.\"\"\"\n\n    def __init__(self):\n        super().__init__(name=\"log\")\n\n    def can_handle(self, content: str) -&gt; bool:\n        return \"[INFO]\" in content or \"[ERROR]\" in content\n\n    def _extract_mask(self, content, tokens, **kwargs):\n        # Mark timestamps and log levels as structural\n        mask = [False] * len(content)\n        # ... (custom logic)\n        return HandlerResult(\n            mask=StructureMask(tokens=tokens, mask=mask),\n            handler_name=self.name,\n            confidence=0.9,\n        )\n\n\n# Register the custom handler\ncompressor = UniversalCompressor()\ncompressor.register_handler(ContentType.TEXT, LogStructureHandler())\n</code></pre>"},{"location":"compression/#ccr-integration","title":"CCR Integration","text":"<p>Universal Compression integrates with CCR (Compress-Cache-Retrieve) for reversible compression:</p> <pre><code>from headroom.compression import UniversalCompressor, UniversalCompressorConfig\n\nconfig = UniversalCompressorConfig(ccr_enabled=True)\ncompressor = UniversalCompressor(config=config)\n\nresult = compressor.compress(large_content)\n\n# CCR key for retrieval\nif result.ccr_key:\n    print(f\"Original stored with key: {result.ccr_key}\")\n    # LLM can request original via CCR when needed\n</code></pre> <p>See CCR Guide for full CCR documentation.</p>"},{"location":"compression/#performance","title":"Performance","text":"Content Type Compression Speed Accuracy JSON (large arrays) 70-90% ~1ms Keys preserved Code (Python) 50-70% ~10ms Signatures preserved Plain text 60-80% ~5ms High-entropy preserved <p>Overhead: ~1-10ms per compression depending on content size and type.</p>"},{"location":"compression/#installation","title":"Installation","text":"<pre><code># Basic compression (fallback to simple compression)\npip install headroom-ai\n\n# With ML detection (recommended)\npip install \"headroom-ai[magika]\"\n\n# With LLMLingua compression\npip install \"headroom-ai[llmlingua]\"\n\n# With AST-based code handling\npip install \"headroom-ai[code]\"\n\n# Everything\npip install \"headroom-ai[all]\"\n</code></pre>"},{"location":"compression/#example-full-pipeline","title":"Example: Full Pipeline","text":"<pre><code>from headroom.compression import UniversalCompressor, UniversalCompressorConfig\n\n# Configure for aggressive compression\nconfig = UniversalCompressorConfig(\n    compression_ratio_target=0.25,  # Keep 25%\n    use_magika=True,\n    use_llmlingua=True,\n    ccr_enabled=True,\n)\n\ncompressor = UniversalCompressor(config=config)\n\n# Compress JSON API response\njson_content = \"\"\"\n{\n    \"users\": [\n        {\"id\": \"usr_123\", \"name\": \"Alice\", \"bio\": \"Software engineer...\"},\n        {\"id\": \"usr_456\", \"name\": \"Bob\", \"bio\": \"Product manager...\"}\n    ],\n    \"total\": 2,\n    \"page\": 1\n}\n\"\"\"\n\nresult = compressor.compress(json_content)\n\nprint(f\"Type: {result.content_type}\")          # ContentType.JSON\nprint(f\"Handler: {result.handler_used}\")        # json\nprint(f\"Saved: {result.savings_percentage:.0f}%\")  # ~60%\nprint(f\"Structure: {result.preservation_ratio:.0%} preserved\")  # ~40%\nprint(f\"CCR Key: {result.ccr_key}\")             # For retrieval\n</code></pre>"},{"location":"compression/#see-also","title":"See Also","text":"<ul> <li>Transforms Reference - Other compression transforms</li> <li>CCR Guide - Reversible compression architecture</li> <li>Text Compression - Opt-in utilities for search/logs</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>Headroom can be configured via the SDK, proxy command line, or per-request overrides.</p>"},{"location":"configuration/#sdk-configuration","title":"SDK Configuration","text":"<pre><code>from headroom import HeadroomClient, OpenAIProvider\nfrom openai import OpenAI\n\nclient = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n\n    # Mode: \"audit\" (observe only) or \"optimize\" (apply transforms)\n    default_mode=\"optimize\",\n\n    # Enable provider-specific cache optimization\n    enable_cache_optimizer=True,\n\n    # Enable query-level semantic caching\n    enable_semantic_cache=False,\n\n    # Override default context limits per model\n    model_context_limits={\n        \"gpt-4o\": 128000,\n        \"gpt-4o-mini\": 128000,\n    },\n\n    # Database location (defaults to temp directory)\n    # store_url=\"sqlite:////absolute/path/to/headroom.db\",\n)\n</code></pre>"},{"location":"configuration/#proxy-configuration","title":"Proxy Configuration","text":""},{"location":"configuration/#command-line-options","title":"Command Line Options","text":"<pre><code>headroom proxy \\\n  --port 8787 \\              # Port to listen on\n  --host 0.0.0.0 \\           # Host to bind to\n  --budget 10.00 \\           # Daily budget limit in USD\n  --log-file headroom.jsonl  # Log file path\n</code></pre>"},{"location":"configuration/#feature-flags","title":"Feature Flags","text":"<pre><code># Disable optimization (passthrough mode)\nheadroom proxy --no-optimize\n\n# Disable semantic caching\nheadroom proxy --no-cache\n\n# Disable CCR response handling\nheadroom proxy --no-ccr-responses\n\n# Disable proactive expansion\nheadroom proxy --no-ccr-expansion\n\n# Enable LLMLingua ML compression\nheadroom proxy --llmlingua\nheadroom proxy --llmlingua --llmlingua-device cuda --llmlingua-rate 0.4\n</code></pre>"},{"location":"configuration/#all-options","title":"All Options","text":"<pre><code>headroom proxy --help\n</code></pre>"},{"location":"configuration/#per-request-overrides","title":"Per-Request Overrides","text":"<p>Override configuration for specific requests:</p> <pre><code>response = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[...],\n\n    # Override mode for this request\n    headroom_mode=\"audit\",\n\n    # Reserve more tokens for output\n    headroom_output_buffer_tokens=8000,\n\n    # Keep last N turns (don't compress)\n    headroom_keep_turns=5,\n\n    # Skip compression for specific tools\n    headroom_tool_profiles={\n        \"important_tool\": {\"skip_compression\": True}\n    }\n)\n</code></pre>"},{"location":"configuration/#modes","title":"Modes","text":"Mode Behavior Use Case <code>audit</code> Observes and logs, no modifications Production monitoring, baseline measurement <code>optimize</code> Applies safe, deterministic transforms Production optimization <code>simulate</code> Returns plan without API call Testing, cost estimation"},{"location":"configuration/#simulate-mode","title":"Simulate Mode","text":"<p>Preview what would happen without making an API call:</p> <pre><code>plan = client.chat.completions.simulate(\n    model=\"gpt-4o\",\n    messages=large_conversation,\n)\n\nprint(f\"Would save {plan.tokens_saved} tokens\")\nprint(f\"Transforms: {plan.transforms}\")\nprint(f\"Estimated savings: {plan.estimated_savings}\")\n</code></pre>"},{"location":"configuration/#smartcrusher-configuration","title":"SmartCrusher Configuration","text":"<p>Fine-tune JSON compression behavior:</p> <pre><code>from headroom.transforms import SmartCrusherConfig\n\nconfig = SmartCrusherConfig(\n    # Maximum items to keep after compression\n    max_items_after_crush=15,\n\n    # Minimum tokens before applying compression\n    min_tokens_to_crush=200,\n\n    # Relevance scoring tier: \"bm25\" (fast) or \"embedding\" (accurate)\n    relevance_tier=\"bm25\",\n\n    # Always keep items with these field values\n    preserve_fields=[\"error\", \"warning\", \"failure\"],\n)\n</code></pre>"},{"location":"configuration/#cache-aligner-configuration","title":"Cache Aligner Configuration","text":"<p>Control prefix stabilization:</p> <pre><code>from headroom.transforms import CacheAlignerConfig\n\nconfig = CacheAlignerConfig(\n    # Enable/disable cache alignment\n    enabled=True,\n\n    # Patterns to extract from system prompt\n    dynamic_patterns=[\n        r\"Today is \\w+ \\d+, \\d{4}\",\n        r\"Current time: .*\",\n    ],\n)\n</code></pre>"},{"location":"configuration/#rolling-window-configuration","title":"Rolling Window Configuration","text":"<p>Control context window management:</p> <pre><code>from headroom.transforms import RollingWindowConfig\n\nconfig = RollingWindowConfig(\n    # Minimum turns to always keep\n    min_keep_turns=3,\n\n    # Reserve tokens for output\n    output_buffer_tokens=4000,\n\n    # Drop oldest tool outputs first\n    prefer_drop_tool_outputs=True,\n)\n</code></pre>"},{"location":"configuration/#intelligent-context-manager-configuration","title":"Intelligent Context Manager Configuration","text":"<p>For semantic-aware context management with importance scoring:</p> <pre><code>from headroom.config import IntelligentContextConfig, ScoringWeights\n\n# Customize scoring weights (must sum to 1.0, or will be normalized)\nweights = ScoringWeights(\n    recency=0.20,              # Newer messages score higher\n    semantic_similarity=0.20,  # Similarity to recent context\n    toin_importance=0.25,      # TOIN-learned retrieval patterns\n    error_indicator=0.15,      # TOIN-learned error field types\n    forward_reference=0.15,    # Messages referenced by later messages\n    token_density=0.05,        # Information density\n)\n\nconfig = IntelligentContextConfig(\n    # Enable/disable the manager\n    enabled=True,\n\n    # Protection settings\n    keep_system=True,           # Never drop system messages\n    keep_last_turns=2,          # Protect last N user turns\n\n    # Token budget\n    output_buffer_tokens=4000,  # Reserve for model output\n\n    # Scoring settings\n    use_importance_scoring=True,    # Use semantic scoring (vs position-only)\n    scoring_weights=weights,        # Custom weights\n    toin_integration=True,          # Use TOIN patterns if available\n    recency_decay_rate=0.1,         # Exponential decay lambda\n\n    # Strategy thresholds\n    compress_threshold=0.1,     # Try compression first if &lt;10% over budget\n)\n</code></pre>"},{"location":"configuration/#ccr-integration","title":"CCR Integration","text":"<p>When IntelligentContext drops messages, they're stored in CCR for potential retrieval:</p> <pre><code>from headroom.telemetry import get_toin\n\n# Pass TOIN for bidirectional integration\ntoin = get_toin()\nmanager = IntelligentContextManager(config=config, toin=toin)\n\n# Dropped messages are:\n# 1. Stored in CCR (so LLM can retrieve if needed)\n# 2. Recorded to TOIN (so it learns which patterns matter)\n# 3. Marked with CCR reference in the inserted message\n</code></pre> <p>The marker inserted when messages are dropped includes the CCR reference: <pre><code>[Earlier context compressed: 14 message(s) dropped by importance scoring.\nFull content available via ccr_retrieve tool with reference 'abc123def456'.]\n</code></pre></p>"},{"location":"configuration/#scoring-weights","title":"Scoring Weights","text":"<p>The <code>ScoringWeights</code> class controls how messages are scored:</p> Weight Default Description <code>recency</code> 0.20 Exponential decay from conversation end <code>semantic_similarity</code> 0.20 Embedding cosine similarity to recent context <code>toin_importance</code> 0.25 TOIN retrieval_rate (high retrieval = important) <code>error_indicator</code> 0.15 TOIN field_semantics error detection <code>forward_reference</code> 0.15 Count of later messages referencing this one <code>token_density</code> 0.05 Unique tokens / total tokens <p>Weights are automatically normalized to sum to 1.0:</p> <pre><code>weights = ScoringWeights(recency=1.0, toin_importance=1.0)\nnormalized = weights.normalized()\n# recency=0.5, toin_importance=0.5, others=0.0\n</code></pre>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>Some settings can be configured via environment variables:</p> Variable Description Default <code>HEADROOM_LOG_LEVEL</code> Logging level <code>INFO</code> <code>HEADROOM_STORE_URL</code> Database URL temp directory <code>HEADROOM_DEFAULT_MODE</code> Default mode <code>optimize</code> <code>HEADROOM_MODEL_LIMITS</code> Custom model config (JSON string or file path) -"},{"location":"configuration/#custom-model-configuration","title":"Custom Model Configuration","text":"<p>Configure context limits and pricing for new or custom models. Useful when: - A new model is released before Headroom is updated - You're using fine-tuned or custom models - You want to override built-in limits</p>"},{"location":"configuration/#configuration-methods","title":"Configuration Methods","text":"<p>Settings are resolved in this order (later overrides earlier): 1. Built-in defaults 2. <code>~/.headroom/models.json</code> config file 3. <code>HEADROOM_MODEL_LIMITS</code> environment variable 4. SDK constructor arguments</p>"},{"location":"configuration/#config-file-format","title":"Config File Format","text":"<p>Create <code>~/.headroom/models.json</code>:</p> <pre><code>{\n  \"anthropic\": {\n    \"context_limits\": {\n      \"claude-4-opus-20250301\": 200000,\n      \"claude-custom-finetune\": 128000\n    },\n    \"pricing\": {\n      \"claude-4-opus-20250301\": {\n        \"input\": 15.00,\n        \"output\": 75.00,\n        \"cached_input\": 1.50\n      }\n    }\n  },\n  \"openai\": {\n    \"context_limits\": {\n      \"gpt-5\": 256000,\n      \"ft:gpt-4o:my-org\": 128000\n    },\n    \"pricing\": {\n      \"gpt-5\": [5.00, 15.00]\n    }\n  }\n}\n</code></pre>"},{"location":"configuration/#environment-variable","title":"Environment Variable","text":"<p>Set <code>HEADROOM_MODEL_LIMITS</code> as a JSON string or file path:</p> <pre><code># JSON string\nexport HEADROOM_MODEL_LIMITS='{\"anthropic\":{\"context_limits\":{\"claude-new\":200000}}}'\n\n# File path\nexport HEADROOM_MODEL_LIMITS=/path/to/models.json\n</code></pre>"},{"location":"configuration/#pattern-based-inference","title":"Pattern-Based Inference","text":"<p>Unknown models are automatically inferred from naming patterns:</p> Pattern Inferred Settings <code>*opus*</code> 200K context, Opus-tier pricing <code>*sonnet*</code> 200K context, Sonnet-tier pricing <code>*haiku*</code> 200K context, Haiku-tier pricing <code>gpt-4o*</code> 128K context, GPT-4o pricing <code>o1*</code>, <code>o3*</code> 200K context, reasoning model pricing <p>This means new models like <code>claude-4-sonnet-20251201</code> will work automatically with Sonnet-tier defaults.</p>"},{"location":"configuration/#sdk-override","title":"SDK Override","text":"<p>Override in code for specific models:</p> <pre><code>from headroom import HeadroomClient, AnthropicProvider\n\nclient = HeadroomClient(\n    original_client=Anthropic(),\n    provider=AnthropicProvider(\n        context_limits={\n            \"claude-new-model\": 300000,\n        }\n    ),\n)\n</code></pre>"},{"location":"configuration/#provider-specific-settings","title":"Provider-Specific Settings","text":""},{"location":"configuration/#openai","title":"OpenAI","text":"<pre><code>from headroom import OpenAIProvider\n\nprovider = OpenAIProvider(\n    # Enable automatic prefix caching\n    enable_prefix_caching=True,\n)\n</code></pre>"},{"location":"configuration/#anthropic","title":"Anthropic","text":"<pre><code>from headroom import AnthropicProvider\n\nprovider = AnthropicProvider(\n    # Enable cache_control blocks\n    enable_cache_control=True,\n)\n</code></pre>"},{"location":"configuration/#google","title":"Google","text":"<pre><code>from headroom import GoogleProvider\n\nprovider = GoogleProvider(\n    # Enable context caching\n    enable_context_caching=True,\n)\n</code></pre>"},{"location":"configuration/#configuration-precedence","title":"Configuration Precedence","text":"<p>Settings are applied in this order (later overrides earlier):</p> <ol> <li>Default values</li> <li>Environment variables</li> <li>SDK constructor arguments</li> <li>Per-request overrides</li> </ol>"},{"location":"configuration/#validation","title":"Validation","text":"<p>Validate your configuration:</p> <pre><code>result = client.validate_setup()\n\nif not result[\"valid\"]:\n    print(\"Configuration issues:\")\n    for issue in result[\"issues\"]:\n        print(f\"  - {issue}\")\n</code></pre>"},{"location":"errors/","title":"Error Handling","text":"<p>Headroom provides explicit exceptions for debugging, with a safety guarantee that compression failures never break your LLM calls.</p>"},{"location":"errors/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>from headroom import (\n    HeadroomError,        # Base class - catch all Headroom errors\n    ConfigurationError,   # Invalid configuration\n    ProviderError,        # Provider issues (unknown model, etc.)\n    StorageError,         # Database/storage failures\n    CompressionError,     # Compression failures (rare)\n    ValidationError,      # Setup validation failures\n)\n</code></pre>"},{"location":"errors/#usage","title":"Usage","text":"<pre><code>from headroom import (\n    HeadroomClient,\n    HeadroomError,\n    ConfigurationError,\n    StorageError,\n)\n\ntry:\n    client = HeadroomClient(...)\n    response = client.chat.completions.create(...)\n\nexcept ConfigurationError as e:\n    print(f\"Config issue: {e}\")\n    print(f\"Details: {e.details}\")  # Additional context\n\nexcept StorageError as e:\n    print(f\"Storage issue: {e}\")\n    # Headroom continues to work, just without metrics persistence\n\nexcept HeadroomError as e:\n    print(f\"Headroom error: {e}\")\n</code></pre>"},{"location":"errors/#exception-types","title":"Exception Types","text":""},{"location":"errors/#configurationerror","title":"ConfigurationError","text":"<p>Raised when configuration is invalid.</p> <pre><code># Examples:\n# - Invalid mode value\n# - Missing required provider\n# - Invalid model context limit\n\ntry:\n    client = HeadroomClient(\n        original_client=OpenAI(),\n        provider=OpenAIProvider(),\n        default_mode=\"invalid_mode\",  # Will raise ConfigurationError\n    )\nexcept ConfigurationError as e:\n    print(f\"Config error: {e}\")\n    print(f\"Field: {e.details.get('field')}\")\n</code></pre>"},{"location":"errors/#providererror","title":"ProviderError","text":"<p>Raised for provider-specific issues.</p> <pre><code># Examples:\n# - Unknown model name\n# - Provider API error\n# - Token counting failure\n\ntry:\n    response = client.chat.completions.create(\n        model=\"unknown-model-xyz\",\n        messages=[...]\n    )\nexcept ProviderError as e:\n    print(f\"Provider error: {e}\")\n    print(f\"Provider: {e.details.get('provider')}\")\n</code></pre>"},{"location":"errors/#storageerror","title":"StorageError","text":"<p>Raised when database operations fail.</p> <pre><code># Examples:\n# - Database connection failure\n# - Write permission denied\n# - Disk full\n\ntry:\n    metrics = client.get_metrics()\nexcept StorageError as e:\n    print(f\"Storage error: {e}\")\n    # Application can continue - just won't have metrics\n</code></pre>"},{"location":"errors/#compressionerror","title":"CompressionError","text":"<p>Raised when compression fails (rare).</p> <pre><code># Examples:\n# - Malformed JSON in tool output\n# - Unexpected data structure\n\n# Note: In practice, compression errors are caught internally\n# and the original content passes through unchanged.\n# This exception is only raised if you explicitly enable strict mode.\n</code></pre>"},{"location":"errors/#validationerror","title":"ValidationError","text":"<p>Raised when setup validation fails.</p> <pre><code>result = client.validate_setup()\nif not result[\"valid\"]:\n    raise ValidationError(\n        \"Setup validation failed\",\n        details={\"issues\": result[\"issues\"]}\n    )\n</code></pre>"},{"location":"errors/#safety-guarantee","title":"Safety Guarantee","text":"<p>If compression fails, the original content passes through unchanged.</p> <p>This is a core design principle. Your LLM calls never fail due to Headroom:</p> <pre><code># Even if SmartCrusher encounters unexpected data:\nmessages = [\n    {\"role\": \"tool\", \"content\": \"malformed json {{{\"}\n]\n\n# This will NOT raise an exception\n# Instead, the malformed content passes through unchanged\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages\n)\n</code></pre>"},{"location":"errors/#logging-errors","title":"Logging Errors","text":"<p>Enable logging to see error details:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.WARNING)\n\n# Now you'll see warnings when compression is skipped:\n# WARNING:headroom.transforms.smart_crusher:Skipping compression: invalid JSON\n</code></pre>"},{"location":"errors/#error-details","title":"Error Details","text":"<p>All Headroom exceptions include a <code>details</code> dict with context:</p> <pre><code>try:\n    client = HeadroomClient(...)\nexcept HeadroomError as e:\n    print(f\"Error: {e}\")\n    print(f\"Type: {type(e).__name__}\")\n    print(f\"Details: {e.details}\")\n\n    # Details might include:\n    # - field: which config field caused the error\n    # - provider: which provider was involved\n    # - model: which model was requested\n    # - original_error: underlying exception\n</code></pre>"},{"location":"errors/#best-practices","title":"Best Practices","text":""},{"location":"errors/#1-catch-specific-exceptions","title":"1. Catch Specific Exceptions","text":"<pre><code># Good: catch specific exceptions\ntry:\n    response = client.chat.completions.create(...)\nexcept ConfigurationError:\n    # Handle config issues\n    pass\nexcept ProviderError:\n    # Handle provider issues\n    pass\n\n# Avoid: catching all exceptions\ntry:\n    response = client.chat.completions.create(...)\nexcept Exception:\n    # Too broad - might hide real bugs\n    pass\n</code></pre>"},{"location":"errors/#2-let-storageerror-pass","title":"2. Let StorageError Pass","text":"<pre><code># Storage errors don't affect core functionality\ntry:\n    metrics = client.get_metrics()\nexcept StorageError:\n    metrics = []  # Continue without historical metrics\n</code></pre>"},{"location":"errors/#3-validate-on-startup","title":"3. Validate on Startup","text":"<pre><code>client = HeadroomClient(...)\n\n# Validate once at startup\nresult = client.validate_setup()\nif not result[\"valid\"]:\n    raise SystemExit(f\"Headroom setup invalid: {result['issues']}\")\n\n# Then use client normally\nresponse = client.chat.completions.create(...)\n</code></pre>"},{"location":"errors/#debugging","title":"Debugging","text":""},{"location":"errors/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Shows detailed transform decisions\n# DEBUG:headroom.transforms.smart_crusher:Analyzing 1000 items...\n# DEBUG:headroom.transforms.smart_crusher:Kept 15 items (errors: 2, anomalies: 3)\n</code></pre>"},{"location":"errors/#check-stats-after-error","title":"Check Stats After Error","text":"<pre><code>try:\n    response = client.chat.completions.create(...)\nexcept HeadroomError:\n    # Check what happened\n    stats = client.get_stats()\n    print(f\"Last request stats: {stats}\")\n</code></pre>"},{"location":"getting-started/","title":"Getting Started with Headroom","text":"<p>This guide will help you get up and running with Headroom in under 5 minutes.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code># Core package (minimal dependencies)\npip install headroom\n\n# With proxy server\npip install headroom[proxy]\n\n# With semantic relevance (for smarter compression)\npip install headroom[relevance]\n\n# Everything\npip install headroom[all]\n</code></pre>"},{"location":"getting-started/#quick-start-proxy-mode-recommended","title":"Quick Start: Proxy Mode (Recommended)","text":"<p>The easiest way to use Headroom is as a proxy server:</p> <pre><code># Start the proxy\nheadroom proxy --port 8787\n</code></pre> <p>Then point your LLM client at it:</p> <pre><code># Claude Code\nANTHROPIC_BASE_URL=http://localhost:8787 claude\n\n# OpenAI-compatible clients\nOPENAI_BASE_URL=http://localhost:8787/v1 your-app\n</code></pre> <p>That's it! All your requests now go through Headroom and get optimized automatically.</p>"},{"location":"getting-started/#quick-start-python-sdk","title":"Quick Start: Python SDK","text":"<p>If you want programmatic control:</p> <pre><code>from headroom import HeadroomClient\nfrom openai import OpenAI\n\n# Create a wrapped client\nclient = HeadroomClient(\n    original_client=OpenAI(),\n    default_mode=\"optimize\",\n)\n\n# Use exactly like the original\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"},\n    ],\n)\n</code></pre>"},{"location":"getting-started/#modes","title":"Modes","text":""},{"location":"getting-started/#audit-mode","title":"Audit Mode","text":"<p>Observe without modifying:</p> <pre><code>client = HeadroomClient(\n    original_client=OpenAI(),\n    default_mode=\"audit\",\n)\n# Logs metrics but doesn't change requests\n</code></pre>"},{"location":"getting-started/#optimize-mode","title":"Optimize Mode","text":"<p>Apply transforms to reduce tokens:</p> <pre><code>client = HeadroomClient(\n    original_client=OpenAI(),\n    default_mode=\"optimize\",\n)\n# Compresses tool outputs, aligns cache prefixes, etc.\n</code></pre>"},{"location":"getting-started/#simulate-mode","title":"Simulate Mode","text":"<p>Preview what optimizations would do:</p> <pre><code>plan = client.chat.completions.simulate(\n    model=\"gpt-4o\",\n    messages=[...],\n)\nprint(f\"Would save {plan.tokens_saved} tokens\")\nprint(f\"Transforms: {plan.transforms_applied}\")\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Proxy Server Documentation - Configure the proxy</li> <li>Transforms Reference - Understand each transform</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"image-compression/","title":"Image Compression","text":"<p>Headroom automatically compresses images in your LLM requests, reducing token usage by 40-90% while maintaining answer accuracy.</p>"},{"location":"image-compression/#overview","title":"Overview","text":"<p>Vision models charge by the token, and images are expensive: - A 1024x1024 image costs ~765 tokens (OpenAI) - A 2048x2048 image costs ~2,900 tokens</p> <p>Headroom's image compression uses a trained ML router to analyze your query and automatically select the optimal compression technique:</p> Technique Savings When Used <code>full_low</code> ~87% General questions (\"What is this?\") <code>preserve</code> 0% Fine details needed (\"Count the whiskers\") <code>crop</code> 50-90% Region-specific (\"What's in the corner?\") <code>transcode</code> ~99% Text extraction (\"Read the sign\")"},{"location":"image-compression/#how-it-works","title":"How It Works","text":"<pre><code>User uploads image + asks question\n           \u2193\n   [Query Analysis]\n   TrainedRouter (MiniLM from HuggingFace)\n   Classifies: \"What animal is this?\" \u2192 full_low\n           \u2193\n   [Image Analysis]\n   SigLIP analyzes image properties\n   (has text? complex? fine details?)\n           \u2193\n   [Apply Compression]\n   OpenAI: detail=\"low\"\n   Anthropic: Resize to 512px\n   Google: Resize to 768px\n           \u2193\n   Compressed request to LLM\n</code></pre>"},{"location":"image-compression/#quick-start","title":"Quick Start","text":""},{"location":"image-compression/#with-headroom-proxy-zero-code-changes","title":"With Headroom Proxy (Zero Code Changes)","text":"<pre><code># Start the proxy\nheadroom proxy --port 8787\n\n# Connect your client\nANTHROPIC_BASE_URL=http://localhost:8787 claude\n</code></pre> <p>Images are automatically compressed based on your queries.</p>"},{"location":"image-compression/#with-headroomclient","title":"With HeadroomClient","text":"<pre><code>from headroom import HeadroomClient\n\nclient = HeadroomClient(provider=\"openai\")\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What animal is this?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,...\"}}\n        ]\n    }]\n)\n# Image automatically compressed with detail=\"low\" (87% savings)\n</code></pre>"},{"location":"image-compression/#direct-api","title":"Direct API","text":"<pre><code>from headroom.image import ImageCompressor\n\ncompressor = ImageCompressor()\n\n# Compress images in messages\ncompressed_messages = compressor.compress(messages, provider=\"openai\")\n\n# Check savings\nprint(f\"Saved {compressor.last_savings:.0f}% tokens\")\nprint(f\"Technique: {compressor.last_result.technique.value}\")\n</code></pre>"},{"location":"image-compression/#configuration","title":"Configuration","text":""},{"location":"image-compression/#proxy-configuration","title":"Proxy Configuration","text":"<pre><code># Enable image compression (default: true)\nheadroom proxy --image-optimize\n\n# Disable image compression\nheadroom proxy --no-image-optimize\n</code></pre>"},{"location":"image-compression/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>from headroom.image import ImageCompressor\n\ncompressor = ImageCompressor(\n    model_id=\"chopratejas/technique-router\",  # HuggingFace model\n    use_siglip=True,   # Enable image analysis\n    device=\"cuda\",     # Use GPU if available\n)\n</code></pre>"},{"location":"image-compression/#provider-support","title":"Provider Support","text":"Provider Detection Compression Method OpenAI <code>image_url</code> Sets <code>detail=\"low\"</code> Anthropic <code>image</code> with <code>source</code> Resizes to 512px Google <code>inlineData</code> Resizes to 768px (tile-optimized)"},{"location":"image-compression/#openai","title":"OpenAI","text":"<p>Uses the native <code>detail</code> parameter: <pre><code># Before\n{\"type\": \"image_url\", \"image_url\": {\"url\": \"data:...\"}}\n\n# After (full_low technique)\n{\"type\": \"image_url\", \"image_url\": {\"url\": \"data:...\", \"detail\": \"low\"}}\n</code></pre></p>"},{"location":"image-compression/#anthropic","title":"Anthropic","text":"<p>Resizes the image using PIL: <pre><code># Before: 1024x1024 image (~1,398 tokens)\n# After:  512x512 image (~349 tokens) - 75% savings\n</code></pre></p>"},{"location":"image-compression/#google-gemini","title":"Google Gemini","text":"<p>Resizes to 768px (optimal for Gemini's 768x768 tile system): <pre><code># Before: 1536x1536 image (4 tiles \u00d7 258 = 1,032 tokens)\n# After:  768x768 image (1 tile \u00d7 258 = 258 tokens) - 75% savings\n</code></pre></p>"},{"location":"image-compression/#techniques-explained","title":"Techniques Explained","text":""},{"location":"image-compression/#full_low-87-savings","title":"<code>full_low</code> (87% savings)","text":"<p>Best for general understanding questions: - \"What is this?\" - \"Describe the scene\" - \"Is this indoors or outdoors?\"</p> <p>The model doesn't need fine details to answer these questions.</p>"},{"location":"image-compression/#preserve-0-savings","title":"<code>preserve</code> (0% savings)","text":"<p>Required when fine details matter: - \"Count the whiskers\" - \"What brand is shown?\" - \"Read the serial number\" - \"What time does the clock show?\"</p>"},{"location":"image-compression/#crop-50-90-savings","title":"<code>crop</code> (50-90% savings)","text":"<p>For region-specific queries: - \"What's in the top-right corner?\" - \"Focus on the background\" - \"Zoom into the left side\"</p> <p>Note: Currently implemented as resize. True cropping coming soon.</p>"},{"location":"image-compression/#transcode-99-savings","title":"<code>transcode</code> (99% savings)","text":"<p>For text extraction (converts image to text): - \"Read the sign\" - \"What does it say?\" - \"Transcribe the document\"</p> <p>Note: Requires vision model call. Currently falls back to preserve.</p>"},{"location":"image-compression/#the-trained-router","title":"The Trained Router","text":"<p>The routing decision is made by a fine-tuned MiniLM classifier:</p> <ul> <li>Model: <code>chopratejas/technique-router</code> on HuggingFace</li> <li>Size: ~128MB</li> <li>Accuracy: 93.7% on validation set</li> <li>Training data: 1,157 examples across 4 techniques</li> </ul> <p>The model is downloaded automatically on first use and cached locally.</p>"},{"location":"image-compression/#training-data-examples","title":"Training Data Examples","text":"Query Technique \"What animal is this?\" <code>full_low</code> \"Count the spots\" <code>preserve</code> \"Read the text on the sign\" <code>transcode</code> \"What's in the corner?\" <code>crop</code>"},{"location":"image-compression/#performance","title":"Performance","text":""},{"location":"image-compression/#token-savings-by-query-type","title":"Token Savings by Query Type","text":"Query Type Before After Savings General (\"What is this?\") 765 85 89% Detail (\"Count items\") 765 765 0% Region (\"Top corner?\") 765 85 89% Text (\"Read the sign\") 765 85 89%"},{"location":"image-compression/#latency","title":"Latency","text":"<ul> <li>Router inference: ~10ms (CPU), ~2ms (GPU)</li> <li>Image resize: ~5-20ms depending on size</li> <li>First request: +2-3s (model download, cached after)</li> </ul>"},{"location":"image-compression/#troubleshooting","title":"Troubleshooting","text":""},{"location":"image-compression/#model-download-issues","title":"Model Download Issues","text":"<p>The HuggingFace model downloads on first use:</p> <pre><code># Force a specific cache directory\nimport os\nos.environ[\"HF_HOME\"] = \"/path/to/cache\"\n\nfrom headroom.image import ImageCompressor\ncompressor = ImageCompressor()\n</code></pre>"},{"location":"image-compression/#gpu-memory","title":"GPU Memory","text":"<p>SigLIP requires ~400MB GPU memory. To use CPU only:</p> <pre><code>compressor = ImageCompressor(device=\"cpu\")\n</code></pre>"},{"location":"image-compression/#disable-image-compression","title":"Disable Image Compression","text":"<pre><code># Proxy\nheadroom proxy --no-image-optimize\n\n# Direct\n# Simply don't call compress()\n</code></pre>"},{"location":"image-compression/#api-reference","title":"API Reference","text":""},{"location":"image-compression/#imagecompressor","title":"<code>ImageCompressor</code>","text":"<pre><code>class ImageCompressor:\n    def __init__(\n        self,\n        model_id: str = \"chopratejas/technique-router\",\n        use_siglip: bool = True,\n        device: str | None = None,\n    ): ...\n\n    def has_images(self, messages: list[dict]) -&gt; bool:\n        \"\"\"Check if messages contain images.\"\"\"\n\n    def compress(\n        self,\n        messages: list[dict],\n        provider: str = \"openai\",\n    ) -&gt; list[dict]:\n        \"\"\"Compress images in messages.\"\"\"\n\n    @property\n    def last_result(self) -&gt; CompressionResult | None:\n        \"\"\"Result of last compression.\"\"\"\n\n    @property\n    def last_savings(self) -&gt; float:\n        \"\"\"Savings percentage from last compression.\"\"\"\n</code></pre>"},{"location":"image-compression/#compressionresult","title":"<code>CompressionResult</code>","text":"<pre><code>@dataclass\nclass CompressionResult:\n    technique: Technique      # full_low, preserve, crop, transcode\n    original_tokens: int      # Estimated tokens before\n    compressed_tokens: int    # Estimated tokens after\n    confidence: float         # Router confidence (0-1)\n\n    @property\n    def savings_percent(self) -&gt; float:\n        \"\"\"Percentage of tokens saved.\"\"\"\n</code></pre>"},{"location":"image-compression/#technique","title":"<code>Technique</code>","text":"<pre><code>class Technique(Enum):\n    FULL_LOW = \"full_low\"     # 87% savings\n    PRESERVE = \"preserve\"     # 0% savings\n    CROP = \"crop\"             # 50-90% savings\n    TRANSCODE = \"transcode\"   # 99% savings\n</code></pre>"},{"location":"image-compression/#see-also","title":"See Also","text":"<ul> <li>Compression Guide - Text compression techniques</li> <li>CCR Guide - Reversible compression with retrieval</li> <li>Proxy Guide - Zero-code deployment</li> <li>Architecture - System design</li> </ul>"},{"location":"integration-guide/","title":"Integration Guide","text":"<p>You don't need to run the Headroom proxy. Headroom is a compression library that works with any LLM client, proxy, or framework.</p>"},{"location":"integration-guide/#pick-your-path","title":"Pick Your Path","text":"You have... Use this Setup Any Python app <code>compress()</code> 2 lines LiteLLM LiteLLM callback 1 line A Python proxy (FastAPI, custom) ASGI middleware 1 line Claude Code / Cursor Headroom proxy 1 env var Agno agents Agno integration Wrap model LangChain LangChain integration Wrap model Non-Python app Headroom proxy HTTP"},{"location":"integration-guide/#compress-function","title":"compress() Function","text":"<p>The simplest integration. Works with any LLM client.</p> <pre><code>from headroom import compress\n\n# Before sending to your LLM:\nresult = compress(messages, model=\"claude-sonnet-4-5-20250929\")\nresponse = your_client.create(messages=result.messages)  # Fewer tokens, same answer\n\nprint(f\"Saved {result.tokens_saved} tokens ({result.compression_ratio:.0%})\")\n</code></pre>"},{"location":"integration-guide/#with-anthropic-sdk","title":"With Anthropic SDK","text":"<pre><code>from anthropic import Anthropic\nfrom headroom import compress\n\nclient = Anthropic()\nmessages = [\n    {\"role\": \"user\", \"content\": \"What went wrong?\"},\n    {\"role\": \"assistant\", \"content\": \"Let me check.\", \"tool_use\": [...]},\n    {\"role\": \"user\", \"content\": [{\"type\": \"tool_result\", \"content\": huge_json}]},\n]\n\ncompressed = compress(messages, model=\"claude-sonnet-4-5-20250929\")\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5-20250929\",\n    messages=compressed.messages,\n    max_tokens=1000,\n)\n</code></pre>"},{"location":"integration-guide/#with-openai-sdk","title":"With OpenAI SDK","text":"<pre><code>from openai import OpenAI\nfrom headroom import compress\n\nclient = OpenAI()\nmessages = [\n    {\"role\": \"user\", \"content\": \"Analyze these results\"},\n    {\"role\": \"tool\", \"content\": big_json_output, \"tool_call_id\": \"call_1\"},\n]\n\ncompressed = compress(messages, model=\"gpt-4o\")\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=compressed.messages,\n)\n</code></pre>"},{"location":"integration-guide/#with-litellm-direct","title":"With LiteLLM (direct)","text":"<pre><code>import litellm\nfrom headroom import compress\n\nmessages = [...]\ncompressed = compress(messages, model=\"bedrock/claude-sonnet\")\nresponse = litellm.completion(model=\"bedrock/claude-sonnet\", messages=compressed.messages)\n</code></pre>"},{"location":"integration-guide/#with-any-http-client","title":"With any HTTP client","text":"<pre><code>import httpx\nfrom headroom import compress\n\ncompressed = compress(messages, model=\"claude-sonnet-4-5-20250929\")\nhttpx.post(\"https://api.anthropic.com/v1/messages\", json={\n    \"model\": \"claude-sonnet-4-5-20250929\",\n    \"messages\": compressed.messages,\n}, headers={\"X-Api-Key\": api_key, \"anthropic-version\": \"2023-06-01\"})\n</code></pre>"},{"location":"integration-guide/#what-compress-returns","title":"What compress() returns","text":"<pre><code>result = compress(messages, model=\"gpt-4o\")\nresult.messages           # list[dict] \u2014 compressed messages, same format as input\nresult.tokens_before      # int \u2014 original token count\nresult.tokens_after       # int \u2014 compressed token count\nresult.tokens_saved       # int \u2014 tokens removed\nresult.compression_ratio  # float \u2014 0.0 (no savings) to 1.0 (100% removed)\nresult.transforms_applied # list[str] \u2014 what ran (e.g., [\"router:smart_crusher:0.35\"])\n</code></pre>"},{"location":"integration-guide/#litellm","title":"LiteLLM","text":"<p>If you're already using LiteLLM as your LLM gateway, add Headroom as a callback:</p> <pre><code>import litellm\nfrom headroom.integrations.litellm_callback import HeadroomCallback\n\nlitellm.callbacks = [HeadroomCallback()]\n\n# All calls now compressed automatically\nresponse = litellm.completion(model=\"gpt-4o\", messages=[...])\nresponse = litellm.completion(model=\"bedrock/claude-sonnet\", messages=[...])\nresponse = litellm.completion(model=\"azure/gpt-4o\", messages=[...])\n</code></pre> <p>The callback compresses messages in LiteLLM's <code>pre_call_hook</code> before they're sent to the provider. Works with all 100+ LiteLLM-supported providers.</p>"},{"location":"integration-guide/#with-litellm-proxy","title":"With LiteLLM Proxy","text":"<p>If you run LiteLLM as a proxy server, use the ASGI middleware instead:</p> <pre><code># In your LiteLLM proxy startup\nfrom litellm.proxy.proxy_server import app\nfrom headroom.integrations.asgi import CompressionMiddleware\n\napp.add_middleware(CompressionMiddleware)\n</code></pre> <p>Or use the callback in your LiteLLM config:</p> <pre><code># litellm_config.yaml\nlitellm_settings:\n  callbacks: [\"headroom.integrations.litellm_callback.HeadroomCallback\"]\n</code></pre>"},{"location":"integration-guide/#asgi-middleware","title":"ASGI Middleware","text":"<p>Drop-in middleware for any ASGI application (FastAPI, Starlette, LiteLLM proxy, custom proxies).</p> <pre><code>from headroom.integrations.asgi import CompressionMiddleware\n\n# FastAPI\napp = FastAPI()\napp.add_middleware(CompressionMiddleware)\n\n# Starlette\napp = Starlette(routes=[...])\napp.add_middleware(CompressionMiddleware)\n\n# LiteLLM proxy\nfrom litellm.proxy.proxy_server import app\napp.add_middleware(CompressionMiddleware)\n</code></pre> <p>The middleware intercepts POST requests to <code>/v1/messages</code>, <code>/v1/chat/completions</code>, <code>/v1/responses</code>, and <code>/chat/completions</code>. All other requests pass through untouched.</p> <p>Response headers include: - <code>x-headroom-compressed: true</code> \u2014 compression was applied - <code>x-headroom-tokens-saved: 1234</code> \u2014 tokens removed</p>"},{"location":"integration-guide/#proxy","title":"Proxy","text":"<p>The Headroom proxy is a standalone HTTP server. Best for non-Python apps or tools that only support base URL configuration (Claude Code, Cursor).</p> <pre><code>pip install \"headroom-ai[all]\"\nheadroom proxy --port 8787\n</code></pre> <pre><code># Claude Code\nANTHROPIC_BASE_URL=http://localhost:8787 claude\n\n# Cursor / Any OpenAI client\nOPENAI_BASE_URL=http://localhost:8787/v1 cursor\n</code></pre>"},{"location":"integration-guide/#with-cloud-providers","title":"With Cloud Providers","text":"<pre><code># AWS Bedrock\nheadroom proxy --backend bedrock --region us-east-1\n\n# Google Vertex AI\nheadroom proxy --backend vertex_ai --region us-central1\n\n# Azure OpenAI\nheadroom proxy --backend azure\n\n# OpenRouter (400+ models)\nOPENROUTER_API_KEY=sk-or-... headroom proxy --backend openrouter\n</code></pre> <p>See Proxy Documentation for all options.</p>"},{"location":"integration-guide/#agno","title":"Agno","text":"<p>Full integration with the Agno agent framework.</p> <pre><code>from agno.agent import Agent\nfrom agno.models.anthropic import Claude\nfrom headroom.integrations.agno import HeadroomAgnoModel\n\nmodel = HeadroomAgnoModel(Claude(id=\"claude-sonnet-4-20250514\"))\nagent = Agent(model=model, tools=[your_tools])\nresponse = agent.run(\"Investigate the issue\")\n\nprint(f\"Tokens saved: {model.total_tokens_saved}\")\n</code></pre> <p>See Agno Guide for hooks, multi-provider, and streaming.</p>"},{"location":"integration-guide/#langchain","title":"LangChain","text":"<p>Experimental. Core compression works. Streaming callbacks and async chains are still being tested.</p> <pre><code>from langchain_openai import ChatOpenAI\nfrom headroom.integrations import HeadroomChatModel\n\nllm = HeadroomChatModel(ChatOpenAI(model=\"gpt-4o\"))\nresponse = llm.invoke(\"Hello!\")\n</code></pre> <p>See LangChain Guide for details and known limitations.</p>"},{"location":"integration-guide/#compression-hooks-advanced","title":"Compression Hooks (Advanced)","text":"<p>Customize compression behavior without modifying Headroom's code:</p> <pre><code>from headroom import compress, CompressionHooks, CompressContext\n\nclass MyHooks(CompressionHooks):\n    def pre_compress(self, messages, ctx):\n        # Modify messages before compression (dedup, filter, inject)\n        return messages\n\n    def compute_biases(self, messages, ctx):\n        # Per-message compression aggressiveness\n        # &gt;1.0 = keep more, &lt;1.0 = compress more\n        return {5: 1.5, 6: 0.5}  # Keep message 5, compress message 6\n\n    def post_compress(self, event):\n        # Observe results (logging, analytics, learning)\n        print(f\"Saved {event.tokens_saved} tokens\")\n\nresult = compress(messages, model=\"gpt-4o\", hooks=MyHooks())\n</code></pre> <p>See Architecture for how hooks integrate with the pipeline.</p>"},{"location":"integration-guide/#faq","title":"FAQ","text":"<p>Q: Does Headroom change the response format? No. Your LLM returns the same response format. Headroom only modifies the input messages.</p> <p>Q: What if compression removes something the LLM needs? Headroom stores originals in CCR (Compress-Cache-Retrieve). The LLM can call <code>headroom_retrieve</code> to get full uncompressed content. Compression summaries tell the LLM what's available.</p> <p>Q: Does it work with streaming? Yes. Compression happens before the request is sent. Streaming responses are unaffected.</p> <p>Q: How much latency does it add? 1-5ms for compression. The token savings typically save more time on the LLM side than compression adds.</p>"},{"location":"langchain/","title":"LangChain Integration","text":"<p>Headroom provides seamless integration with LangChain, enabling automatic context optimization across all LangChain patterns: chat models, memory, retrievers, agents, and observability.</p>"},{"location":"langchain/#installation","title":"Installation","text":"<pre><code>pip install \"headroom-ai[langchain]\"\n</code></pre> <p>This installs Headroom with LangChain dependencies (<code>langchain-core</code>).</p>"},{"location":"langchain/#quick-start","title":"Quick Start","text":""},{"location":"langchain/#wrap-any-chat-model-1-line","title":"Wrap Any Chat Model (1 Line)","text":"<pre><code>from langchain_openai import ChatOpenAI\nfrom headroom.integrations import HeadroomChatModel\n\n# Wrap your model - that's it!\nllm = HeadroomChatModel(ChatOpenAI(model=\"gpt-4o\"))\n\n# Use exactly like before\nresponse = llm.invoke(\"Hello!\")\n</code></pre> <p>Headroom automatically: - Detects the provider (OpenAI, Anthropic, Google) - Compresses tool outputs in conversation history - Optimizes for provider caching - Tracks token savings</p>"},{"location":"langchain/#check-your-savings","title":"Check Your Savings","text":"<pre><code># After some usage\nprint(llm.get_metrics())\n# {'tokens_saved': 12500, 'savings_percent': 45.2, 'requests': 50}\n</code></pre>"},{"location":"langchain/#integration-patterns","title":"Integration Patterns","text":""},{"location":"langchain/#1-chat-model-wrapper","title":"1. Chat Model Wrapper","text":"<p>The <code>HeadroomChatModel</code> wraps any LangChain <code>BaseChatModel</code>:</p> <pre><code>from langchain_openai import ChatOpenAI\nfrom langchain_anthropic import ChatAnthropic\nfrom headroom.integrations import HeadroomChatModel\n\n# OpenAI\nllm = HeadroomChatModel(ChatOpenAI(model=\"gpt-4o\"))\n\n# Anthropic (auto-detected)\nllm = HeadroomChatModel(ChatAnthropic(model=\"claude-3-5-sonnet-20241022\"))\n\n# Custom configuration\nfrom headroom import HeadroomConfig, HeadroomMode\n\nconfig = HeadroomConfig(\n    default_mode=HeadroomMode.OPTIMIZE,\n    smart_crusher_target_ratio=0.3,  # Target 70% compression\n)\nllm = HeadroomChatModel(\n    ChatOpenAI(model=\"gpt-4o\"),\n    headroom_config=config,\n)\n</code></pre>"},{"location":"langchain/#async-support","title":"Async Support","text":"<p>Full async support for <code>ainvoke</code> and <code>astream</code>:</p> <pre><code># Async invoke\nresponse = await llm.ainvoke(\"Hello!\")\n\n# Async streaming\nasync for chunk in llm.astream(\"Tell me a story\"):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"langchain/#tool-calling","title":"Tool Calling","text":"<p>Works seamlessly with LangChain tool calling:</p> <pre><code>from langchain_core.tools import tool\n\n@tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    return {\"results\": [...]}  # Large JSON response\n\nllm_with_tools = llm.bind_tools([search])\nresponse = llm_with_tools.invoke(\"Search for Python tutorials\")\n# Tool outputs are automatically compressed in subsequent turns\n</code></pre>"},{"location":"langchain/#2-memory-integration","title":"2. Memory Integration","text":"<p><code>HeadroomChatMessageHistory</code> wraps any chat history with automatic compression:</p> <pre><code>from langchain.memory import ConversationBufferMemory\nfrom langchain_community.chat_message_histories import ChatMessageHistory\nfrom headroom.integrations import HeadroomChatMessageHistory\n\n# Wrap any history\nbase_history = ChatMessageHistory()\ncompressed_history = HeadroomChatMessageHistory(\n    base_history,\n    compress_threshold_tokens=4000,  # Compress when over 4K tokens\n    keep_recent_turns=5,             # Always keep last 5 turns\n)\n\n# Use with any memory class\nmemory = ConversationBufferMemory(chat_memory=compressed_history)\n\n# Zero changes to your chain!\nchain = ConversationChain(llm=llm, memory=memory)\n</code></pre> <p>Why this matters: Long conversations can blow up to 50K+ tokens. HeadroomChatMessageHistory automatically compresses older turns while preserving recent context.</p> <pre><code># Check compression stats\nprint(compressed_history.get_compression_stats())\n# {'compression_count': 12, 'total_tokens_saved': 28000}\n</code></pre>"},{"location":"langchain/#3-retriever-integration","title":"3. Retriever Integration","text":"<p><code>HeadroomDocumentCompressor</code> filters retrieved documents by relevance:</p> <pre><code>from langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_community.vectorstores import FAISS\nfrom headroom.integrations import HeadroomDocumentCompressor\n\n# Create vector store retriever (retrieve many for recall)\nvectorstore = FAISS.from_documents(documents, embeddings)\nbase_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 50})\n\n# Wrap with Headroom compression (keep best for precision)\ncompressor = HeadroomDocumentCompressor(\n    max_documents=10,      # Keep top 10\n    min_relevance=0.3,     # Minimum relevance score\n    prefer_diverse=True,   # MMR-style diversity\n)\n\nretriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=base_retriever,\n)\n\n# Retrieves 50 docs, returns best 10\ndocs = retriever.invoke(\"What is Python?\")\n</code></pre> <p>Why this matters: Vector search often returns many marginally-relevant documents. HeadroomDocumentCompressor uses BM25-style scoring to keep only the most relevant ones, reducing context size while improving answer quality.</p>"},{"location":"langchain/#4-agent-tool-wrapping","title":"4. Agent Tool Wrapping","text":"<p><code>wrap_tools_with_headroom</code> compresses tool outputs for agents:</p> <pre><code>from langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_core.tools import tool\nfrom headroom.integrations import wrap_tools_with_headroom\n\n@tool\ndef search_database(query: str) -&gt; str:\n    \"\"\"Search the database.\"\"\"\n    # Returns 1000 results as JSON\n    return json.dumps({\"results\": [...], \"total\": 1000})\n\n@tool\ndef fetch_logs(service: str) -&gt; str:\n    \"\"\"Fetch service logs.\"\"\"\n    # Returns 500 log entries\n    return json.dumps({\"logs\": [...]})\n\n# Wrap tools with compression\ntools = [search_database, fetch_logs]\nwrapped_tools = wrap_tools_with_headroom(\n    tools,\n    min_chars_to_compress=1000,  # Only compress large outputs\n)\n\n# Create agent with wrapped tools\nagent = create_openai_tools_agent(llm, wrapped_tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=wrapped_tools)\n\n# Tool outputs are automatically compressed\nresult = executor.invoke({\"input\": \"Find users who logged in yesterday\"})\n</code></pre> <p>Per-tool metrics:</p> <pre><code>from headroom.integrations import get_tool_metrics\n\nmetrics = get_tool_metrics()\nprint(metrics.get_summary())\n# {\n#   'total_invocations': 25,\n#   'total_compressions': 18,\n#   'total_chars_saved': 450000,\n#   'by_tool': {\n#     'search_database': {'invocations': 15, 'chars_saved': 320000},\n#     'fetch_logs': {'invocations': 10, 'chars_saved': 130000},\n#   }\n# }\n</code></pre>"},{"location":"langchain/#5-streaming-metrics","title":"5. Streaming Metrics","text":"<p>Track output tokens during streaming:</p> <pre><code>from headroom.integrations import StreamingMetricsTracker\n\ntracker = StreamingMetricsTracker(model=\"gpt-4o\")\n\nfor chunk in llm.stream(\"Write a poem about coding\"):\n    tracker.add_chunk(chunk)\n    print(chunk.content, end=\"\", flush=True)\n\nmetrics = tracker.finish()\nprint(f\"\\nOutput tokens: {metrics.output_tokens}\")\nprint(f\"Duration: {metrics.duration_ms:.0f}ms\")\n</code></pre> <p>Context manager style:</p> <pre><code>from headroom.integrations import StreamingMetricsCallback\n\nwith StreamingMetricsCallback(model=\"gpt-4o\") as tracker:\n    for chunk in llm.stream(messages):\n        tracker.add_chunk(chunk)\n        print(chunk.content, end=\"\")\n\nprint(f\"Metrics: {tracker.metrics}\")\n</code></pre>"},{"location":"langchain/#6-langsmith-integration","title":"6. LangSmith Integration","text":"<p>Add Headroom metrics to LangSmith traces:</p> <pre><code>from headroom.integrations import HeadroomLangSmithCallbackHandler\n\n# Create callback handler\nlangsmith_handler = HeadroomLangSmithCallbackHandler()\n\n# Use with your LLM\nllm = HeadroomChatModel(\n    ChatOpenAI(model=\"gpt-4o\"),\n    callbacks=[langsmith_handler],\n)\n\n# After calls, metrics appear in LangSmith traces:\n# - headroom.tokens_before\n# - headroom.tokens_after\n# - headroom.tokens_saved\n# - headroom.compression_ratio\n</code></pre>"},{"location":"langchain/#real-world-examples","title":"Real-World Examples","text":""},{"location":"langchain/#example-1-langgraph-react-agent","title":"Example 1: LangGraph ReAct Agent","text":"<p>The ReAct pattern is the most common agent architecture. Here's how to optimize it:</p> <pre><code>from langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import create_react_agent\nfrom headroom.integrations import HeadroomChatModel, wrap_tools_with_headroom\n\n# Define tools that return large outputs\n@tool\ndef search_web(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    # Simulating large search results\n    return json.dumps({\n        \"results\": [\n            {\"title\": f\"Result {i}\", \"snippet\": \"...\" * 100, \"url\": f\"https://...\"}\n            for i in range(100)\n        ],\n        \"total\": 1000,\n    })\n\n@tool\ndef query_database(sql: str) -&gt; str:\n    \"\"\"Execute SQL query.\"\"\"\n    return json.dumps({\n        \"rows\": [{\"id\": i, \"data\": \"...\" * 50} for i in range(500)],\n        \"total\": 500,\n    })\n\n# Wrap model with Headroom\nllm = HeadroomChatModel(ChatOpenAI(model=\"gpt-4o\"))\n\n# Wrap tools with compression\ntools = wrap_tools_with_headroom([search_web, query_database])\n\n# Create ReAct agent\nagent = create_react_agent(llm, tools)\n\n# Run - tool outputs are automatically compressed between iterations\nresult = agent.invoke({\n    \"messages\": [(\"user\", \"Find all users who signed up last week and their activity\")]\n})\n\n# Check savings\nprint(f\"Tokens saved: {llm.get_metrics()['tokens_saved']}\")\n</code></pre> <p>Without Headroom: Each tool call adds 10-50K tokens to context. With Headroom: Tool outputs compressed to 1-2K tokens, agent runs faster and cheaper.</p>"},{"location":"langchain/#example-2-rag-pipeline-with-document-filtering","title":"Example 2: RAG Pipeline with Document Filtering","text":"<pre><code>from langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom headroom.integrations import HeadroomChatModel, HeadroomDocumentCompressor\n\n# Setup vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(documents, embeddings)\n\n# High-recall retriever (get many candidates)\nbase_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 50})\n\n# Headroom compressor for precision\ncompressor = HeadroomDocumentCompressor(\n    max_documents=5,       # Keep only top 5\n    min_relevance=0.4,     # Must be 40%+ relevant\n    prefer_diverse=True,   # Avoid redundant docs\n)\n\n# Combine into compression retriever\nretriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=base_retriever,\n)\n\n# Wrap LLM\nllm = HeadroomChatModel(ChatOpenAI(model=\"gpt-4o\"))\n\n# Create QA chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    retriever=retriever,\n    return_source_documents=True,\n)\n\n# Query - retrieves 50 docs, uses best 5\nresult = qa_chain.invoke({\"query\": \"How do I configure authentication?\"})\nprint(f\"Answer: {result['result']}\")\nprint(f\"Sources: {len(result['source_documents'])} docs\")\n</code></pre> <p>Impact: - Without filtering: 50 docs \u00d7 ~500 tokens = 25K context tokens - With Headroom: 5 docs \u00d7 ~500 tokens = 2.5K context tokens (90% reduction)</p>"},{"location":"langchain/#example-3-conversational-agent-with-memory","title":"Example 3: Conversational Agent with Memory","text":"<pre><code>from langchain_openai import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_community.chat_message_histories import ChatMessageHistory\nfrom langchain.chains import ConversationChain\nfrom headroom.integrations import HeadroomChatModel, HeadroomChatMessageHistory\n\n# Wrap LLM\nllm = HeadroomChatModel(ChatOpenAI(model=\"gpt-4o\"))\n\n# Wrap memory with auto-compression\nbase_history = ChatMessageHistory()\ncompressed_history = HeadroomChatMessageHistory(\n    base_history,\n    compress_threshold_tokens=8000,  # Compress when over 8K\n    keep_recent_turns=10,            # Always keep last 10 turns\n)\n\nmemory = ConversationBufferMemory(\n    chat_memory=compressed_history,\n    return_messages=True,\n)\n\n# Create conversation chain\nchain = ConversationChain(llm=llm, memory=memory)\n\n# Long conversation - memory auto-compresses\nfor i in range(100):\n    response = chain.invoke({\"input\": f\"Tell me about topic {i}\"})\n    print(f\"Turn {i}: {len(response['response'])} chars\")\n\n# Check memory stats\nprint(compressed_history.get_compression_stats())\n# {'compression_count': 8, 'total_tokens_saved': 45000}\n</code></pre> <p>Impact: Without compression, 100-turn conversation = 100K+ tokens. With HeadroomChatMessageHistory, it stays under 8K tokens while preserving recent context.</p>"},{"location":"langchain/#example-4-multi-tool-research-agent","title":"Example 4: Multi-Tool Research Agent","text":"<pre><code>from langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_openai_tools_agent\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\nfrom headroom.integrations import (\n    HeadroomChatModel,\n    wrap_tools_with_headroom,\n    get_tool_metrics,\n    reset_tool_metrics,\n)\n\n@tool\ndef search_arxiv(query: str) -&gt; str:\n    \"\"\"Search arXiv for papers.\"\"\"\n    return json.dumps({\"papers\": [{\"title\": f\"Paper {i}\", \"abstract\": \"...\" * 200} for i in range(50)]})\n\n@tool\ndef search_github(query: str) -&gt; str:\n    \"\"\"Search GitHub repositories.\"\"\"\n    return json.dumps({\"repos\": [{\"name\": f\"repo-{i}\", \"description\": \"...\" * 100, \"stars\": i * 100} for i in range(100)]})\n\n@tool\ndef fetch_documentation(url: str) -&gt; str:\n    \"\"\"Fetch documentation from URL.\"\"\"\n    return \"...\" * 5000  # Large doc content\n\n# Wrap everything\nllm = HeadroomChatModel(ChatOpenAI(model=\"gpt-4o\"))\ntools = wrap_tools_with_headroom([search_arxiv, search_github, fetch_documentation])\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a research assistant. Use tools to gather information.\"),\n    (\"human\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\"),\n])\n\nagent = create_openai_tools_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\n# Reset metrics for this session\nreset_tool_metrics()\n\n# Run complex research task\nresult = executor.invoke({\n    \"input\": \"Research the latest advances in LLM context compression and find relevant GitHub projects\"\n})\n\n# Check per-tool metrics\nmetrics = get_tool_metrics().get_summary()\nprint(f\"Total chars saved: {metrics['total_chars_saved']:,}\")\nprint(f\"Per-tool breakdown: {metrics['by_tool']}\")\n</code></pre>"},{"location":"langchain/#configuration-options","title":"Configuration Options","text":""},{"location":"langchain/#headroomchatmodel","title":"HeadroomChatModel","text":"<pre><code>HeadroomChatModel(\n    wrapped_model,                     # Any LangChain BaseChatModel\n    headroom_config=HeadroomConfig(),  # Headroom configuration\n    auto_detect_provider=True,         # Auto-detect from wrapped model\n)\n</code></pre>"},{"location":"langchain/#headroomchatmessagehistory","title":"HeadroomChatMessageHistory","text":"<pre><code>HeadroomChatMessageHistory(\n    base_history,                      # Any BaseChatMessageHistory\n    compress_threshold_tokens=4000,    # Token threshold for compression\n    keep_recent_turns=5,               # Minimum turns to preserve\n    model=\"gpt-4o\",                    # Model for token counting\n)\n</code></pre>"},{"location":"langchain/#headroomdocumentcompressor","title":"HeadroomDocumentCompressor","text":"<pre><code>HeadroomDocumentCompressor(\n    max_documents=10,                  # Maximum docs to return\n    min_relevance=0.0,                 # Minimum relevance score (0-1)\n    prefer_diverse=False,              # Use MMR for diversity\n)\n</code></pre>"},{"location":"langchain/#wrap_tools_with_headroom","title":"wrap_tools_with_headroom","text":"<pre><code>wrap_tools_with_headroom(\n    tools,                             # List of LangChain tools\n    min_chars_to_compress=1000,        # Minimum output size\n    smart_crusher_config=None,         # SmartCrusher configuration\n)\n</code></pre>"},{"location":"langchain/#import-reference","title":"Import Reference","text":"<pre><code>from headroom.integrations import (\n    # Chat Model\n    HeadroomChatModel,\n\n    # Memory\n    HeadroomChatMessageHistory,\n\n    # Retrievers\n    HeadroomDocumentCompressor,\n\n    # Agents\n    HeadroomToolWrapper,\n    wrap_tools_with_headroom,\n    get_tool_metrics,\n    reset_tool_metrics,\n\n    # Streaming\n    StreamingMetricsTracker,\n    StreamingMetricsCallback,\n    track_streaming_response,\n\n    # LangSmith\n    HeadroomLangSmithCallbackHandler,\n\n    # Provider Detection\n    detect_provider,\n    get_headroom_provider,\n)\n\n# Or import from subpackage directly\nfrom headroom.integrations.langchain import HeadroomChatModel\nfrom headroom.integrations.langchain.memory import HeadroomChatMessageHistory\n</code></pre>"},{"location":"langchain/#troubleshooting","title":"Troubleshooting","text":""},{"location":"langchain/#langchain-not-detected","title":"LangChain not detected","text":"<pre><code>from headroom.integrations import langchain_available\n\nif not langchain_available():\n    print(\"Install with: pip install headroom-ai[langchain]\")\n</code></pre>"},{"location":"langchain/#provider-detection-failing","title":"Provider detection failing","text":"<pre><code># Force a specific provider\nfrom headroom.providers import AnthropicProvider\n\nllm = HeadroomChatModel(\n    ChatAnthropic(model=\"claude-3-5-sonnet-20241022\"),\n    auto_detect_provider=False,\n)\nllm._provider = AnthropicProvider()\n</code></pre>"},{"location":"langchain/#memory-not-compressing","title":"Memory not compressing","text":"<p>Check that your message count exceeds the threshold:</p> <pre><code>history = HeadroomChatMessageHistory(\n    base_history,\n    compress_threshold_tokens=1000,  # Lower threshold\n    keep_recent_turns=2,             # Fewer preserved turns\n)\n</code></pre>"},{"location":"langchain/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use tool wrapping for agents - Agents with tools benefit most from compression</li> <li>Set appropriate thresholds - Don't compress small conversations</li> <li>Enable diversity for RAG - <code>prefer_diverse=True</code> improves answer quality</li> <li>Monitor with LangSmith - Use the callback handler to track savings over time</li> <li>Batch similar requests - Provider caching works better with stable prefixes</li> </ol>"},{"location":"llmlingua/","title":"LLMLingua-2 Integration","text":"<p>For maximum compression, Headroom integrates with LLMLingua-2, Microsoft's BERT-based token classifier trained via GPT-4 distillation. It achieves up to 20x compression while preserving semantic meaning.</p>"},{"location":"llmlingua/#when-to-use-llmlingua-2","title":"When to Use LLMLingua-2","text":"Approach Best For Compression Speed SmartCrusher JSON tool outputs 70-90% ~1ms Text Utilities Search/logs 50-90% ~1ms LLMLingua-2 Any text, max compression 80-95% ~50-200ms <p>LLMLingua-2 is ideal when you need maximum compression and can tolerate slightly higher latency (e.g., compressing large tool outputs before storage, offline processing).</p>"},{"location":"llmlingua/#installation","title":"Installation","text":"<pre><code># Adds ~2GB of model weights\npip install \"headroom-ai[llmlingua]\"\n</code></pre>"},{"location":"llmlingua/#basic-usage","title":"Basic Usage","text":"<pre><code>from headroom.transforms import LLMLinguaCompressor\n\n# Create compressor (model loaded lazily on first use)\ncompressor = LLMLinguaCompressor()\n\n# Compress any text\nlong_output = \"The function processUserData takes a user object and validates...\"\nresult = compressor.compress(long_output)\n\nprint(f\"Before: {result.original_tokens} tokens\")\nprint(f\"After: {result.compressed_tokens} tokens\")\nprint(f\"Saved: {result.savings_percentage:.1f}%\")\nprint(result.compressed)\n</code></pre>"},{"location":"llmlingua/#content-aware-compression","title":"Content-Aware Compression","text":"<p>LLMLingua-2 automatically adjusts compression based on content type:</p> <pre><code>from headroom.transforms import LLMLinguaCompressor, LLMLinguaConfig\n\n# Conservative for code (keep 40% of tokens)\nconfig = LLMLinguaConfig(\n    code_compression_rate=0.4,    # More conservative\n    json_compression_rate=0.35,   # Moderate\n    text_compression_rate=0.25,   # Aggressive\n)\n\ncompressor = LLMLinguaCompressor(config)\n\n# Auto-detects content type\ncode_result = compressor.compress(\"def calculate(x): return x * 2\")\ntext_result = compressor.compress(\"This is a verbose explanation...\")\n</code></pre>"},{"location":"llmlingua/#memory-management","title":"Memory Management","text":"<p>The model uses ~1GB RAM. Unload it when done:</p> <pre><code>from headroom.transforms import (\n    LLMLinguaCompressor,\n    unload_llmlingua_model,\n    is_llmlingua_model_loaded,\n)\n\ncompressor = LLMLinguaCompressor()\nresult = compressor.compress(content)  # Model loaded here\n\n# Check if loaded\nprint(is_llmlingua_model_loaded())  # True\n\n# Free memory when done\nunload_llmlingua_model()  # Frees ~1GB\nprint(is_llmlingua_model_loaded())  # False\n\n# Next compression will reload automatically\n</code></pre>"},{"location":"llmlingua/#device-configuration","title":"Device Configuration","text":"<pre><code>from headroom.transforms import LLMLinguaConfig, LLMLinguaCompressor\n\n# Force CPU (slower but works everywhere)\nconfig = LLMLinguaConfig(device=\"cpu\")\n\n# Force GPU (faster but needs CUDA)\nconfig = LLMLinguaConfig(device=\"cuda\")\n\n# Auto-detect (default): uses CUDA &gt; MPS &gt; CPU\nconfig = LLMLinguaConfig(device=\"auto\")\n\ncompressor = LLMLinguaCompressor(config)\n</code></pre>"},{"location":"llmlingua/#use-in-pipeline","title":"Use in Pipeline","text":"<pre><code>from headroom.transforms import TransformPipeline, LLMLinguaCompressor, SmartCrusher\n\n# Combine with other transforms\npipeline = TransformPipeline([\n    SmartCrusher(),        # First: compress JSON\n    LLMLinguaCompressor(), # Then: ML compression on remaining text\n])\n\nresult = pipeline.apply(messages, tokenizer)\n</code></pre>"},{"location":"llmlingua/#proxy-integration","title":"Proxy Integration","text":"<p>Enable LLMLingua in the proxy server for automatic ML compression:</p> <pre><code># Enable LLMLingua in proxy (requires: pip install headroom-ai[llmlingua,proxy])\nheadroom proxy --llmlingua\n\n# With custom settings\nheadroom proxy --llmlingua --llmlingua-device cuda --llmlingua-rate 0.4\n\n# The proxy shows LLMLingua status at startup:\n#   LLMLingua: ENABLED  (device=cuda, rate=0.4)\n#\n# If llmlingua is installed but not enabled, you'll see a helpful hint:\n#   LLMLingua: available (enable with --llmlingua for ML compression)\n</code></pre>"},{"location":"llmlingua/#configuration-reference","title":"Configuration Reference","text":"Option Default Description <code>device</code> <code>\"auto\"</code> Device to run model on: auto, cpu, cuda, mps <code>code_compression_rate</code> <code>0.4</code> Keep 40% of tokens for code <code>json_compression_rate</code> <code>0.35</code> Keep 35% of tokens for JSON <code>text_compression_rate</code> <code>0.25</code> Keep 25% of tokens for text <code>force_tokens</code> <code>[]</code> Tokens to always preserve <code>drop_consecutive</code> <code>True</code> Drop consecutive whitespace"},{"location":"llmlingua/#performance-characteristics","title":"Performance Characteristics","text":"Metric Value Model size ~500MB Memory usage ~1GB RAM Cold start 10-30s (first load) Inference 50-200ms per request Compression 80-95%"},{"location":"llmlingua/#why-opt-in","title":"Why Opt-In?","text":"<p>LLMLingua adds significant dependencies and overhead:</p> Aspect Default Proxy With LLMLingua Dependencies ~50MB ~2GB Cold start &lt;1s 10-30s Per-request ~1-5ms ~50-200ms Compression 70-90% 80-95% <p>The default proxy is lightweight and fast. Enable LLMLingua when you need maximum compression and can accept the tradeoffs.</p>"},{"location":"llmlingua/#troubleshooting","title":"Troubleshooting","text":""},{"location":"llmlingua/#model-not-found","title":"\"Model not found\"","text":"<pre><code># Ensure llmlingua extra is installed\npip install \"headroom-ai[llmlingua]\"\n</code></pre>"},{"location":"llmlingua/#cuda-out-of-memory","title":"\"CUDA out of memory\"","text":"<pre><code># Force CPU mode\nconfig = LLMLinguaConfig(device=\"cpu\")\n</code></pre>"},{"location":"llmlingua/#slow-compression","title":"\"Slow compression\"","text":"<ul> <li>Use GPU if available: <code>device=\"cuda\"</code></li> <li>Batch multiple compressions</li> <li>Consider using SmartCrusher for JSON (faster, similar results)</li> </ul>"},{"location":"macos-deployment/","title":"macOS Deployment Guide","text":"<p>This guide covers deploying the headroom proxy server as a background service on macOS using LaunchAgent. The service will start automatically on login and restart on crash.</p>"},{"location":"macos-deployment/#overview","title":"Overview","text":"<p>macOS LaunchAgent provides a native way to run background services with:</p> <ul> <li>Automatic startup on user login</li> <li>Crash recovery with automatic restart</li> <li>Standard logging to <code>~/Library/Logs/</code></li> <li>Native lifecycle management via <code>launchctl</code></li> </ul> <p>This is ideal for local development environments where you want \"set and forget\" proxy configuration.</p>"},{"location":"macos-deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>macOS 10.13+ (High Sierra or later)</li> <li>headroom-ai installed with proxy support</li> <li>Anthropic API key configured</li> </ul>"},{"location":"macos-deployment/#installing-headroom-with-proxy-support","title":"Installing Headroom with Proxy Support","text":"<pre><code># Install with proxy support\npip install headroom-ai[proxy]\n\n# Verify installation\nheadroom proxy --help\n</code></pre>"},{"location":"macos-deployment/#api-key-configuration","title":"API Key Configuration","text":"<p>Your Anthropic API key can be configured in several ways:</p> <p>Option 1: Shell environment (recommended)</p> <pre><code># Add to ~/.bashrc or ~/.zshrc\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre> <p>Option 2: LaunchAgent plist</p> <pre><code>&lt;key&gt;EnvironmentVariables&lt;/key&gt;\n&lt;dict&gt;\n    &lt;key&gt;ANTHROPIC_API_KEY&lt;/key&gt;\n    &lt;string&gt;sk-ant-...&lt;/string&gt;\n&lt;/dict&gt;\n</code></pre> <p>Option 3: System environment</p> <pre><code># Add to /etc/launchd.conf (requires admin)\nsetenv ANTHROPIC_API_KEY sk-ant-...\n</code></pre>"},{"location":"macos-deployment/#quick-install","title":"Quick Install","text":"<p>The automated installer handles all setup:</p> <pre><code># Clone or navigate to headroom repository\ncd examples/deployment/macos-launchagent\n\n# Run installer\n./install.sh\n</code></pre> <p>The installer will:</p> <ol> <li>Detect your headroom installation</li> <li>Prompt for port configuration (default: 8787)</li> <li>Create log directory</li> <li>Generate LaunchAgent plist</li> <li>Load and start the service</li> <li>Verify service is running</li> </ol>"},{"location":"macos-deployment/#installation-options","title":"Installation Options","text":"<p>Custom port:</p> <pre><code>./install.sh --port 9000\n</code></pre> <p>Unattended install (no prompts):</p> <pre><code>./install.sh --port 8787 --unattended\n</code></pre> <p>Reinstall over existing:</p> <pre><code># Installer will prompt to reinstall if service exists\n./install.sh\n</code></pre>"},{"location":"macos-deployment/#manual-installation","title":"Manual Installation","text":"<p>If you prefer full control over the installation:</p>"},{"location":"macos-deployment/#step-1-create-log-directory","title":"Step 1: Create Log Directory","text":"<pre><code>mkdir -p ~/Library/Logs/headroom\n</code></pre>"},{"location":"macos-deployment/#step-2-generate-launchagent-plist","title":"Step 2: Generate LaunchAgent Plist","text":"<p>Copy and customize the template:</p> <pre><code>cd examples/deployment/macos-launchagent\ncp com.headroom.proxy.plist.template ~/Library/LaunchAgents/com.headroom.proxy.plist\n</code></pre> <p>Edit <code>~/Library/LaunchAgents/com.headroom.proxy.plist</code>:</p> <ol> <li>Replace <code>__HEADROOM_PATH__</code> with your headroom path:</li> </ol> <pre><code>command -v headroom\n# Example output: /usr/local/bin/headroom\n</code></pre> <ol> <li> <p>Replace <code>__PORT__</code> with your desired port (e.g., <code>8787</code>)</p> </li> <li> <p>Replace <code>__HOME__</code> with your home directory:</p> </li> </ol> <pre><code>echo $HOME\n# Example output: /Users/yourusername\n</code></pre>"},{"location":"macos-deployment/#step-3-load-the-launchagent","title":"Step 3: Load the LaunchAgent","text":"<pre><code>launchctl bootstrap gui/$(id -u) ~/Library/LaunchAgents/com.headroom.proxy.plist\n</code></pre>"},{"location":"macos-deployment/#step-4-verify-service","title":"Step 4: Verify Service","text":"<pre><code># Check if service is running\nlaunchctl print gui/$(id -u)/com.headroom.proxy\n\n# Check if port is listening\nlsof -iTCP:8787 -sTCP:LISTEN\n\n# Test health endpoint\ncurl http://localhost:8787/health\n</code></pre>"},{"location":"macos-deployment/#configuration","title":"Configuration","text":""},{"location":"macos-deployment/#port-customization","title":"Port Customization","text":"<p>The default port is 8787. To use a custom port:</p> <p>During installation:</p> <pre><code>./install.sh --port 9000\n</code></pre> <p>After installation:</p> <ol> <li>Uninstall: <code>./uninstall.sh</code></li> <li>Reinstall with new port: <code>./install.sh --port 9000</code></li> <li>Update shell integration: <code>export HEADROOM_PROXY_PORT=9000</code></li> </ol>"},{"location":"macos-deployment/#log-location","title":"Log Location","text":"<p>Logs are written to standard macOS locations:</p> <ul> <li>Standard output: <code>~/Library/Logs/headroom/proxy.log</code></li> <li>Error output: <code>~/Library/Logs/headroom/proxy-error.log</code></li> </ul> <p>To change log locations, edit the plist:</p> <pre><code>&lt;key&gt;StandardOutPath&lt;/key&gt;\n&lt;string&gt;/custom/path/proxy.log&lt;/string&gt;\n</code></pre>"},{"location":"macos-deployment/#environment-variables","title":"Environment Variables","text":"<p>Configure additional options in the plist <code>EnvironmentVariables</code> section:</p> <pre><code>&lt;key&gt;EnvironmentVariables&lt;/key&gt;\n&lt;dict&gt;\n    &lt;!-- Required: Proxy port --&gt;\n    &lt;key&gt;HEADROOM_PROXY_PORT&lt;/key&gt;\n    &lt;string&gt;8787&lt;/string&gt;\n\n    &lt;!-- Optional: API key (or set in shell) --&gt;\n    &lt;key&gt;ANTHROPIC_API_KEY&lt;/key&gt;\n    &lt;string&gt;sk-ant-...&lt;/string&gt;\n\n    &lt;!-- Optional: Enable LLMLingua compression --&gt;\n    &lt;key&gt;HEADROOM_COMPRESSION_PROVIDER&lt;/key&gt;\n    &lt;string&gt;llmlingua&lt;/string&gt;\n\n    &lt;!-- Optional: LLMLingua device (auto, cuda, cpu, mps) --&gt;\n    &lt;key&gt;HEADROOM_LLMLINGUA_DEVICE&lt;/key&gt;\n    &lt;string&gt;mps&lt;/string&gt;\n&lt;/dict&gt;\n</code></pre> <p>Note: LLMLingua requires additional installation:</p> <pre><code>pip install headroom-ai[llmlingua]\n</code></pre>"},{"location":"macos-deployment/#crash-recovery","title":"Crash Recovery","text":"<p>The LaunchAgent is configured with:</p> <ul> <li>KeepAlive: Automatically restarts on crash</li> <li>ThrottleInterval: 10 seconds between restart attempts</li> </ul> <p>To disable automatic restart, edit the plist:</p> <pre><code>&lt;key&gt;KeepAlive&lt;/key&gt;\n&lt;false/&gt;\n</code></pre>"},{"location":"macos-deployment/#shell-integration","title":"Shell Integration","text":"<p>Automatically configure your shell to use the proxy when available.</p>"},{"location":"macos-deployment/#setup","title":"Setup","text":"<p>Add to <code>~/.bashrc</code> (bash) or <code>~/.zshrc</code> (zsh):</p> <pre><code># Configure port (optional, defaults to 8787)\nexport HEADROOM_PROXY_PORT=8787\n\n# Source shell integration\nsource /path/to/headroom/examples/deployment/macos-launchagent/shell-integration.sh\n</code></pre>"},{"location":"macos-deployment/#what-it-does","title":"What It Does","text":"<p>The shell integration script:</p> <ol> <li>Checks if proxy is running on configured port</li> <li>If running, sets <code>ANTHROPIC_BASE_URL=http://localhost:8787</code></li> <li>If not running, attempts to start the LaunchAgent</li> <li>Provides status messages on first load</li> </ol> <p>This makes Claude clients automatically use the proxy without manual configuration.</p>"},{"location":"macos-deployment/#manual-configuration","title":"Manual Configuration","text":"<p>If you prefer not to use shell integration:</p> <pre><code># Add to ~/.bashrc or ~/.zshrc\nexport ANTHROPIC_BASE_URL=http://localhost:8787\n</code></pre>"},{"location":"macos-deployment/#service-management","title":"Service Management","text":""},{"location":"macos-deployment/#check-status","title":"Check Status","text":"<pre><code># View service status\nlaunchctl print gui/$(id -u)/com.headroom.proxy\n\n# Check if port is listening\nlsof -iTCP:8787 -sTCP:LISTEN\n\n# Test health endpoint\ncurl http://localhost:8787/health\n</code></pre>"},{"location":"macos-deployment/#view-logs","title":"View Logs","text":"<pre><code># Tail standard output\ntail -f ~/Library/Logs/headroom/proxy.log\n\n# Tail error output\ntail -f ~/Library/Logs/headroom/proxy-error.log\n\n# View last 50 lines\ntail -n 50 ~/Library/Logs/headroom/proxy-error.log\n</code></pre>"},{"location":"macos-deployment/#restart-service","title":"Restart Service","text":"<pre><code># Graceful restart (stop and let KeepAlive restart it)\nlaunchctl kickstart -k gui/$(id -u)/com.headroom.proxy\n\n# Manual stop/start\nlaunchctl bootout gui/$(id -u)/com.headroom.proxy\nlaunchctl bootstrap gui/$(id -u) ~/Library/LaunchAgents/com.headroom.proxy.plist\n</code></pre>"},{"location":"macos-deployment/#stop-service-temporarily","title":"Stop Service Temporarily","text":"<pre><code># Disable without uninstalling\nlaunchctl disable gui/$(id -u)/com.headroom.proxy\n\n# Re-enable\nlaunchctl enable gui/$(id -u)/com.headroom.proxy\n</code></pre>"},{"location":"macos-deployment/#verification","title":"Verification","text":"<p>After installation, verify everything is working:</p>"},{"location":"macos-deployment/#1-check-service-status","title":"1. Check Service Status","text":"<pre><code>launchctl print gui/$(id -u)/com.headroom.proxy\n</code></pre> <p>Expected output includes:</p> <pre><code>state = running\n</code></pre>"},{"location":"macos-deployment/#2-check-port","title":"2. Check Port","text":"<pre><code>lsof -iTCP:8787 -sTCP:LISTEN\n</code></pre> <p>Should show headroom listening on port 8787.</p>"},{"location":"macos-deployment/#3-test-health-endpoint","title":"3. Test Health Endpoint","text":"<pre><code>curl http://localhost:8787/health\n</code></pre> <p>Expected response:</p> <pre><code>{\"status\": \"healthy\"}\n</code></pre>"},{"location":"macos-deployment/#4-test-proxy-functionality","title":"4. Test Proxy Functionality","text":"<pre><code># Set base URL\nexport ANTHROPIC_BASE_URL=http://localhost:8787\n\n# Test with Python\npython -c \"\nimport anthropic\nclient = anthropic.Anthropic()\nresponse = client.messages.create(\n    model='claude-3-5-sonnet-20241022',\n    max_tokens=50,\n    messages=[{'role': 'user', 'content': 'Hi'}]\n)\nprint(response.content[0].text)\n\"\n</code></pre>"},{"location":"macos-deployment/#5-check-logs-for-errors","title":"5. Check Logs for Errors","text":"<pre><code>tail -n 20 ~/Library/Logs/headroom/proxy-error.log\n</code></pre> <p>Should show no errors. Common startup errors are listed in Troubleshooting.</p>"},{"location":"macos-deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"macos-deployment/#service-wont-start","title":"Service Won't Start","text":"<p>Symptom: <code>launchctl print</code> shows service not loaded or failed state</p> <p>Check logs:</p> <pre><code>tail -n 50 ~/Library/Logs/headroom/proxy-error.log\n</code></pre> <p>Common causes:</p> Error Solution <code>ANTHROPIC_API_KEY not set</code> Set API key in environment or plist <code>ModuleNotFoundError: No module named 'headroom'</code> Install: <code>pip install headroom-ai[proxy]</code> <code>command not found: headroom</code> Update plist with correct path: <code>command -v headroom</code> <code>Address already in use</code> Change port or stop conflicting service"},{"location":"macos-deployment/#port-already-in-use","title":"Port Already in Use","text":"<p>Symptom: Service starts but port not listening, logs show \"Address already in use\"</p> <p>Find what's using the port:</p> <pre><code>lsof -iTCP:8787 -sTCP:LISTEN\n</code></pre> <p>Solutions:</p> <ol> <li>Stop conflicting service</li> <li>Use different port: <code>./uninstall.sh &amp;&amp; ./install.sh --port 9000</code></li> </ol>"},{"location":"macos-deployment/#service-crashes-immediately","title":"Service Crashes Immediately","text":"<p>Symptom: Service starts but immediately exits</p> <p>Check for Python errors:</p> <pre><code>tail -f ~/Library/Logs/headroom/proxy-error.log\n</code></pre> <p>Common causes:</p> <ul> <li>Missing dependencies: <code>pip install headroom-ai[proxy]</code></li> <li>Invalid API key: Verify <code>ANTHROPIC_API_KEY</code></li> <li>Python version incompatible: Requires Python 3.9+</li> </ul>"},{"location":"macos-deployment/#anthropic_base_url-not-set","title":"ANTHROPIC_BASE_URL Not Set","text":"<p>Symptom: Shell integration not setting environment variable</p> <p>Verify proxy is running:</p> <pre><code>curl http://localhost:8787/health\n</code></pre> <p>Reload shell configuration:</p> <pre><code>source ~/.bashrc  # or ~/.zshrc\n</code></pre> <p>Check shell integration is sourced:</p> <pre><code># Should be set to 1\necho $HEADROOM_SHELL_INTEGRATION_LOADED\n</code></pre>"},{"location":"macos-deployment/#service-not-auto-starting-on-login","title":"Service Not Auto-Starting on Login","text":"<p>Symptom: Service doesn't start after reboot</p> <p>Verify LaunchAgent is loaded:</p> <pre><code>launchctl list | grep headroom\n</code></pre> <p>If not listed:</p> <pre><code>launchctl bootstrap gui/$(id -u) ~/Library/LaunchAgents/com.headroom.proxy.plist\n</code></pre> <p>Check RunAtLoad is enabled:</p> <pre><code>grep -A1 RunAtLoad ~/Library/LaunchAgents/com.headroom.proxy.plist\n</code></pre> <p>Should show:</p> <pre><code>&lt;key&gt;RunAtLoad&lt;/key&gt;\n&lt;true/&gt;\n</code></pre>"},{"location":"macos-deployment/#permission-issues","title":"Permission Issues","text":"<p>Symptom: \"Operation not permitted\" errors</p> <p>Ensure plist has correct permissions:</p> <pre><code>chmod 644 ~/Library/LaunchAgents/com.headroom.proxy.plist\n</code></pre> <p>Verify ownership:</p> <pre><code>ls -l ~/Library/LaunchAgents/com.headroom.proxy.plist\n</code></pre> <p>Should be owned by your user, not root.</p>"},{"location":"macos-deployment/#uninstallation","title":"Uninstallation","text":""},{"location":"macos-deployment/#quick-uninstall","title":"Quick Uninstall","text":"<pre><code>cd examples/deployment/macos-launchagent\n./uninstall.sh\n</code></pre> <p>This will:</p> <ol> <li>Stop the service</li> <li>Remove LaunchAgent plist</li> <li>Optionally remove log directory (prompts)</li> </ol>"},{"location":"macos-deployment/#remove-everything","title":"Remove Everything","text":"<pre><code># Uninstall service and remove logs\n./uninstall.sh --remove-logs\n\n# Remove shell integration from ~/.bashrc or ~/.zshrc\n# Delete or comment out:\n#   export HEADROOM_PROXY_PORT=8787\n#   source .../shell-integration.sh\n</code></pre>"},{"location":"macos-deployment/#manual-uninstall","title":"Manual Uninstall","text":"<pre><code># Stop service\nlaunchctl bootout gui/$(id -u)/com.headroom.proxy\n\n# Remove plist\nrm ~/Library/LaunchAgents/com.headroom.proxy.plist\n\n# Remove logs (optional)\nrm -rf ~/Library/Logs/headroom\n</code></pre>"},{"location":"macos-deployment/#production-deployment","title":"Production Deployment","text":"<p>For production environments, consider:</p> <ul> <li>System-wide LaunchDaemon instead of per-user LaunchAgent</li> <li>Resource limits in plist (CPU, memory)</li> <li>Log rotation for long-running deployments</li> <li>Monitoring via external tools</li> <li>Multiple instances on different ports for redundancy</li> </ul> <p>LaunchAgent is designed for single-user development. For production, evaluate:</p> <ul> <li>Docker deployment for containerized environments</li> <li>systemd on Linux servers</li> <li>Cloud-native solutions (ECS, Cloud Run, etc.)</li> </ul>"},{"location":"macos-deployment/#related-documentation","title":"Related Documentation","text":"<ul> <li>Proxy Server Documentation - Core proxy configuration and features</li> <li>Configuration Guide - Detailed configuration options</li> <li>Architecture - How Headroom works internally</li> <li>Troubleshooting - General troubleshooting guide</li> </ul>"},{"location":"macos-deployment/#platform-alternatives","title":"Platform Alternatives","text":"<ul> <li>Linux: Use systemd instead of LaunchAgent</li> <li>Windows: Use Task Scheduler or NSSM (Non-Sucking Service Manager)</li> <li>Docker: See proxy.md for containerized deployment</li> </ul>"},{"location":"macos-deployment/#security-considerations","title":"Security Considerations","text":""},{"location":"macos-deployment/#launchagent-vs-launchdaemon","title":"LaunchAgent vs LaunchDaemon","text":"<p>LaunchAgent (used here):</p> <ul> <li>Runs in user context</li> <li>No root privileges required</li> <li>Starts on user login</li> <li>Per-user isolation</li> </ul> <p>LaunchDaemon (not covered):</p> <ul> <li>Runs as root or specific user</li> <li>System-wide service</li> <li>Starts on boot</li> <li>Requires admin privileges</li> </ul> <p>For single-user development, LaunchAgent is recommended for security.</p>"},{"location":"macos-deployment/#api-key-security","title":"API Key Security","text":"<p>Store API keys securely:</p> <ul> <li>\u2705 Use environment variables in shell config</li> <li>\u2705 Use macOS Keychain (advanced)</li> <li>\u2705 Restrict plist file permissions: <code>chmod 600</code></li> <li>\u274c Don't commit API keys to version control</li> <li>\u274c Don't store in world-readable files</li> </ul>"},{"location":"macos-deployment/#network-security","title":"Network Security","text":"<p>The proxy binds to <code>127.0.0.1</code> (localhost only) by default:</p> <ul> <li>\u2705 Only accessible from local machine</li> <li>\u2705 No external network exposure</li> <li>\u274c Don't bind to <code>0.0.0.0</code> without firewall rules</li> </ul>"},{"location":"macos-deployment/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"macos-deployment/#multiple-proxy-instances","title":"Multiple Proxy Instances","text":"<p>Run multiple proxies on different ports:</p> <pre><code># Install first instance\n./install.sh --port 8787\n\n# For second instance, manually create plist with different label\ncp com.headroom.proxy.plist.template ~/Library/LaunchAgents/com.headroom.proxy-2.plist\n# Edit: Change Label to com.headroom.proxy-2, port to 8788\nlaunchctl bootstrap gui/$(id -u) ~/Library/LaunchAgents/com.headroom.proxy-2.plist\n</code></pre>"},{"location":"macos-deployment/#custom-launchagent-schedule","title":"Custom LaunchAgent Schedule","text":"<p>Run proxy only during business hours:</p> <pre><code>&lt;!-- Add to plist --&gt;\n&lt;key&gt;StartCalendarInterval&lt;/key&gt;\n&lt;dict&gt;\n    &lt;key&gt;Hour&lt;/key&gt;\n    &lt;integer&gt;9&lt;/integer&gt;\n    &lt;key&gt;Minute&lt;/key&gt;\n    &lt;integer&gt;0&lt;/integer&gt;\n&lt;/dict&gt;\n</code></pre>"},{"location":"macos-deployment/#resource-limits","title":"Resource Limits","text":"<p>Limit CPU and memory usage:</p> <pre><code>&lt;!-- Add to plist --&gt;\n&lt;key&gt;HardResourceLimits&lt;/key&gt;\n&lt;dict&gt;\n    &lt;key&gt;NumberOfProcesses&lt;/key&gt;\n    &lt;integer&gt;1&lt;/integer&gt;\n    &lt;key&gt;MemoryMax&lt;/key&gt;\n    &lt;integer&gt;536870912&lt;/integer&gt; &lt;!-- 512 MB --&gt;\n&lt;/dict&gt;\n</code></pre>"},{"location":"macos-deployment/#faq","title":"FAQ","text":"<p>Q: Why LaunchAgent instead of running <code>headroom proxy</code> manually?</p> <p>A: LaunchAgent provides automatic startup, crash recovery, and proper lifecycle management. You don't have to remember to start the proxy or keep a terminal window open.</p> <p>Q: Can I use this in production?</p> <p>A: LaunchAgent is designed for development. For production, use Docker, systemd, or cloud-native deployment.</p> <p>Q: How much does the proxy impact performance?</p> <p>A: Minimal. The proxy adds ~10-50ms latency while reducing token costs by 50-90%. The cost savings far outweigh the latency.</p> <p>Q: Do I need to restart the proxy when configuration changes?</p> <p>A: Yes. After changing the plist, reload the service:</p> <pre><code>launchctl kickstart -k gui/$(id -u)/com.headroom.proxy\n</code></pre> <p>Q: Can I use this with multiple API providers?</p> <p>A: The LaunchAgent setup is Anthropic-specific. For other providers, see proxy.md for configuration options.</p> <p>Q: Does this work with Apple Silicon (M1/M2/M3)?</p> <p>A: Yes, fully compatible. For LLMLingua compression, use <code>--llmlingua-device mps</code> for Apple Silicon acceleration.</p>"},{"location":"mcp/","title":"MCP Server for Claude Code Subscriptions","text":"<p>Headroom's MCP (Model Context Protocol) server enables CCR (Compress-Cache-Retrieve) for Claude Code subscription users who don't have direct API access.</p>"},{"location":"mcp/#quick-start","title":"Quick Start","text":"<pre><code># Install MCP dependencies\npip install \"headroom-ai[mcp]\"\n\n# Configure Claude Code (one-time)\nheadroom mcp install\n\n# Start the proxy\nheadroom proxy\n\n# Use Claude Code - it now has headroom_retrieve!\nclaude\n</code></pre>"},{"location":"mcp/#why-mcp","title":"Why MCP?","text":"Authentication Custom Tools Solution API Key Direct injection via Messages API Works automatically Subscription Claude Code's built-in tools only MCP server <p>Claude Code subscription users can't inject custom tools programmatically. MCP is Claude's official extension mechanism that works with subscriptions.</p>"},{"location":"mcp/#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Claude Code   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Headroom Proxy \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   LLM Provider  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502\n         \u2502 MCP                   \u2502 Stores compressed\n         \u2502                       \u2502 content\n         \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   MCP Server    \u2502\u25c0\u2500\u2500\u2500\u25b6\u2502 Compression     \u2502\n\u2502 (headroom_      \u2502     \u2502 Store           \u2502\n\u2502  retrieve)      \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ol> <li>Proxy compresses large tool outputs (file listings, search results, logs)</li> <li>Claude sees compressed summaries with hash markers: <code>[47 items compressed... hash=abc123]</code></li> <li>When needed, Claude calls <code>headroom_retrieve</code> to get the original content</li> <li>MCP server fetches from the proxy's compression store</li> </ol>"},{"location":"mcp/#cli-commands","title":"CLI Commands","text":""},{"location":"mcp/#install-mcp-configuration","title":"Install MCP Configuration","text":"<pre><code>headroom mcp install\n</code></pre> <p>This writes to <code>~/.claude/mcp.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"headroom\": {\n      \"command\": \"headroom\",\n      \"args\": [\"mcp\", \"serve\"]\n    }\n  }\n}\n</code></pre> <p>Options: - <code>--proxy-url URL</code> - Custom proxy URL (default: <code>http://127.0.0.1:8787</code>) - <code>--force</code> - Overwrite existing configuration</p>"},{"location":"mcp/#check-status","title":"Check Status","text":"<pre><code>headroom mcp status\n</code></pre> <p>Shows: - MCP SDK installation status - Claude Code configuration status - Proxy connectivity</p> <p>Example output: <pre><code>Headroom MCP Status\n========================================\nMCP SDK:        \u2713 Installed\nClaude Config:  \u2713 Configured\n                /Users/you/.claude/mcp.json\nProxy URL:      http://127.0.0.1:8787\nProxy Status:   \u2713 Running at http://127.0.0.1:8787\n</code></pre></p>"},{"location":"mcp/#uninstall","title":"Uninstall","text":"<pre><code>headroom mcp uninstall\n</code></pre> <p>Removes headroom from <code>~/.claude/mcp.json</code> while preserving other MCP servers.</p>"},{"location":"mcp/#manual-server-start","title":"Manual Server Start","text":"<pre><code>headroom mcp serve\n</code></pre> <p>This is called by Claude Code automatically. For debugging:</p> <pre><code>headroom mcp serve --debug\n</code></pre>"},{"location":"mcp/#the-headroom_retrieve-tool","title":"The headroom_retrieve Tool","text":"<p>When the MCP server is active, Claude has access to:</p> <pre><code>Tool: headroom_retrieve\n\nParameters:\n  - hash (required): Hash key from compression marker\n  - query (optional): Search query to filter results\n\nReturns:\n  - Full original content, or\n  - Filtered results matching query\n</code></pre> <p>Example interaction:</p> <pre><code>Claude sees:\n  [47 log entries compressed. Showing first 3 + anomalies.\n   Use headroom_retrieve(hash=\"a1b2c3\") for full logs]\n\nClaude calls:\n  headroom_retrieve(hash=\"a1b2c3\", query=\"error\")\n\nReturns:\n  [All log entries containing \"error\"]\n</code></pre>"},{"location":"mcp/#custom-proxy-url","title":"Custom Proxy URL","text":"<p>If your proxy runs on a different port:</p> <pre><code># During install\nheadroom mcp install --proxy-url http://localhost:9000\n\n# Or via environment variable\nexport HEADROOM_PROXY_URL=http://localhost:9000\nheadroom mcp serve\n</code></pre>"},{"location":"mcp/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mcp/#mcp-sdk-not-installed","title":"\"MCP SDK not installed\"","text":"<pre><code>pip install \"headroom-ai[mcp]\"\n</code></pre>"},{"location":"mcp/#proxy-not-running","title":"\"Proxy not running\"","text":"<p>Start the proxy in another terminal:</p> <pre><code>headroom proxy\n</code></pre>"},{"location":"mcp/#entry-not-found-or-expired","title":"\"Entry not found or expired\"","text":"<p>Compressed entries expire after 5 minutes (TTL). The proxy must be running continuously during your session.</p>"},{"location":"mcp/#claude-doesnt-see-headroom_retrieve","title":"Claude doesn't see headroom_retrieve","text":"<ol> <li>Check status: <code>headroom mcp status</code></li> <li>Restart Claude Code after installing MCP</li> <li>Verify <code>~/.claude/mcp.json</code> exists and contains headroom</li> </ol>"},{"location":"mcp/#api-users","title":"API Users","text":"<p>If you have an <code>ANTHROPIC_API_KEY</code>, you don't need MCP. The proxy automatically injects the <code>headroom_retrieve</code> tool into API requests.</p> <p>MCP is specifically for subscription users who authenticate via Claude Code's OAuth flow rather than an API key.</p>"},{"location":"memory/","title":"Memory","text":"<p>Hierarchical, temporal memory for LLM applications. Enable your AI to remember across conversations with intelligent scoping and versioning.</p>"},{"location":"memory/#why-memory","title":"Why Memory?","text":"<p>LLMs have two fundamental limitations: 1. Context windows overflow - Too much history, need to truncate 2. No persistence - Every conversation starts from zero</p> <p>Memory solves both: extract key facts, persist them, inject when relevant.</p> <p>This is temporal compression - instead of carrying 10,000 tokens of conversation history, carry 100 tokens of extracted memories.</p>"},{"location":"memory/#what-makes-headroom-memory-different","title":"What Makes Headroom Memory Different?","text":"Feature Headroom Letta (MemGPT) Mem0 Hierarchical Scoping User \u2192 Session \u2192 Agent \u2192 Turn Flat (per-agent) Flat (per-user) Temporal Versioning Full supersession chains No No Zero-Latency Extraction Inline (Letta-style) Inline Separate call One-Liner Integration <code>with_memory(client)</code> Requires agent setup Requires separate client Pluggable Backends SQLite, HNSW, FTS5, any embedder PostgreSQL Qdrant/Chroma Semantic + Full-Text Search Both Semantic only Semantic only Memory Bubbling Auto-promote important memories No No Protocol-Based Architecture Yes (dependency injection) No No"},{"location":"memory/#quick-start","title":"Quick Start","text":"<pre><code>from openai import OpenAI\nfrom headroom import with_memory\n\n# One line - that's it\nclient = with_memory(OpenAI(), user_id=\"alice\")\n\n# Use exactly like normal\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"I prefer Python for backend work\"}]\n)\n# Memory extracted INLINE - zero extra latency\n\n# Later, in a new conversation...\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What language should I use?\"}]\n)\n# \u2192 Response uses the Python preference from memory\n</code></pre>"},{"location":"memory/#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      with_memory()                          \u2502\n\u2502                                                              \u2502\n\u2502   1. INJECT: Semantic search \u2192 prepend to user message      \u2502\n\u2502   2. INSTRUCT: Add memory extraction instruction            \u2502\n\u2502   3. CALL: Forward to LLM                                   \u2502\n\u2502   4. PARSE: Extract &lt;memory&gt; block from response            \u2502\n\u2502   5. STORE: Save with embeddings + vector index + FTS       \u2502\n\u2502   6. RETURN: Clean response (without memory block)          \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key insight: Memory extraction happens inline as part of the LLM response (Letta-style). No extra API calls, no extra latency.</p>"},{"location":"memory/#hierarchical-scoping","title":"Hierarchical Scoping","text":"<p>Memories exist at different scope levels, enabling fine-grained control:</p> <pre><code>USER (broadest)\n \u2514\u2500\u2500 SESSION\n      \u2514\u2500\u2500 AGENT\n           \u2514\u2500\u2500 TURN (narrowest)\n</code></pre>"},{"location":"memory/#scope-levels","title":"Scope Levels","text":"Scope Persists Across Use Case USER All sessions, all time Long-term preferences, identity SESSION Current session only Current task context AGENT Current agent in session Agent-specific context TURN Single turn only Ephemeral working memory"},{"location":"memory/#example-multi-session-memory","title":"Example: Multi-Session Memory","text":"<pre><code>from openai import OpenAI\nfrom headroom import with_memory\n\n# Session 1: Morning\nclient1 = with_memory(\n    OpenAI(),\n    user_id=\"bob\",\n    session_id=\"morning-session\",\n)\nresponse = client1.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"I prefer Go for performance-critical code\"}]\n)\n# Memory stored at USER level (persists across sessions)\n\n# Session 2: Afternoon (different session, same user)\nclient2 = with_memory(\n    OpenAI(),\n    user_id=\"bob\",  # Same user\n    session_id=\"afternoon-session\",  # Different session\n)\nresponse = client2.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What language for my new microservice?\"}]\n)\n# \u2192 Recalls Go preference from morning session!\n</code></pre>"},{"location":"memory/#temporal-versioning-supersession","title":"Temporal Versioning (Supersession)","text":"<p>Memories evolve over time. When facts change, Headroom creates a supersession chain preserving history:</p> <pre><code>from headroom.memory import HierarchicalMemory, MemoryConfig\n\nmemory = await HierarchicalMemory.create()\n\n# Original fact\norig = await memory.add(\n    content=\"User works at Google\",\n    user_id=\"alice\",\n    category=MemoryCategory.FACT,\n)\n\n# User changes jobs - supersede the old memory\nnew = await memory.supersede(\n    old_memory_id=orig.id,\n    new_content=\"User now works at Anthropic\",\n)\n\n# Query current state (excludes superseded)\ncurrent = await memory.query(MemoryFilter(\n    user_id=\"alice\",\n    include_superseded=False,  # Default\n))\n# \u2192 Returns only \"User now works at Anthropic\"\n\n# Query full history (includes superseded)\nhistory = await memory.query(MemoryFilter(\n    user_id=\"alice\",\n    include_superseded=True,\n))\n# \u2192 Returns both memories with validity timestamps\n\n# Get the chain\nchain = await memory.get_history(new.id)\n# \u2192 [\n#     Memory(content=\"User works at Google\", valid_until=..., is_current=False),\n#     Memory(content=\"User now works at Anthropic\", valid_until=None, is_current=True),\n#   ]\n</code></pre>"},{"location":"memory/#why-temporal-versioning-matters","title":"Why Temporal Versioning Matters","text":"<ol> <li>Audit trail - Know what was true at any point in time</li> <li>Debugging - Understand why the LLM made certain decisions</li> <li>Rollback - Restore previous state if needed</li> <li>Analytics - Track how user preferences evolve</li> </ol>"},{"location":"memory/#memory-categories","title":"Memory Categories","text":"<p>Memories are categorized for better organization and retrieval:</p> Category Description Examples <code>PREFERENCE</code> Likes, dislikes, preferred approaches \"Prefers Python\", \"Likes dark mode\" <code>FACT</code> Identity, role, constraints \"Works at fintech startup\", \"Senior engineer\" <code>CONTEXT</code> Current goals, ongoing tasks \"Migrating to microservices\", \"Working on auth\" <code>ENTITY</code> Information about entities \"Project Apollo uses React\", \"Team lead is Sarah\" <code>DECISION</code> Decisions made \"Chose PostgreSQL over MySQL\", \"Using REST not GraphQL\" <code>INSIGHT</code> Derived insights \"User tends to prefer typed languages\""},{"location":"memory/#memory-api","title":"Memory API","text":"<p>The <code>with_memory()</code> wrapper provides a <code>.memory</code> API for direct access:</p> <pre><code>client = with_memory(OpenAI(), user_id=\"alice\")\n\n# Search memories (semantic)\nresults = client.memory.search(\"python preferences\", top_k=5)\nfor memory in results:\n    print(f\"{memory.content}\")\n\n# Add manual memory\nclient.memory.add(\n    \"User is a senior engineer\",\n    category=\"fact\",\n    importance=0.9,\n)\n\n# Get all memories\nall_memories = client.memory.get_all()\n\n# Clear memories\nclient.memory.clear()\n\n# Get stats\nstats = client.memory.stats()\nprint(f\"Total memories: {stats['total']}\")\nprint(f\"By category: {stats['categories']}\")\n</code></pre>"},{"location":"memory/#advanced-usage-direct-hierarchicalmemory-api","title":"Advanced Usage: Direct HierarchicalMemory API","text":"<p>For full control, use the <code>HierarchicalMemory</code> class directly:</p> <pre><code>import asyncio\nfrom headroom.memory import (\n    HierarchicalMemory,\n    MemoryConfig,\n    MemoryCategory,\n    EmbedderBackend,\n)\nfrom headroom.memory.ports import MemoryFilter, VectorFilter\n\nasync def main():\n    # Create with custom configuration\n    config = MemoryConfig(\n        db_path=\"my_memory.db\",\n        embedder_backend=EmbedderBackend.LOCAL,  # or OPENAI, OLLAMA\n        vector_dimension=384,\n        cache_max_size=2000,\n    )\n    memory = await HierarchicalMemory.create(config)\n\n    # Add memory with full control\n    mem = await memory.add(\n        content=\"User prefers functional programming\",\n        user_id=\"alice\",\n        session_id=\"sess-123\",\n        agent_id=\"code-assistant\",\n        category=MemoryCategory.PREFERENCE,\n        importance=0.9,\n        entity_refs=[\"functional-programming\", \"coding-style\"],\n        metadata={\"source\": \"conversation\", \"confidence\": 0.95},\n    )\n\n    # Semantic search\n    results = await memory.search(\n        query=\"programming paradigm preferences\",\n        user_id=\"alice\",\n        top_k=5,\n        min_similarity=0.5,\n        categories=[MemoryCategory.PREFERENCE],\n    )\n    for r in results:\n        print(f\"[{r.similarity:.3f}] {r.memory.content}\")\n\n    # Full-text search\n    text_results = await memory.text_search(\n        query=\"functional\",\n        user_id=\"alice\",\n    )\n\n    # Query with filters\n    memories = await memory.query(MemoryFilter(\n        user_id=\"alice\",\n        categories=[MemoryCategory.PREFERENCE, MemoryCategory.FACT],\n        min_importance=0.7,\n        limit=10,\n    ))\n\n    # Convenience methods\n    await memory.remember(\"Likes coffee\", user_id=\"alice\", importance=0.6)\n    relevant = await memory.recall(\"beverage preferences\", user_id=\"alice\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"memory/#configuration","title":"Configuration","text":""},{"location":"memory/#embedder-backends","title":"Embedder Backends","text":"<pre><code>from headroom.memory import MemoryConfig, EmbedderBackend\n\n# Local embeddings (recommended - fast, free, private)\nconfig = MemoryConfig(\n    embedder_backend=EmbedderBackend.LOCAL,\n    embedder_model=\"all-MiniLM-L6-v2\",  # 384 dimensions, fast\n)\n\n# OpenAI embeddings (higher quality, costs money)\nconfig = MemoryConfig(\n    embedder_backend=EmbedderBackend.OPENAI,\n    openai_api_key=\"sk-...\",\n    embedder_model=\"text-embedding-3-small\",\n)\n\n# Ollama embeddings (local server, many models)\nconfig = MemoryConfig(\n    embedder_backend=EmbedderBackend.OLLAMA,\n    ollama_base_url=\"http://localhost:11434\",\n    embedder_model=\"nomic-embed-text\",\n)\n</code></pre>"},{"location":"memory/#storage-configuration","title":"Storage Configuration","text":"<pre><code>config = MemoryConfig(\n    db_path=\"memory.db\",          # SQLite database path\n    vector_dimension=384,          # Must match embedder output\n    hnsw_ef_construction=200,      # HNSW index quality (higher = better, slower)\n    hnsw_m=16,                     # HNSW connections per node\n    hnsw_ef_search=50,             # HNSW search quality\n    cache_enabled=True,            # Enable LRU cache\n    cache_max_size=1000,           # Max cached memories\n)\n</code></pre>"},{"location":"memory/#wrapper-configuration","title":"Wrapper Configuration","text":"<pre><code>client = with_memory(\n    OpenAI(),\n    user_id=\"alice\",\n    db_path=\"memory.db\",\n    top_k=5,                       # Memories to inject per request\n    session_id=\"optional-session\",\n    agent_id=\"optional-agent\",\n    embedder_backend=EmbedderBackend.LOCAL,\n)\n</code></pre>"},{"location":"memory/#architecture","title":"Architecture","text":""},{"location":"memory/#protocol-based-design","title":"Protocol-Based Design","text":"<p>Headroom Memory uses Protocol interfaces (ports) for all components, enabling easy swapping:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   HierarchicalMemory                        \u2502\n\u2502                     (Orchestrator)                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502 MemoryStore \u2502  \u2502 VectorIndex \u2502  \u2502  TextIndex  \u2502        \u2502\n\u2502  \u2502  Protocol   \u2502  \u2502  Protocol   \u2502  \u2502  Protocol   \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502         \u2502                \u2502                \u2502                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502   SQLite    \u2502  \u2502    HNSW     \u2502  \u2502    FTS5     \u2502        \u2502\n\u2502  \u2502  Adapter    \u2502  \u2502   Adapter   \u2502  \u2502   Adapter   \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u2502\n\u2502  \u2502  Embedder   \u2502  \u2502 MemoryCache \u2502                          \u2502\n\u2502  \u2502  Protocol   \u2502  \u2502  Protocol   \u2502                          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2502         \u2502                \u2502                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u2502\n\u2502  \u2502Local/OpenAI/\u2502  \u2502  LRU Cache  \u2502                          \u2502\n\u2502  \u2502   Ollama    \u2502  \u2502             \u2502                          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"memory/#components","title":"Components","text":"Component Protocol Default Adapter Purpose MemoryStore <code>MemoryStore</code> <code>SQLiteMemoryStore</code> CRUD + filtering + supersession VectorIndex <code>VectorIndex</code> <code>HNSWVectorIndex</code> Semantic similarity search TextIndex <code>TextIndex</code> <code>FTS5TextIndex</code> Full-text keyword search Embedder <code>Embedder</code> <code>LocalEmbedder</code> Text \u2192 vector conversion Cache <code>MemoryCache</code> <code>LRUMemoryCache</code> Hot memory caching"},{"location":"memory/#comparison-with-state-of-the-art","title":"Comparison with State of the Art","text":""},{"location":"memory/#vs-letta-memgpt","title":"vs Letta (MemGPT)","text":"<p>Letta pioneered inline memory extraction. Headroom builds on this with:</p> Aspect Headroom Letta Scoping 4-level hierarchy (user/session/agent/turn) Flat per-agent Temporal Full supersession chains with history No versioning Integration One-liner wrapper for any client Requires Letta agent framework Search Semantic + full-text Semantic only Storage SQLite + HNSW (embedded) PostgreSQL (external) Extensibility Protocol-based adapters Monolithic <p>When to use Letta: You want a full agent framework with built-in memory. When to use Headroom: You want memory as a layer on your existing stack.</p>"},{"location":"memory/#vs-mem0","title":"vs Mem0","text":"<p>Mem0 provides a managed memory service. Headroom differs:</p> Aspect Headroom Mem0 Deployment Embedded (no server) Managed service or self-hosted Scoping 4-level hierarchy Flat per-user Temporal Supersession chains No versioning Extraction Inline (zero latency) Separate API call Search Semantic + full-text Semantic only Cost Free (local embeddings) API costs or infra costs Privacy All local Data leaves your infra <p>When to use Mem0: You want a managed service and don't mind external dependencies. When to use Headroom: You want embedded memory with no external services.</p>"},{"location":"memory/#feature-matrix","title":"Feature Matrix","text":"Feature Headroom Letta Mem0 Hierarchical scoping \u2705 \u274c \u274c Temporal versioning \u2705 \u274c \u274c Zero-latency extraction \u2705 \u2705 \u274c Full-text search \u2705 \u274c \u274c Embedded (no server) \u2705 \u274c \u274c One-liner integration \u2705 \u274c \u274c Protocol-based extensibility \u2705 \u274c \u274c Memory bubbling \u2705 \u274c \u274c Local embeddings \u2705 \u274c \u2705 Managed service option \u274c \u274c \u2705"},{"location":"memory/#multi-user-isolation","title":"Multi-User Isolation","text":"<p>Memories are isolated by <code>user_id</code>:</p> <pre><code># Alice's memories\nalice_client = with_memory(OpenAI(), user_id=\"alice\")\n\n# Bob's memories (completely separate)\nbob_client = with_memory(OpenAI(), user_id=\"bob\")\n\n# Bob cannot see Alice's memories, even with the same database\n</code></pre>"},{"location":"memory/#performance","title":"Performance","text":"Operation Latency Notes Memory injection &lt;50ms Local embeddings + HNSW search Memory extraction +50-100 tokens Part of LLM response (inline) Memory storage &lt;10ms SQLite + HNSW + FTS5 indexing Cache hit &lt;1ms LRU cache lookup <p>Overhead: ~100 extra output tokens per response for the <code>&lt;memory&gt;</code> block.</p>"},{"location":"memory/#providers","title":"Providers","text":"<p>Memory works with any OpenAI-compatible client:</p> <pre><code>from openai import OpenAI\nfrom headroom import with_memory\n\n# OpenAI\nclient = with_memory(OpenAI(), user_id=\"alice\")\n\n# Azure OpenAI\nclient = with_memory(\n    OpenAI(base_url=\"https://your-resource.openai.azure.com/...\"),\n    user_id=\"alice\",\n)\n\n# Groq\nfrom groq import Groq\nclient = with_memory(Groq(), user_id=\"alice\")\n\n# Any OpenAI-compatible client\nclient = with_memory(YourClient(), user_id=\"alice\")\n</code></pre>"},{"location":"memory/#example-full-conversation-flow","title":"Example: Full Conversation Flow","text":"<pre><code>from openai import OpenAI\nfrom headroom import with_memory\n\nclient = with_memory(OpenAI(), user_id=\"developer_jane\")\n\n# Conversation 1: User shares context\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"I'm a Python developer at a fintech startup. We use PostgreSQL and FastAPI.\"\n    }]\n)\n# Memories extracted:\n#   - [FACT] Python developer at fintech startup\n#   - [PREFERENCE] Uses PostgreSQL for databases\n#   - [PREFERENCE] Uses FastAPI for web APIs\n\n# Conversation 2 (new session): User asks question\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"What database should I use for my new project?\"\n    }]\n)\n# Response references PostgreSQL preference from memory:\n# \u2192 \"Given your experience with PostgreSQL at your fintech company,\n#    I'd recommend sticking with it for consistency...\"\n\n# Check stored memories\nprint(\"Stored memories:\")\nfor m in client.memory.get_all():\n    print(f\"  [{m.category.value}] {m.content}\")\n</code></pre>"},{"location":"memory/#troubleshooting","title":"Troubleshooting","text":""},{"location":"memory/#memories-not-being-extracted","title":"Memories not being extracted","text":"<ol> <li>Check if the conversation has memory-worthy content (not just greetings)</li> <li>Verify the LLM is following the memory instruction</li> <li>Enable logging: <code>import logging; logging.basicConfig(level=logging.DEBUG)</code></li> </ol>"},{"location":"memory/#memories-not-being-retrieved","title":"Memories not being retrieved","text":"<ol> <li>Verify <code>user_id</code> matches between sessions</li> <li>Check if memories exist: <code>client.memory.get_all()</code></li> <li>Try a more specific search query</li> <li>Check similarity threshold</li> </ol>"},{"location":"memory/#high-latency","title":"High latency","text":"<ol> <li>Use local embeddings: <code>embedder_backend=EmbedderBackend.LOCAL</code></li> <li>Reduce <code>top_k</code> for fewer memories to retrieve</li> <li>Enable caching (enabled by default)</li> </ol>"},{"location":"memory/#memory-not-persisting","title":"Memory not persisting","text":"<ol> <li>Check <code>db_path</code> is the same across sessions</li> <li>Ensure the database file is writable</li> <li>Check for exceptions in logs</li> </ol>"},{"location":"memory/#best-practices","title":"Best Practices","text":"<ol> <li>Use consistent <code>user_id</code> - Same ID across sessions for continuity</li> <li>Use session scoping - Set <code>session_id</code> for session-specific context</li> <li>Start with local embeddings - Faster, free, good enough for most cases</li> <li>Monitor memory growth - Use <code>client.memory.stats()</code> to track</li> <li>Use importance scores - Higher importance = more likely to be retrieved</li> <li>Leverage categories - Helps with debugging and selective retrieval</li> <li>Consider supersession - Use <code>supersede()</code> when facts change, not <code>add()</code></li> </ol>"},{"location":"metrics/","title":"Metrics &amp; Monitoring","text":"<p>Headroom provides comprehensive metrics for monitoring compression performance, cost savings, and system health.</p>"},{"location":"metrics/#proxy-metrics","title":"Proxy Metrics","text":""},{"location":"metrics/#stats-endpoint","title":"Stats Endpoint","text":"<pre><code>curl http://localhost:8787/stats\n</code></pre> <pre><code>{\n  \"requests\": {\n    \"total\": 42,\n    \"cached\": 5,\n    \"rate_limited\": 0,\n    \"failed\": 0\n  },\n  \"tokens\": {\n    \"input\": 50000,\n    \"output\": 8000,\n    \"saved\": 12500,\n    \"savings_percent\": 25.0\n  },\n  \"cost\": {\n    \"total_cost_usd\": 0.15,\n    \"total_savings_usd\": 0.04\n  },\n  \"cache\": {\n    \"entries\": 10,\n    \"total_hits\": 5\n  }\n}\n</code></pre>"},{"location":"metrics/#prometheus-metrics","title":"Prometheus Metrics","text":"<pre><code>curl http://localhost:8787/metrics\n</code></pre> <pre><code># HELP headroom_requests_total Total requests processed\nheadroom_requests_total{mode=\"optimize\"} 1234\n\n# HELP headroom_tokens_saved_total Total tokens saved\nheadroom_tokens_saved_total 5678900\n\n# HELP headroom_compression_ratio Compression ratio histogram\nheadroom_compression_ratio_bucket{le=\"0.5\"} 890\nheadroom_compression_ratio_bucket{le=\"0.7\"} 1100\nheadroom_compression_ratio_bucket{le=\"0.9\"} 1200\n\n# HELP headroom_latency_seconds Request latency histogram\nheadroom_latency_seconds_bucket{le=\"0.01\"} 800\nheadroom_latency_seconds_bucket{le=\"0.1\"} 1150\n\n# HELP headroom_cache_hits_total Cache hit counter\nheadroom_cache_hits_total 456\n\n# HELP headroom_cache_misses_total Cache miss counter\nheadroom_cache_misses_total 778\n</code></pre>"},{"location":"metrics/#health-check","title":"Health Check","text":"<pre><code>curl http://localhost:8787/health\n</code></pre> <pre><code>{\n  \"status\": \"healthy\",\n  \"version\": \"0.1.0\",\n  \"uptime_seconds\": 3600,\n  \"llmlingua_enabled\": false\n}\n</code></pre>"},{"location":"metrics/#sdk-metrics","title":"SDK Metrics","text":""},{"location":"metrics/#session-stats","title":"Session Stats","text":"<p>Quick stats for the current session (no database query):</p> <pre><code>stats = client.get_stats()\nprint(stats)\n</code></pre> <pre><code>{\n    \"session\": {\n        \"requests_total\": 10,\n        \"tokens_input_before\": 50000,\n        \"tokens_input_after\": 35000,\n        \"tokens_saved_total\": 15000,\n        \"tokens_output_total\": 8000,\n        \"cache_hits\": 3,\n        \"compression_ratio_avg\": 0.70\n    },\n    \"config\": {\n        \"mode\": \"optimize\",\n        \"provider\": \"openai\",\n        \"cache_optimizer_enabled\": True,\n        \"semantic_cache_enabled\": False\n    },\n    \"transforms\": {\n        \"smart_crusher_enabled\": True,\n        \"cache_aligner_enabled\": True,\n        \"rolling_window_enabled\": True\n    }\n}\n</code></pre>"},{"location":"metrics/#historical-metrics","title":"Historical Metrics","text":"<p>Query stored metrics from the database:</p> <pre><code>from datetime import datetime, timedelta\n\n# Get recent metrics\nmetrics = client.get_metrics(\n    start_time=datetime.utcnow() - timedelta(hours=1),\n    limit=100,\n)\n\nfor m in metrics:\n    print(f\"{m.timestamp}: {m.tokens_input_before} -&gt; {m.tokens_input_after}\")\n</code></pre>"},{"location":"metrics/#summary-statistics","title":"Summary Statistics","text":"<p>Aggregate statistics across all stored metrics:</p> <pre><code>summary = client.get_summary()\nprint(f\"Total requests: {summary['total_requests']}\")\nprint(f\"Total tokens saved: {summary['total_tokens_saved']}\")\nprint(f\"Average compression: {summary['avg_compression_ratio']:.1%}\")\nprint(f\"Total cost savings: ${summary['total_cost_saved_usd']:.2f}\")\n</code></pre>"},{"location":"metrics/#logging","title":"Logging","text":""},{"location":"metrics/#enable-logging","title":"Enable Logging","text":"<pre><code>import logging\n\n# INFO level shows compression summaries\nlogging.basicConfig(level=logging.INFO)\n\n# DEBUG level shows detailed transform decisions\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"metrics/#log-output-examples","title":"Log Output Examples","text":"<pre><code>INFO:headroom.transforms.pipeline:Pipeline complete: 45000 -&gt; 4500 tokens (saved 40500, 90.0% reduction)\nINFO:headroom.transforms.smart_crusher:SmartCrusher applied top_n strategy: kept 15 of 1000 items\nINFO:headroom.cache.compression_store:CCR cache hit: hash=abc123, retrieved 1000 items\nDEBUG:headroom.transforms.smart_crusher:Kept items: [0,1,2,42,77,97,98,99] (errors at 42, warnings at 77)\n</code></pre>"},{"location":"metrics/#proxy-logging","title":"Proxy Logging","text":"<pre><code># Log to file\nheadroom proxy --log-file headroom.jsonl\n\n# Increase verbosity\nheadroom proxy --log-level debug\n</code></pre>"},{"location":"metrics/#grafana-dashboard","title":"Grafana Dashboard","text":"<p>Example Grafana dashboard configuration for Prometheus metrics:</p> <pre><code>{\n  \"panels\": [\n    {\n      \"title\": \"Tokens Saved\",\n      \"type\": \"stat\",\n      \"targets\": [{\"expr\": \"headroom_tokens_saved_total\"}]\n    },\n    {\n      \"title\": \"Compression Ratio\",\n      \"type\": \"gauge\",\n      \"targets\": [{\"expr\": \"histogram_quantile(0.5, headroom_compression_ratio_bucket)\"}]\n    },\n    {\n      \"title\": \"Request Latency (p99)\",\n      \"type\": \"graph\",\n      \"targets\": [{\"expr\": \"histogram_quantile(0.99, headroom_latency_seconds_bucket)\"}]\n    },\n    {\n      \"title\": \"Cache Hit Rate\",\n      \"type\": \"gauge\",\n      \"targets\": [{\"expr\": \"headroom_cache_hits_total / (headroom_cache_hits_total + headroom_cache_misses_total)\"}]\n    }\n  ]\n}\n</code></pre>"},{"location":"metrics/#cost-tracking","title":"Cost Tracking","text":""},{"location":"metrics/#per-request-cost","title":"Per-Request Cost","text":"<p>Each request includes cost metadata in the response:</p> <pre><code>response = client.chat.completions.create(...)\n\n# Access via response metadata (if available)\n# Cost is calculated based on model pricing and token counts\n</code></pre>"},{"location":"metrics/#budget-alerts","title":"Budget Alerts","text":"<p>Set a budget limit in the proxy:</p> <pre><code>headroom proxy --budget 10.00\n</code></pre> <p>When the budget is exceeded: - Requests return a budget exceeded error - The <code>/stats</code> endpoint shows budget status - Logs indicate budget state</p>"},{"location":"metrics/#validation","title":"Validation","text":"<p>Validate your setup is correct:</p> <pre><code>result = client.validate_setup()\n\nif result[\"valid\"]:\n    print(\"Setup is correct!\")\nelse:\n    print(\"Issues found:\")\n    for issue in result[\"issues\"]:\n        print(f\"  - {issue}\")\n</code></pre>"},{"location":"metrics/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"Metric What It Tells You Target <code>tokens_saved_total</code> Total cost savings Higher is better <code>compression_ratio_avg</code> Efficiency 0.7-0.9 typical <code>cache_hit_rate</code> Cache effectiveness &gt;20% is good <code>latency_p99</code> Performance impact &lt;10ms <code>failed_requests</code> Reliability 0"},{"location":"proxy/","title":"Proxy Server Documentation","text":"<p>The Headroom proxy server is a production-ready HTTP server that applies context optimization to all requests passing through it.</p>"},{"location":"proxy/#starting-the-proxy","title":"Starting the Proxy","text":"<pre><code># Basic usage\nheadroom proxy\n\n# Custom port\nheadroom proxy --port 8080\n\n# With all options\nheadroom proxy \\\n  --host 0.0.0.0 \\\n  --port 8787 \\\n  --log-file /var/log/headroom.jsonl \\\n  --budget 100.0\n</code></pre>"},{"location":"proxy/#command-line-options","title":"Command Line Options","text":""},{"location":"proxy/#core-options","title":"Core Options","text":"Option Default Description <code>--host</code> <code>127.0.0.1</code> Host to bind to <code>--port</code> <code>8787</code> Port to bind to <code>--no-optimize</code> <code>false</code> Disable optimization (passthrough mode) <code>--no-cache</code> <code>false</code> Disable semantic caching <code>--no-rate-limit</code> <code>false</code> Disable rate limiting <code>--log-file</code> None Path to JSONL log file <code>--budget</code> None Daily budget limit in USD <code>--openai-api-url</code> <code>https://api.openai.com</code> Custom OpenAI API URL endpoint"},{"location":"proxy/#context-management-options","title":"Context Management Options","text":"Option Default Description <code>--no-intelligent-context</code> <code>false</code> Disable IntelligentContextManager (fall back to RollingWindow) <code>--no-intelligent-scoring</code> <code>false</code> Disable multi-factor importance scoring (use position-based) <code>--no-compress-first</code> <code>false</code> Disable trying deeper compression before dropping messages <p>By default, the proxy uses IntelligentContextManager which scores messages by multiple factors (recency, semantic similarity, TOIN-learned patterns, error indicators, forward references) and drops lowest-scored messages first. This is smarter than simple age-based truncation.</p> <p>CCR Integration: When messages are dropped, they're stored in CCR so the LLM can retrieve them if needed. The inserted marker includes the CCR reference. Drops are also recorded to TOIN, so the system learns which message patterns are important across all users.</p> <pre><code># Use legacy RollingWindow (drops oldest first)\nheadroom proxy --no-intelligent-context\n\n# Disable semantic scoring (faster, but less intelligent)\nheadroom proxy --no-intelligent-scoring\n</code></pre>"},{"location":"proxy/#llmlingua-options-ml-compression","title":"LLMLingua Options (ML Compression)","text":"Option Default Description <code>--llmlingua</code> <code>false</code> Enable LLMLingua-2 ML-based compression <code>--llmlingua-device</code> <code>auto</code> Device for model: <code>auto</code>, <code>cuda</code>, <code>cpu</code>, <code>mps</code> <code>--llmlingua-rate</code> <code>0.3</code> Target compression rate (0.3 = keep 30% of tokens) <p>Note: LLMLingua requires additional dependencies: <code>pip install headroom-ai[llmlingua]</code></p> <pre><code># Enable LLMLingua with GPU acceleration\nheadroom proxy --llmlingua --llmlingua-device cuda\n\n# More aggressive compression (keep only 20%)\nheadroom proxy --llmlingua --llmlingua-rate 0.2\n\n# Conservative compression for code (keep 50%)\nheadroom proxy --llmlingua --llmlingua-rate 0.5\n</code></pre>"},{"location":"proxy/#api-endpoints","title":"API Endpoints","text":""},{"location":"proxy/#health-check","title":"Health Check","text":"<pre><code>curl http://localhost:8787/health\n</code></pre> <p>Response: <pre><code>{\n  \"status\": \"healthy\",\n  \"optimize\": true,\n  \"stats\": {\n    \"total_requests\": 42,\n    \"tokens_saved\": 15000,\n    \"savings_percent\": 45.2\n  }\n}\n</code></pre></p>"},{"location":"proxy/#detailed-statistics","title":"Detailed Statistics","text":"<pre><code>curl http://localhost:8787/stats\n</code></pre>"},{"location":"proxy/#prometheus-metrics","title":"Prometheus Metrics","text":"<pre><code>curl http://localhost:8787/metrics\n</code></pre>"},{"location":"proxy/#llm-apis","title":"LLM APIs","text":"<p>The proxy supports both Anthropic and OpenAI API formats:</p> <pre><code># Anthropic format\nPOST /v1/messages\n\n# OpenAI format\nPOST /v1/chat/completions\n</code></pre>"},{"location":"proxy/#using-with-claude-code","title":"Using with Claude Code","text":"<pre><code># Start proxy\nheadroom proxy --port 8787\n\n# In another terminal\nANTHROPIC_BASE_URL=http://localhost:8787 claude\n</code></pre>"},{"location":"proxy/#using-with-cursor","title":"Using with Cursor","text":"<ol> <li>Start the proxy: <code>headroom proxy</code></li> <li>In Cursor settings, set the base URL to <code>http://localhost:8787</code></li> </ol>"},{"location":"proxy/#using-with-openai-sdk","title":"Using with OpenAI SDK","text":"<pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:8787/v1\",\n    api_key=\"your-api-key\",  # Still needed for upstream\n)\n</code></pre>"},{"location":"proxy/#features","title":"Features","text":""},{"location":"proxy/#llmlingua-ml-compression-opt-in","title":"LLMLingua ML Compression (Opt-In)","text":"<p>When enabled, the proxy uses Microsoft's LLMLingua-2 model for ML-based token compression:</p> <pre><code>headroom proxy --llmlingua\n</code></pre> <p>How it works: - LLMLinguaCompressor is added to the transform pipeline (before RollingWindow) - Automatically detects content type (JSON, code, text) and adjusts compression - Stores original content in CCR for retrieval if needed</p> <p>Startup feedback:</p> <pre><code># When enabled and available:\nLLMLingua: ENABLED  (device=cuda, rate=0.3)\n\n# When installed but not enabled (helpful hint):\nLLMLingua: available (enable with --llmlingua for ML compression)\n\n# When enabled but not installed:\nWARNING: LLMLingua requested but not installed. Install with: pip install headroom-ai[llmlingua]\n</code></pre> <p>Why opt-in? | Concern | Default Proxy | With LLMLingua | |---------|---------------|----------------| | Dependencies | ~50MB | +2GB (torch, transformers) | | Cold start | &lt;1s | 10-30s (model load) | | Memory | ~100MB | +1GB (model in RAM) | | Overhead | &lt;5ms | 50-200ms per request |</p> <p>Enable LLMLingua when maximum compression justifies the resource cost.</p>"},{"location":"proxy/#semantic-caching","title":"Semantic Caching","text":"<p>The proxy caches responses for repeated queries:</p> <ul> <li>LRU eviction with configurable max entries</li> <li>TTL-based expiration</li> <li>Cache key based on message content hash</li> </ul>"},{"location":"proxy/#rate-limiting","title":"Rate Limiting","text":"<p>Token bucket rate limiting protects against runaway costs:</p> <ul> <li>Configurable requests per minute</li> <li>Configurable tokens per minute</li> <li>Per-API-key tracking</li> </ul>"},{"location":"proxy/#cost-tracking","title":"Cost Tracking","text":"<p>Track spending and enforce budgets:</p> <ul> <li>Real-time cost estimation</li> <li>Budget periods: hourly, daily, monthly</li> <li>Automatic request rejection when over budget</li> </ul>"},{"location":"proxy/#prometheus-metrics_1","title":"Prometheus Metrics","text":"<p>Export metrics for monitoring:</p> <pre><code>headroom_requests_total\nheadroom_tokens_saved_total\nheadroom_cost_usd_total\nheadroom_latency_ms_sum\n</code></pre>"},{"location":"proxy/#configuration-via-environment","title":"Configuration via Environment","text":"<pre><code>export HEADROOM_HOST=0.0.0.0\nexport HEADROOM_PORT=8787\nexport HEADROOM_BUDGET=100.0\nexport OPENAI_TARGET_API_URL=https://custom.openai.endpoint.com\nheadroom proxy\n</code></pre>"},{"location":"proxy/#running-in-production","title":"Running in Production","text":"<p>For production deployments:</p> <pre><code># Use a process manager\npip install gunicorn\n\n# Run with gunicorn\ngunicorn headroom.proxy.server:app \\\n  --workers 4 \\\n  --bind 0.0.0.0:8787 \\\n  --worker-class uvicorn.workers.UvicornWorker\n</code></pre> <p>Or with Docker:</p> <pre><code>FROM python:3.11-slim\nRUN pip install headroom[proxy]\nEXPOSE 8787\nCMD [\"headroom\", \"proxy\", \"--host\", \"0.0.0.0\"]\n</code></pre>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>Get Headroom running in 5 minutes with these copy-paste examples.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code># Core only (minimal dependencies)\npip install headroom\n\n# With proxy server\npip install \"headroom[proxy]\"\n\n# Everything\npip install \"headroom[all]\"\n</code></pre>"},{"location":"quickstart/#option-1-proxy-server-zero-code-changes","title":"Option 1: Proxy Server (Zero Code Changes)","text":"<p>The fastest way to start saving tokens. Works with any OpenAI-compatible client.</p>"},{"location":"quickstart/#step-1-start-the-proxy","title":"Step 1: Start the Proxy","text":"<pre><code>headroom proxy --port 8787\n</code></pre>"},{"location":"quickstart/#step-2-verify-its-running","title":"Step 2: Verify It's Running","text":"<pre><code>curl http://localhost:8787/health\n# Expected: {\"status\": \"healthy\", \"mode\": \"optimize\", ...}\n</code></pre>"},{"location":"quickstart/#step-3-point-your-client","title":"Step 3: Point Your Client","text":"<pre><code># Claude Code\nANTHROPIC_BASE_URL=http://localhost:8787 claude\n\n# Cursor / Continue / any OpenAI client\nOPENAI_BASE_URL=http://localhost:8787/v1 your-app\n\n# Python\nexport OPENAI_BASE_URL=http://localhost:8787/v1\npython your_script.py\n</code></pre>"},{"location":"quickstart/#step-4-check-savings","title":"Step 4: Check Savings","text":"<pre><code>curl http://localhost:8787/stats\n# {\"requests_total\": 42, \"tokens_saved_total\": 125000, ...}\n</code></pre>"},{"location":"quickstart/#option-2-python-sdk","title":"Option 2: Python SDK","text":"<p>Wrap your existing client for fine-grained control.</p>"},{"location":"quickstart/#basic-example","title":"Basic Example","text":"<pre><code>from headroom import HeadroomClient, OpenAIProvider\nfrom openai import OpenAI\n\n# Create wrapped client\nclient = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n    default_mode=\"optimize\",\n)\n\n# Use exactly like OpenAI client\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"},\n    ],\n)\n\nprint(response.choices[0].message.content)\n\n# Check what happened\nstats = client.get_stats()\nprint(f\"Tokens saved: {stats['session']['tokens_saved_total']}\")\n</code></pre>"},{"location":"quickstart/#with-tool-outputs-where-savings-happen","title":"With Tool Outputs (Where Savings Happen)","text":"<pre><code>from headroom import HeadroomClient, OpenAIProvider\nfrom openai import OpenAI\nimport json\n\nclient = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n    default_mode=\"optimize\",\n)\n\n# Simulate a conversation with large tool outputs\nmessages = [\n    {\"role\": \"system\", \"content\": \"You analyze search results.\"},\n    {\"role\": \"user\", \"content\": \"Search for Python tutorials.\"},\n    {\n        \"role\": \"assistant\",\n        \"content\": None,\n        \"tool_calls\": [{\n            \"id\": \"call_1\",\n            \"type\": \"function\",\n            \"function\": {\"name\": \"search\", \"arguments\": '{\"q\": \"python\"}'},\n        }],\n    },\n    {\n        \"role\": \"tool\",\n        \"tool_call_id\": \"call_1\",\n        # This is where Headroom shines - compressing large outputs\n        \"content\": json.dumps({\n            \"results\": [{\"title\": f\"Result {i}\", \"score\": 100-i} for i in range(500)]\n        }),\n    },\n    {\"role\": \"user\", \"content\": \"What are the top 3 results?\"},\n]\n\n# Headroom compresses the 500 results to ~20, keeping the most relevant\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages,\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"quickstart/#simulate-before-sending","title":"Simulate Before Sending","text":"<p>Preview optimizations without making an API call:</p> <pre><code># See what would happen without calling the API\nplan = client.chat.completions.simulate(\n    model=\"gpt-4o\",\n    messages=messages,\n)\n\nprint(f\"Tokens before: {plan.tokens_before}\")\nprint(f\"Tokens after: {plan.tokens_after}\")\nprint(f\"Would save: {plan.tokens_saved} tokens ({plan.tokens_saved/plan.tokens_before*100:.0f}%)\")\nprint(f\"Transforms: {plan.transforms}\")\nprint(f\"Estimated savings: {plan.estimated_savings}\")\n</code></pre>"},{"location":"quickstart/#option-3-anthropic-sdk","title":"Option 3: Anthropic SDK","text":"<pre><code>from headroom import HeadroomClient, AnthropicProvider\nfrom anthropic import Anthropic\n\nclient = HeadroomClient(\n    original_client=Anthropic(),\n    provider=AnthropicProvider(),\n    default_mode=\"optimize\",\n)\n\n# Use Anthropic-style API\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"},\n    ],\n)\n\nprint(response.content[0].text)\n</code></pre>"},{"location":"quickstart/#verify-its-working","title":"Verify It's Working","text":""},{"location":"quickstart/#method-1-enable-logging","title":"Method 1: Enable Logging","text":"<pre><code>import logging\nlogging.basicConfig(level=logging.INFO)\n\n# Now you'll see:\n# INFO:headroom.transforms.pipeline:Pipeline complete: 45000 -&gt; 4500 tokens (saved 40500, 90.0% reduction)\n# INFO:headroom.transforms.smart_crusher:SmartCrusher: keeping 15 of 500 items\n</code></pre>"},{"location":"quickstart/#method-2-check-session-stats","title":"Method 2: Check Session Stats","text":"<pre><code>stats = client.get_stats()\nprint(stats)\n# {\n#   \"session\": {\"requests_total\": 10, \"tokens_saved_total\": 5000, ...},\n#   \"config\": {\"mode\": \"optimize\", \"provider\": \"openai\", ...},\n#   \"transforms\": {\"smart_crusher_enabled\": True, ...}\n# }\n</code></pre>"},{"location":"quickstart/#method-3-validate-setup","title":"Method 3: Validate Setup","text":"<pre><code>result = client.validate_setup()\nif not result[\"valid\"]:\n    print(\"Setup issues:\", result)\nelse:\n    print(\"Setup OK!\")\n    print(f\"Provider: {result['provider']['name']}\")\n    print(f\"Storage: {result['storage']['url']}\")\n</code></pre>"},{"location":"quickstart/#common-configuration","title":"Common Configuration","text":""},{"location":"quickstart/#adjust-compression","title":"Adjust Compression","text":"<pre><code>from headroom import HeadroomClient, OpenAIProvider, HeadroomConfig\n\nconfig = HeadroomConfig()\n\n# Keep more items after compression (default: 15)\nconfig.smart_crusher.max_items_after_crush = 30\n\n# Only compress if tool output has &gt; 500 tokens (default: 200)\nconfig.smart_crusher.min_tokens_to_crush = 500\n\nclient = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n    config=config,  # Pass custom config\n    default_mode=\"optimize\",\n)\n</code></pre>"},{"location":"quickstart/#skip-compression-for-specific-tools","title":"Skip Compression for Specific Tools","text":"<pre><code>response = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages,\n    headroom_tool_profiles={\n        \"database_query\": {\"skip_compression\": True},  # Never compress\n        \"search\": {\"max_items\": 50},  # Keep more items\n    },\n)\n</code></pre>"},{"location":"quickstart/#audit-mode-observe-only","title":"Audit Mode (Observe Only)","text":"<pre><code># Start in audit mode - see what WOULD be optimized\nclient = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n    default_mode=\"audit\",  # No modifications, just logging\n)\n\n# Override per-request\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages,\n    headroom_mode=\"optimize\",  # Enable for this request only\n)\n</code></pre>"},{"location":"quickstart/#what-gets-optimized","title":"What Gets Optimized?","text":"Content Type What Headroom Does Typical Savings Tool outputs with lists Keeps errors, anomalies, high-score items 70-90% Repeated search results Deduplicates and samples 60-80% Long conversations Drops old turns, keeps recent 40-60% System prompts with dates Stabilizes for cache hits Cache savings"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Reference - All configuration options</li> <li>Transform Reference - How each transform works</li> <li>Troubleshooting - Common issues and solutions</li> <li>Examples - More complete examples</li> </ul>"},{"location":"quickstart/#quick-troubleshooting","title":"Quick Troubleshooting","text":""},{"location":"quickstart/#no-token-savings","title":"\"No token savings\"","text":"<pre><code># 1. Check mode\nstats = client.get_stats()\nprint(stats[\"config\"][\"mode\"])  # Should be \"optimize\"\n\n# 2. Enable logging to see what's happening\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"quickstart/#high-latency","title":"\"High latency\"","text":"<pre><code># Use BM25 instead of embeddings for faster relevance scoring\nconfig.smart_crusher.relevance.tier = \"bm25\"\n</code></pre>"},{"location":"quickstart/#compression-too-aggressive","title":"\"Compression too aggressive\"","text":"<pre><code># Keep more items\nconfig.smart_crusher.max_items_after_crush = 50\n</code></pre> <p>See Troubleshooting Guide for more solutions.</p>"},{"location":"sdk/","title":"SDK Guide","text":"<p>The Headroom SDK wraps your existing LLM client to add compression and optimization transparently.</p>"},{"location":"sdk/#installation","title":"Installation","text":"<pre><code>pip install headroom-ai openai\n</code></pre>"},{"location":"sdk/#quick-start","title":"Quick Start","text":"<pre><code>from headroom import HeadroomClient, OpenAIProvider\nfrom openai import OpenAI\n\n# Create wrapped client\nclient = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n    default_mode=\"optimize\",\n)\n\n# Use exactly like the original client\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"},\n    ],\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"sdk/#tool-output-compression","title":"Tool Output Compression","text":"<p>Real savings happen with tool outputs. Here's where Headroom shines:</p> <pre><code>import json\n\n# Conversation with large tool output\nmessages = [\n    {\"role\": \"user\", \"content\": \"Search for Python tutorials\"},\n    {\n        \"role\": \"assistant\",\n        \"content\": None,\n        \"tool_calls\": [{\n            \"id\": \"call_123\",\n            \"type\": \"function\",\n            \"function\": {\"name\": \"search\", \"arguments\": '{\"q\": \"python\"}'},\n        }],\n    },\n    {\n        \"role\": \"tool\",\n        \"tool_call_id\": \"call_123\",\n        \"content\": json.dumps({\n            \"results\": [\n                {\"title\": f\"Tutorial {i}\", \"score\": 100-i}\n                for i in range(500)\n            ]\n        }),\n    },\n    {\"role\": \"user\", \"content\": \"What are the top 3?\"},\n]\n\n# Headroom compresses 500 results to ~15, keeping highest-scoring items\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=messages\n)\n\n# Check savings\nstats = client.get_stats()\nprint(f\"Tokens saved: {stats['session']['tokens_saved_total']}\")\n# Typical output: \"Tokens saved: 3500\"\n</code></pre>"},{"location":"sdk/#supported-providers","title":"Supported Providers","text":""},{"location":"sdk/#openai","title":"OpenAI","text":"<pre><code>from headroom import HeadroomClient, OpenAIProvider\nfrom openai import OpenAI\n\nclient = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n)\n</code></pre>"},{"location":"sdk/#anthropic","title":"Anthropic","text":"<pre><code>from headroom import HeadroomClient, AnthropicProvider\nfrom anthropic import Anthropic\n\nclient = HeadroomClient(\n    original_client=Anthropic(),\n    provider=AnthropicProvider(),\n)\n\nresponse = client.messages.create(\n    model=\"claude-3-5-sonnet-20241022\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n)\n</code></pre>"},{"location":"sdk/#google","title":"Google","text":"<pre><code>from headroom import HeadroomClient, GoogleProvider\nimport google.generativeai as genai\n\nclient = HeadroomClient(\n    original_client=genai,\n    provider=GoogleProvider(),\n)\n</code></pre>"},{"location":"sdk/#check-stats","title":"Check Stats","text":"<pre><code># Session stats (no database query)\nstats = client.get_stats()\nprint(stats)\n# {\n#   \"session\": {\"requests_total\": 10, \"tokens_saved_total\": 5000, ...},\n#   \"config\": {\"mode\": \"optimize\", \"provider\": \"openai\", ...},\n#   \"transforms\": {\"smart_crusher_enabled\": True, ...}\n# }\n</code></pre>"},{"location":"sdk/#validate-setup","title":"Validate Setup","text":"<pre><code>result = client.validate_setup()\nif not result[\"valid\"]:\n    print(\"Setup issues:\", result[\"issues\"])\n</code></pre>"},{"location":"sdk/#modes","title":"Modes","text":""},{"location":"sdk/#optimize-default","title":"Optimize (Default)","text":"<p>Applies all safe transforms:</p> <pre><code>client = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n    default_mode=\"optimize\",\n)\n</code></pre>"},{"location":"sdk/#audit","title":"Audit","text":"<p>Observes and logs without modifying:</p> <pre><code>client = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n    default_mode=\"audit\",\n)\n</code></pre>"},{"location":"sdk/#simulate","title":"Simulate","text":"<p>Returns a plan without making the API call:</p> <pre><code>plan = client.chat.completions.simulate(\n    model=\"gpt-4o\",\n    messages=large_conversation,\n)\n\nprint(f\"Would save {plan.tokens_saved} tokens\")\nprint(f\"Transforms: {plan.transforms}\")\n</code></pre>"},{"location":"sdk/#per-request-overrides","title":"Per-Request Overrides","text":"<pre><code>response = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[...],\n\n    # Override mode for this request\n    headroom_mode=\"audit\",\n\n    # Reserve more tokens for output\n    headroom_output_buffer_tokens=8000,\n\n    # Keep last N turns\n    headroom_keep_turns=5,\n)\n</code></pre>"},{"location":"sdk/#enable-logging","title":"Enable Logging","text":"<pre><code>import logging\nlogging.basicConfig(level=logging.INFO)\n\n# Now you'll see:\n# INFO:headroom.transforms.pipeline:Pipeline complete: 45000 -&gt; 4500 tokens\n# INFO:headroom.transforms.smart_crusher:SmartCrusher: kept 15 of 1000 items\n</code></pre>"},{"location":"sdk/#streaming","title":"Streaming","text":"<p>Streaming works transparently:</p> <pre><code>stream = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    stream=True,\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n</code></pre>"},{"location":"sdk/#error-handling","title":"Error Handling","text":"<pre><code>from headroom import (\n    HeadroomClient,\n    HeadroomError,\n    ConfigurationError,\n    ProviderError,\n)\n\ntry:\n    response = client.chat.completions.create(...)\nexcept ConfigurationError as e:\n    print(f\"Config issue: {e}\")\nexcept ProviderError as e:\n    print(f\"Provider issue: {e}\")\nexcept HeadroomError as e:\n    print(f\"Headroom error: {e}\")\n</code></pre>"},{"location":"sdk/#historical-metrics","title":"Historical Metrics","text":"<p>Query stored metrics:</p> <pre><code>from datetime import datetime, timedelta\n\nmetrics = client.get_metrics(\n    start_time=datetime.utcnow() - timedelta(hours=1),\n    limit=100,\n)\n\nfor m in metrics:\n    print(f\"{m.timestamp}: {m.tokens_input_before} -&gt; {m.tokens_input_after}\")\n</code></pre>"},{"location":"sdk/#advanced-configuration","title":"Advanced Configuration","text":"<p>See Configuration for full options:</p> <pre><code>client = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n    default_mode=\"optimize\",\n    enable_cache_optimizer=True,\n    enable_semantic_cache=False,\n    model_context_limits={\n        \"gpt-4o\": 128000,\n        \"gpt-4o-mini\": 128000,\n    },\n)\n</code></pre>"},{"location":"sdk/#comparison-with-proxy","title":"Comparison with Proxy","text":"Aspect SDK Proxy Setup Wrap client Point URL Control Fine-grained Global Metrics In-process Centralized Best for Custom apps Existing tools <p>Use the SDK when you need fine-grained control. Use the proxy for existing tools like Claude Code, Cursor, etc.</p>"},{"location":"text-compression/","title":"Text Compression Utilities","text":"<p>For coding tasks, Headroom provides standalone text compression utilities that applications can use explicitly. These are opt-in \u2014 they're not applied automatically, giving you full control over when and how to compress text content.</p> <p>Design Philosophy: SmartCrusher compresses JSON automatically because it's structure-preserving and safe. Text compression is lossy and context-dependent, so applications should decide when to use it.</p>"},{"location":"text-compression/#available-utilities","title":"Available Utilities","text":"Utility Input Type Use Case <code>SearchCompressor</code> grep/ripgrep output Search results with <code>file:line:content</code> format <code>LogCompressor</code> Build/test logs pytest, npm, cargo, make output <code>TextCompressor</code> Generic text Any plain text with anchor preservation <code>detect_content_type</code> Any content Detect content type for routing decisions"},{"location":"text-compression/#searchcompressor","title":"SearchCompressor","text":"<p>Compresses search results (grep, ripgrep, ag) while preserving relevant matches.</p> <pre><code>from headroom.transforms import SearchCompressor\n\n# Your grep/ripgrep output (could be 1000s of lines)\nsearch_results = \"\"\"\nsrc/utils.py:42:def process_data(items):\nsrc/utils.py:43:    \\\"\\\"\\\"Process items.\\\"\\\"\\\"\nsrc/models.py:15:class DataProcessor:\nsrc/models.py:89:    def process(self, items):\n... hundreds more matches ...\n\"\"\"\n\n# Explicitly compress when you decide it's appropriate\ncompressor = SearchCompressor()\nresult = compressor.compress(search_results, context=\"find process\")\n\nprint(f\"Compressed {result.original_match_count} matches to {result.compressed_match_count}\")\nprint(result.compressed)\n</code></pre>"},{"location":"text-compression/#what-gets-preserved","title":"What Gets Preserved","text":"<ul> <li>Exact query matches: Lines containing the search term</li> <li>High-relevance matches: Scored by BM25 similarity to context</li> <li>File diversity: Ensures results from different files are kept</li> <li>First/last matches: Context from start and end of results</li> </ul>"},{"location":"text-compression/#logcompressor","title":"LogCompressor","text":"<p>Compresses build and test output while preserving errors, warnings, and summaries.</p> <pre><code>from headroom.transforms import LogCompressor\n\n# pytest output with 1000s of lines\nbuild_output = \"\"\"\n===== test session starts =====\ncollected 500 items\ntests/test_foo.py::test_1 PASSED\n... hundreds of passed tests ...\ntests/test_bar.py::test_fail FAILED\nAssertionError: expected 5, got 3\n===== 1 failed, 499 passed =====\n\"\"\"\n\n# Compress logs, preserving errors and stack traces\ncompressor = LogCompressor()\nresult = compressor.compress(build_output)\n\n# Errors, stack traces, and summary are preserved\nprint(result.compressed)\nprint(f\"Compression ratio: {result.compression_ratio:.1%}\")\n</code></pre>"},{"location":"text-compression/#what-gets-preserved_1","title":"What Gets Preserved","text":"<ul> <li>Errors and failures: Any line with ERROR, FAILED, Exception, etc.</li> <li>Warnings: Warning messages that might be important</li> <li>Stack traces: Full tracebacks for debugging</li> <li>Summaries: Test/build summary lines</li> <li>Section headers: Structural markers like <code>=====</code></li> </ul>"},{"location":"text-compression/#textcompressor","title":"TextCompressor","text":"<p>General-purpose text compression with anchor preservation.</p> <pre><code>from headroom.transforms import TextCompressor\n\nlong_text = \"\"\"\n... thousands of lines of documentation ...\n\"\"\"\n\ncompressor = TextCompressor()\nresult = compressor.compress(long_text, context=\"authentication\")\n\nprint(result.compressed)\n</code></pre>"},{"location":"text-compression/#what-gets-preserved_2","title":"What Gets Preserved","text":"<ul> <li>Relevant paragraphs: Scored by similarity to context</li> <li>Anchors: Headers, section markers, important keywords</li> <li>Structure: Document organization is maintained</li> </ul>"},{"location":"text-compression/#content-type-detection","title":"Content Type Detection","text":"<p>Automatically detect content type to route to the right compressor.</p> <pre><code>from headroom.transforms import detect_content_type, ContentType\n\ncontent = \"src/main.py:42:def process():\"\n\ndetection = detect_content_type(content)\nif detection.content_type == ContentType.SEARCH_RESULTS:\n    # Route to SearchCompressor\n    pass\nelif detection.content_type == ContentType.BUILD_OUTPUT:\n    # Route to LogCompressor\n    pass\nelif detection.content_type == ContentType.PLAIN_TEXT:\n    # Route to TextCompressor\n    pass\n</code></pre>"},{"location":"text-compression/#content-types","title":"Content Types","text":"Type Detection Pattern <code>SEARCH_RESULTS</code> <code>file:line:content</code> format <code>BUILD_OUTPUT</code> pytest, npm, cargo markers <code>JSON</code> Valid JSON structure <code>PLAIN_TEXT</code> Default fallback"},{"location":"text-compression/#integration-pattern","title":"Integration Pattern","text":"<pre><code>from headroom.transforms import (\n    detect_content_type, ContentType,\n    SearchCompressor, LogCompressor, TextCompressor\n)\n\ndef compress_tool_output(content: str, context: str = \"\") -&gt; str:\n    \"\"\"Application-level compression with explicit control.\"\"\"\n    detection = detect_content_type(content)\n\n    if detection.content_type == ContentType.SEARCH_RESULTS:\n        result = SearchCompressor().compress(content, context)\n        return result.compressed\n    elif detection.content_type == ContentType.BUILD_OUTPUT:\n        result = LogCompressor().compress(content)\n        return result.compressed\n    elif detection.content_type == ContentType.PLAIN_TEXT:\n        result = TextCompressor().compress(content, context)\n        return result.compressed\n    else:\n        # JSON or other - let SmartCrusher handle it automatically\n        return content\n</code></pre>"},{"location":"text-compression/#configuration","title":"Configuration","text":"<p>Each compressor accepts configuration options:</p> <pre><code>from headroom.transforms import SearchCompressor, SearchCompressorConfig\n\nconfig = SearchCompressorConfig(\n    max_results=50,           # Keep up to 50 matches\n    preserve_file_diversity=True,  # Ensure different files represented\n    relevance_threshold=0.3,  # Minimum relevance score to keep\n)\n\ncompressor = SearchCompressor(config)\n</code></pre>"},{"location":"text-compression/#performance","title":"Performance","text":"Compressor Typical Input Output Speed SearchCompressor 1000 matches 30-50 matches ~2ms LogCompressor 5000 lines 100-200 lines ~3ms TextCompressor 10000 chars 2000 chars ~2ms"},{"location":"text-compression/#when-to-use","title":"When to Use","text":"Scenario Recommendation JSON tool output Let SmartCrusher handle automatically grep/ripgrep results Use SearchCompressor pytest/npm/cargo output Use LogCompressor Documentation/README Use TextCompressor Unknown content Use detect_content_type to route"},{"location":"transforms/","title":"Transform Reference","text":"<p>Headroom provides several transforms that work together to optimize LLM context.</p>"},{"location":"transforms/#smartcrusher","title":"SmartCrusher","text":"<p>Statistical compression for JSON tool outputs.</p>"},{"location":"transforms/#how-it-works","title":"How It Works","text":"<p>SmartCrusher analyzes JSON arrays and selectively keeps important items:</p> <ol> <li>First/Last items - Context for pagination and recency</li> <li>Error items - 100% preservation of error states</li> <li>Anomalies - Statistical outliers (&gt; 2 std dev from mean)</li> <li>Relevant items - Matches to user's query via BM25/embeddings</li> <li>Change points - Significant transitions in data</li> </ol>"},{"location":"transforms/#configuration","title":"Configuration","text":"<pre><code>from headroom import SmartCrusherConfig\n\nconfig = SmartCrusherConfig(\n    min_tokens_to_crush=200,      # Only compress if &gt; 200 tokens\n    max_items_after_crush=50,     # Keep at most 50 items\n    keep_first=3,                 # Always keep first 3 items\n    keep_last=2,                  # Always keep last 2 items\n    relevance_threshold=0.3,      # Keep items with relevance &gt; 0.3\n    anomaly_std_threshold=2.0,    # Keep items &gt; 2 std dev from mean\n    preserve_errors=True,         # Always keep error items\n)\n</code></pre>"},{"location":"transforms/#example","title":"Example","text":"<pre><code>from headroom import SmartCrusher\n\ncrusher = SmartCrusher(config)\n\n# Before: 1000 search results (45,000 tokens)\ntool_output = {\"results\": [...1000 items...]}\n\n# After: ~50 important items (4,500 tokens) - 90% reduction\ncompressed = crusher.crush(tool_output, query=\"user's question\")\n</code></pre>"},{"location":"transforms/#what-gets-preserved","title":"What Gets Preserved","text":"Category Preserved Why Errors 100% Critical for debugging First N 100% Context/pagination Last N 100% Recency Anomalies All Unusual values matter Relevant Top K Match user's query Others Sampled Statistical representation"},{"location":"transforms/#cachealigner","title":"CacheAligner","text":"<p>Prefix stabilization for improved cache hit rates.</p>"},{"location":"transforms/#the-problem","title":"The Problem","text":"<p>LLM providers cache request prefixes. But dynamic content breaks caching:</p> <pre><code>\"You are helpful. Today is January 7, 2025.\"  # Changes daily = no cache\n</code></pre>"},{"location":"transforms/#the-solution","title":"The Solution","text":"<p>CacheAligner extracts dynamic content to stabilize the prefix:</p> <pre><code>from headroom import CacheAligner\n\naligner = CacheAligner()\nresult = aligner.align(messages)\n\n# Static prefix (cacheable):\n# \"You are helpful.\"\n\n# Dynamic content moved to end:\n# [Current date context]\n</code></pre>"},{"location":"transforms/#configuration_1","title":"Configuration","text":"<pre><code>from headroom import CacheAlignerConfig\n\nconfig = CacheAlignerConfig(\n    extract_dates=True,           # Move dates to dynamic section\n    normalize_whitespace=True,    # Consistent spacing\n    stable_prefix_min_tokens=100, # Min prefix size for alignment\n)\n</code></pre>"},{"location":"transforms/#cache-hit-improvement","title":"Cache Hit Improvement","text":"Scenario Before After Daily date in prompt 0% hits ~95% hits Dynamic user context ~10% hits ~80% hits Consistent prompts ~90% hits ~95% hits"},{"location":"transforms/#rollingwindow","title":"RollingWindow","text":"<p>Context management within token limits.</p>"},{"location":"transforms/#the-problem_1","title":"The Problem","text":"<p>Long conversations exceed context limits. Naive truncation breaks tool calls:</p> <pre><code>[tool_call: search]  # Kept\n[tool_result: ...]   # Dropped = orphaned call!\n</code></pre>"},{"location":"transforms/#the-solution_1","title":"The Solution","text":"<p>RollingWindow drops complete tool units, preserving pairs:</p> <pre><code>from headroom import RollingWindow\n\nwindow = RollingWindow(config)\nresult = window.apply(messages, max_tokens=100000)\n\n# Guarantees:\n# 1. Tool calls paired with results\n# 2. System prompt preserved\n# 3. Recent turns kept\n# 4. Oldest tool outputs dropped first\n</code></pre>"},{"location":"transforms/#configuration_2","title":"Configuration","text":"<pre><code>from headroom import RollingWindowConfig\n\nconfig = RollingWindowConfig(\n    max_tokens=100000,            # Target token limit\n    preserve_system=True,         # Always keep system prompt\n    preserve_recent_turns=5,      # Keep last 5 user/assistant turns\n    drop_oldest_first=True,       # Remove oldest tool outputs\n)\n</code></pre>"},{"location":"transforms/#drop-priority","title":"Drop Priority","text":"<ol> <li>Oldest tool outputs - First to go</li> <li>Old assistant messages - Summary preserved</li> <li>Old user messages - Only if necessary</li> <li>Never dropped: System prompt, recent turns, active tool pairs</li> </ol> <p>Note: For more intelligent context management based on semantic importance rather than just position, see IntelligentContextManager below.</p>"},{"location":"transforms/#intelligentcontextmanager","title":"IntelligentContextManager","text":"<p>Semantic-aware context management with TOIN-learned importance scoring.</p>"},{"location":"transforms/#the-problem_2","title":"The Problem","text":"<p>RollingWindow drops messages by position (oldest first), but position doesn't equal importance:</p> <ul> <li>An error message from turn 3 might be critical</li> <li>A verbose success response from turn 10 might be expendable</li> <li>Messages referenced by later turns should be preserved</li> </ul>"},{"location":"transforms/#the-solution_2","title":"The Solution","text":"<p>IntelligentContextManager uses multi-factor importance scoring:</p> <pre><code>from headroom.transforms import IntelligentContextManager, IntelligentContextConfig\n\nmanager = IntelligentContextManager(config)\nresult = manager.apply(messages, tokenizer, model_limit=128000)\n\n# Guarantees:\n# 1. System messages never dropped (configurable)\n# 2. Last N turns always protected\n# 3. Tool calls/responses dropped atomically\n# 4. Drops by importance score, not just position\n</code></pre>"},{"location":"transforms/#how-scoring-works","title":"How Scoring Works","text":"<p>Messages are scored on multiple factors (all learned, no hardcodes):</p> Factor Weight Description Recency 20% Exponential decay from conversation end Semantic Similarity 20% Embedding similarity to recent context TOIN Importance 25% Learned from retrieval patterns Error Indicators 15% TOIN-learned error field detection Forward References 15% Messages referenced by later messages Token Density 5% Information density (unique/total tokens) <p>Key principle: No hardcoded patterns. Error detection uses TOIN's <code>field_semantics.inferred_type == \"error_indicator\"</code>, not keyword matching.</p>"},{"location":"transforms/#configuration_3","title":"Configuration","text":"<pre><code>from headroom.transforms import IntelligentContextManager\nfrom headroom.config import IntelligentContextConfig, ScoringWeights\n\n# Custom scoring weights\nweights = ScoringWeights(\n    recency=0.20,\n    semantic_similarity=0.20,\n    toin_importance=0.25,\n    error_indicator=0.15,\n    forward_reference=0.15,\n    token_density=0.05,\n)\n\nconfig = IntelligentContextConfig(\n    enabled=True,\n    keep_system=True,              # Never drop system messages\n    keep_last_turns=2,             # Protect last N user turns\n    output_buffer_tokens=4000,     # Reserve for model output\n    use_importance_scoring=True,   # Enable semantic scoring\n    scoring_weights=weights,       # Custom weights\n    toin_integration=True,         # Use TOIN patterns\n    recency_decay_rate=0.1,        # Exponential decay lambda\n    compress_threshold=0.1,        # Try compression first if &lt;10% over\n)\n\nmanager = IntelligentContextManager(config)\n</code></pre>"},{"location":"transforms/#strategy-selection","title":"Strategy Selection","text":"<p>Based on how much over budget you are:</p> Overage Strategy Action Under budget NONE No action needed &lt; 10% over COMPRESS_FIRST Try deeper compression &gt;= 10% over DROP_BY_SCORE Drop lowest-scored messages"},{"location":"transforms/#toin-ccr-integration","title":"TOIN + CCR Integration","text":"<p>IntelligentContextManager is a message-level compressor. Just like SmartCrusher compresses items in a JSON array, IntelligentContext \"compresses\" messages in a conversation by dropping low-value ones.</p> <p>Bidirectional TOIN integration:</p> <ol> <li>Scoring uses TOIN patterns: Learned retrieval rates and field semantics inform importance scores</li> <li>Drops are recorded to TOIN: When messages are dropped, TOIN learns the pattern</li> <li>CCR stores originals: Dropped messages are stored in CCR for potential retrieval</li> <li>Retrievals feed back to TOIN: If users retrieve dropped messages, TOIN learns to score those patterns higher</li> </ol> <pre><code>from headroom.telemetry import get_toin\n\ntoin = get_toin()\nmanager = IntelligentContextManager(config, toin=toin)\n\n# TOIN provides (for scoring):\n# - retrieval_rate: How often this message pattern is retrieved (high = important)\n# - field_semantics: Learned field types (error_indicator, identifier, etc.)\n# - commonly_retrieved_fields: Fields that users frequently need\n\n# TOIN receives (from drops):\n# - Message pattern signatures (role counts, has_tools, has_errors)\n# - Token counts (original vs marker size)\n# - Retrieval feedback when users access CCR\n</code></pre> <p>What this means: - When you drop a message pattern and users frequently retrieve it, TOIN learns to score it higher next time - When you drop a pattern and no one retrieves it, that confirms it was safe to drop - The feedback loop improves drop decisions across all users, not just in one session</p>"},{"location":"transforms/#example-before-vs-after","title":"Example: Before vs After","text":"<p>RollingWindow (position-based): <pre><code>Messages: [sys, user1, asst1, user2, asst2_error, user3, asst3, user4, asst4]\nOver budget by 3 messages.\nDrops: user1, asst1, user2 (oldest first)\nResult: Loses context, keeps verbose asst3\n</code></pre></p> <p>IntelligentContextManager (score-based): <pre><code>Messages scored:\n  - asst2_error: 0.85 (TOIN learned error indicator)\n  - asst1: 0.45 (old, low density)\n  - asst3: 0.40 (verbose, low unique tokens)\n\nDrops: asst1, asst3, user1 (lowest scores)\nResult: Preserves critical error message\n</code></pre></p>"},{"location":"transforms/#backwards-compatibility","title":"Backwards Compatibility","text":"<p>Convert from RollingWindowConfig:</p> <pre><code>from headroom.config import IntelligentContextConfig, RollingWindowConfig\n\nrolling_config = RollingWindowConfig(\n    max_tokens=100000,\n    preserve_system=True,\n    preserve_recent_turns=3,\n)\n\n# Convert to intelligent context config\nintelligent_config = IntelligentContextConfig(\n    keep_system=rolling_config.preserve_system,\n    keep_last_turns=rolling_config.preserve_recent_turns,\n)\n</code></pre>"},{"location":"transforms/#llmlinguacompressor-optional","title":"LLMLinguaCompressor (Optional)","text":"<p>ML-based compression using Microsoft's LLMLingua-2 model.</p>"},{"location":"transforms/#when-to-use","title":"When to Use","text":"Transform Best For Speed Compression SmartCrusher JSON arrays ~1ms 70-90% Text Utilities Search/logs ~1ms 50-90% LLMLinguaCompressor Any text, max compression 50-200ms 80-95%"},{"location":"transforms/#installation","title":"Installation","text":"<pre><code>pip install \"headroom-ai[llmlingua]\"  # Adds ~2GB\n</code></pre>"},{"location":"transforms/#configuration_4","title":"Configuration","text":"<pre><code>from headroom.transforms import LLMLinguaCompressor, LLMLinguaConfig\n\nconfig = LLMLinguaConfig(\n    device=\"auto\",                    # auto, cuda, cpu, mps\n    target_compression_rate=0.3,      # Keep 30% of tokens\n    min_tokens_for_compression=100,   # Skip small content\n    code_compression_rate=0.4,        # Conservative for code\n    json_compression_rate=0.35,       # Moderate for JSON\n    text_compression_rate=0.25,       # Aggressive for text\n    enable_ccr=True,                  # Store original for retrieval\n)\n\ncompressor = LLMLinguaCompressor(config)\n</code></pre>"},{"location":"transforms/#content-aware-rates","title":"Content-Aware Rates","text":"<p>LLMLinguaCompressor auto-detects content type:</p> Content Type Default Rate Behavior Code 0.4 Conservative - preserves syntax JSON 0.35 Moderate - keeps structure Text 0.3 Aggressive - maximum compression"},{"location":"transforms/#memory-management","title":"Memory Management","text":"<pre><code>from headroom.transforms import (\n    is_llmlingua_model_loaded,\n    unload_llmlingua_model,\n)\n\n# Check if model is loaded\nprint(is_llmlingua_model_loaded())  # True/False\n\n# Free ~1GB RAM when done\nunload_llmlingua_model()\n</code></pre>"},{"location":"transforms/#proxy-integration","title":"Proxy Integration","text":"<pre><code># Enable in proxy\nheadroom proxy --llmlingua --llmlingua-device cuda --llmlingua-rate 0.3\n</code></pre>"},{"location":"transforms/#codeawarecompressor-optional","title":"CodeAwareCompressor (Optional)","text":"<p>AST-based compression for source code using tree-sitter.</p>"},{"location":"transforms/#when-to-use_1","title":"When to Use","text":"Transform Best For Speed Compression SmartCrusher JSON arrays ~1ms 70-90% CodeAwareCompressor Source code ~10-50ms 40-70% LLMLinguaCompressor Any text 50-200ms 80-95%"},{"location":"transforms/#key-benefits","title":"Key Benefits","text":"<ul> <li>Syntax validity guaranteed \u2014 Output always parses correctly</li> <li>Preserves critical structure \u2014 Imports, signatures, types, error handlers</li> <li>Multi-language support \u2014 Python, JavaScript, TypeScript, Go, Rust, Java, C, C++</li> <li>Lightweight \u2014 ~50MB vs ~1GB for LLMLingua</li> </ul>"},{"location":"transforms/#installation_1","title":"Installation","text":"<pre><code>pip install \"headroom-ai[code]\"  # Adds tree-sitter-language-pack\n</code></pre>"},{"location":"transforms/#configuration_5","title":"Configuration","text":"<pre><code>from headroom.transforms import CodeAwareCompressor, CodeCompressorConfig, DocstringMode\n\nconfig = CodeCompressorConfig(\n    preserve_imports=True,              # Always keep imports\n    preserve_signatures=True,           # Always keep function signatures\n    preserve_type_annotations=True,     # Keep type hints\n    preserve_error_handlers=True,       # Keep try/except blocks\n    preserve_decorators=True,           # Keep decorators\n    docstring_mode=DocstringMode.FIRST_LINE,  # FULL, FIRST_LINE, REMOVE\n    target_compression_rate=0.2,        # Keep 20% of tokens\n    max_body_lines=5,                   # Lines to keep per function body\n    min_tokens_for_compression=100,     # Skip small content\n    language_hint=None,                 # Auto-detect if None\n    fallback_to_llmlingua=True,         # Use LLMLingua for unknown langs\n)\n\ncompressor = CodeAwareCompressor(config)\n</code></pre>"},{"location":"transforms/#example_1","title":"Example","text":"<pre><code>from headroom.transforms import CodeAwareCompressor\n\ncompressor = CodeAwareCompressor()\n\ncode = '''\nimport os\nfrom typing import List\n\ndef process_items(items: List[str]) -&gt; List[str]:\n    \"\"\"Process a list of items.\"\"\"\n    results = []\n    for item in items:\n        if not item:\n            continue\n        processed = item.strip().lower()\n        results.append(processed)\n    return results\n'''\n\nresult = compressor.compress(code, language=\"python\")\nprint(result.compressed)\n# import os\n# from typing import List\n#\n# def process_items(items: List[str]) -&gt; List[str]:\n#     \"\"\"Process a list of items.\"\"\"\n#     results = []\n#     for item in items:\n#     # ... (5 lines compressed)\n#     pass\n\nprint(f\"Compression: {result.compression_ratio:.0%}\")  # ~55%\nprint(f\"Syntax valid: {result.syntax_valid}\")  # True\n</code></pre>"},{"location":"transforms/#supported-languages","title":"Supported Languages","text":"Tier Languages Support Level 1 Python, JavaScript, TypeScript Full AST analysis 2 Go, Rust, Java, C, C++ Function body compression"},{"location":"transforms/#memory-management_1","title":"Memory Management","text":"<pre><code>from headroom.transforms import is_tree_sitter_available, unload_tree_sitter\n\n# Check if tree-sitter is installed\nprint(is_tree_sitter_available())  # True/False\n\n# Free memory when done (parsers are lazy-loaded)\nunload_tree_sitter()\n</code></pre>"},{"location":"transforms/#contentrouter","title":"ContentRouter","text":"<p>Intelligent compression orchestrator that routes content to the optimal compressor.</p>"},{"location":"transforms/#how-it-works_1","title":"How It Works","text":"<p>ContentRouter analyzes content and selects the best compression strategy:</p> <ol> <li>Detect content type \u2014 JSON, code, logs, search results, plain text</li> <li>Consider source hints \u2014 File paths, tool names for high-confidence routing</li> <li>Route to compressor \u2014 SmartCrusher, CodeAwareCompressor, SearchCompressor, etc.</li> <li>Log decisions \u2014 Transparent routing for debugging</li> </ol>"},{"location":"transforms/#configuration_6","title":"Configuration","text":"<pre><code>from headroom.transforms import ContentRouter, ContentRouterConfig, CompressionStrategy\n\nconfig = ContentRouterConfig(\n    min_section_tokens=100,             # Minimum tokens to compress\n    enable_code_aware=True,             # Use CodeAwareCompressor for code\n    enable_search_compression=True,     # Use SearchCompressor for grep output\n    enable_log_compression=True,        # Use LogCompressor for logs\n    default_strategy=CompressionStrategy.TEXT,  # Fallback strategy\n)\n\nrouter = ContentRouter(config)\n</code></pre>"},{"location":"transforms/#example_2","title":"Example","text":"<pre><code>from headroom.transforms import ContentRouter\n\nrouter = ContentRouter()\n\n# Router auto-detects content type and routes to optimal compressor\nresult = router.compress(content)\n\nprint(result.strategy_used)  # CompressionStrategy.CODE_AWARE, SMART_CRUSHER, etc.\nprint(result.routing_log)  # List of routing decisions\n</code></pre>"},{"location":"transforms/#compression-strategies","title":"Compression Strategies","text":"Strategy Used For Compressor CODE_AWARE Source code CodeAwareCompressor SMART_CRUSHER JSON arrays SmartCrusher SEARCH Grep/find output SearchCompressor LOG Log files LogCompressor TEXT Plain text TextCompressor LLMLINGUA Any (max compression) LLMLinguaCompressor PASSTHROUGH Small content None"},{"location":"transforms/#content-detection","title":"Content Detection","text":"<p>The router automatically detects content types by analyzing the content itself:</p> <ul> <li>Source code: Detected by syntax patterns, indentation, keywords</li> <li>JSON arrays: Detected by JSON structure with array elements</li> <li>Search results: Detected by <code>file:line:</code> patterns</li> <li>Log output: Detected by timestamp and log level patterns</li> <li>Plain text: Fallback for prose content</li> </ul> <p>No manual hints required - the router inspects content directly.</p>"},{"location":"transforms/#toin-integration","title":"TOIN Integration","text":"<p>ContentRouter records all compressions to TOIN (Tool Output Intelligence Network) for cross-user learning:</p> <ul> <li>All strategies tracked: Code, search, logs, text, and LLMLingua compressions are recorded</li> <li>Retrieval feedback: When users retrieve original content via CCR, TOIN learns which compressions need expansion</li> <li>Pattern learning: TOIN builds signatures for each content type to improve future compressions</li> </ul> <p>This enables the feedback loop where compression decisions improve based on actual user behavior across all content types, not just JSON arrays.</p>"},{"location":"transforms/#transformpipeline","title":"TransformPipeline","text":"<p>Combine transforms for optimal results.</p> <pre><code>from headroom import TransformPipeline, SmartCrusher, CacheAligner, RollingWindow\n\npipeline = TransformPipeline([\n    SmartCrusher(),      # First: compress tool outputs\n    CacheAligner(),      # Then: stabilize prefix\n    RollingWindow(),     # Finally: fit in context\n])\n\nresult = pipeline.transform(messages)\nprint(f\"Saved {result.tokens_saved} tokens\")\n</code></pre>"},{"location":"transforms/#with-llmlingua-optional","title":"With LLMLingua (Optional)","text":"<pre><code>from headroom.transforms import (\n    TransformPipeline, SmartCrusher, CacheAligner,\n    RollingWindow, LLMLinguaCompressor\n)\n\npipeline = TransformPipeline([\n    CacheAligner(),         # 1. Stabilize prefix\n    SmartCrusher(),         # 2. Compress JSON arrays\n    LLMLinguaCompressor(),  # 3. ML compression on remaining text\n    RollingWindow(),        # 4. Final size constraint (always last)\n])\n</code></pre>"},{"location":"transforms/#recommended-order","title":"Recommended Order","text":"Order Transform Purpose 1 CacheAligner Stabilize prefix for caching 2 SmartCrusher Compress JSON tool outputs 3 LLMLinguaCompressor ML compression (optional) 4 RollingWindow Enforce token limits (always last) <p>Why this order? - CacheAligner first to maximize prefix stability - SmartCrusher handles JSON arrays efficiently - LLMLingua compresses remaining long text - RollingWindow truncates only if still over limit</p>"},{"location":"transforms/#safety-guarantees","title":"Safety Guarantees","text":"<p>All transforms follow strict safety rules:</p> <ol> <li>Never remove human content - User/assistant text is sacred</li> <li>Never break tool ordering - Calls and results stay paired</li> <li>Parse failures are no-ops - Malformed content passes through</li> <li>Preserves recency - Last N turns always kept</li> <li>100% error preservation - Error items never dropped</li> </ol>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>Solutions for common Headroom issues.</p>"},{"location":"troubleshooting/#proxy-server-issues","title":"Proxy Server Issues","text":""},{"location":"troubleshooting/#proxy-wont-start","title":"\"Proxy won't start\"","text":"<p>Symptom: <code>headroom proxy</code> fails or hangs.</p> <p>Solutions:</p> <pre><code># 1. Check if port is already in use\nlsof -i :8787\n# If something is using the port, either kill it or use a different port\n\n# 2. Try a different port\nheadroom proxy --port 8788\n\n# 3. Check for missing dependencies\npip install \"headroom[proxy]\"\n\n# 4. Run with debug logging\nheadroom proxy --log-level debug\n</code></pre>"},{"location":"troubleshooting/#connection-refused-when-calling-proxy","title":"\"Connection refused\" when calling proxy","text":"<p>Symptom: <code>curl: (7) Failed to connect to localhost port 8787</code></p> <p>Solutions:</p> <pre><code># 1. Verify proxy is running\ncurl http://localhost:8787/health\n\n# 2. Check if proxy started on a different port\nps aux | grep headroom\n\n# 3. Check firewall settings (macOS)\nsudo pfctl -s rules | grep 8787\n</code></pre>"},{"location":"troubleshooting/#proxy-returns-errors-for-some-requests","title":"\"Proxy returns errors for some requests\"","text":"<p>Symptom: Some requests work, others fail with 502/503.</p> <p>Solutions:</p> <pre><code># 1. Check proxy logs for the actual error\nheadroom proxy --log-level debug\n\n# 2. Verify API key is set\necho $OPENAI_API_KEY  # or ANTHROPIC_API_KEY\n\n# 3. Test the underlying API directly\ncurl https://api.openai.com/v1/models -H \"Authorization: Bearer $OPENAI_API_KEY\"\n</code></pre>"},{"location":"troubleshooting/#sdk-issues","title":"SDK Issues","text":""},{"location":"troubleshooting/#no-token-savings","title":"\"No token savings\"","text":"<p>Symptom: <code>stats['session']['tokens_saved_total']</code> is 0.</p> <p>Diagnosis:</p> <pre><code># 1. Check mode\nstats = client.get_stats()\nprint(f\"Mode: {stats['config']['mode']}\")  # Should be \"optimize\"\n\n# 2. Check transforms are enabled\nprint(f\"SmartCrusher: {stats['transforms']['smart_crusher_enabled']}\")\n\n# 3. Check if content meets threshold\n# SmartCrusher only compresses tool outputs &gt; 200 tokens by default\n</code></pre> <p>Solutions:</p> <pre><code># 1. Ensure mode is \"optimize\"\nclient = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n    default_mode=\"optimize\",  # NOT \"audit\"\n)\n\n# 2. Or override per-request\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages,\n    headroom_mode=\"optimize\",\n)\n\n# 3. Lower the compression threshold\nconfig = HeadroomConfig()\nconfig.smart_crusher.min_tokens_to_crush = 100  # Default is 200\n</code></pre> <p>Why It Might Be 0: - Mode is \"audit\" (observation only) - Messages don't contain tool outputs - Tool outputs are below the token threshold - Data isn't compressible (high uniqueness)</p>"},{"location":"troubleshooting/#compression-too-aggressive","title":"\"Compression too aggressive\"","text":"<p>Symptom: LLM responses are missing information that was in tool outputs.</p> <p>Solutions:</p> <pre><code># 1. Keep more items\nconfig = HeadroomConfig()\nconfig.smart_crusher.max_items_after_crush = 50  # Default: 15\n\n# 2. Skip compression for specific tools\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages,\n    headroom_tool_profiles={\n        \"important_tool\": {\"skip_compression\": True},\n    },\n)\n\n# 3. Disable SmartCrusher entirely\nconfig.smart_crusher.enabled = False\n</code></pre>"},{"location":"troubleshooting/#high-latency","title":"\"High latency\"","text":"<p>Symptom: Requests take longer than expected.</p> <p>Diagnosis:</p> <pre><code>import time\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\n\nstart = time.time()\nresponse = client.chat.completions.create(...)\nprint(f\"Total time: {time.time() - start:.2f}s\")\n\n# Check logs for:\n# - \"SmartCrusher\" timing\n# - \"EmbeddingScorer\" timing (slow if using embeddings)\n</code></pre> <p>Solutions:</p> <pre><code># 1. Use BM25 instead of embeddings (faster)\nconfig = HeadroomConfig()\nconfig.smart_crusher.relevance.tier = \"bm25\"  # Default may use embeddings\n\n# 2. Increase threshold to skip small payloads\nconfig.smart_crusher.min_tokens_to_crush = 500\n\n# 3. Disable transforms you don't need\nconfig.cache_aligner.enabled = False\nconfig.rolling_window.enabled = False\n</code></pre>"},{"location":"troubleshooting/#validationerror-on-setup","title":"\"ValidationError on setup\"","text":"<p>Symptom: <code>validate_setup()</code> returns errors.</p> <p>Common Issues:</p> <pre><code>result = client.validate_setup()\nprint(result)\n\n# Provider error:\n# {\"provider\": {\"ok\": False, \"error\": \"No API key\"}}\n# \u2192 Set OPENAI_API_KEY or pass api_key to OpenAI()\n\n# Storage error:\n# {\"storage\": {\"ok\": False, \"error\": \"unable to open database\"}}\n# \u2192 Check path permissions, use :memory: for testing\n\n# Config error:\n# {\"config\": {\"ok\": False, \"error\": \"Invalid mode\"}}\n# \u2192 Use \"audit\" or \"optimize\" only\n</code></pre> <p>Solutions:</p> <pre><code># 1. For testing, use in-memory storage\nclient = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n    store_url=\"sqlite:///:memory:\",  # No file created\n)\n\n# 2. For temp directory storage\nimport tempfile\nimport os\ndb_path = os.path.join(tempfile.gettempdir(), \"headroom.db\")\nclient = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n    store_url=f\"sqlite:///{db_path}\",\n)\n</code></pre>"},{"location":"troubleshooting/#importinstallation-issues","title":"Import/Installation Issues","text":""},{"location":"troubleshooting/#modulenotfounderror-no-module-named-headroom","title":"\"ModuleNotFoundError: No module named 'headroom'\"","text":"<pre><code># 1. Check it's installed in the right environment\npip show headroom\n\n# 2. If using virtual environment, ensure it's activated\nsource venv/bin/activate  # or equivalent\n\n# 3. Reinstall\npip install --upgrade headroom\n</code></pre>"},{"location":"troubleshooting/#importerror-cannot-import-name-x-from-headroom","title":"\"ImportError: cannot import name 'X' from 'headroom'\"","text":"<pre><code># Check available imports\nimport headroom\nprint(dir(headroom))\n\n# Common imports:\nfrom headroom import (\n    HeadroomClient,\n    OpenAIProvider,\n    AnthropicProvider,\n    HeadroomConfig,\n    # Exceptions\n    HeadroomError,\n    ConfigurationError,\n    ProviderError,\n)\n</code></pre>"},{"location":"troubleshooting/#missing-optional-dependency","title":"\"Missing optional dependency\"","text":"<pre><code># For proxy server\npip install \"headroom[proxy]\"\n\n# For embedding-based relevance scoring\npip install \"headroom[relevance]\"\n\n# For everything\npip install \"headroom[all]\"\n</code></pre>"},{"location":"troubleshooting/#provider-specific-issues","title":"Provider-Specific Issues","text":""},{"location":"troubleshooting/#openai-invalid-api-key","title":"OpenAI: \"Invalid API key\"","text":"<pre><code>from openai import OpenAI\nimport os\n\n# Ensure key is set\napi_key = os.environ.get(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"OPENAI_API_KEY not set\")\n\nclient = HeadroomClient(\n    original_client=OpenAI(api_key=api_key),\n    provider=OpenAIProvider(),\n)\n</code></pre>"},{"location":"troubleshooting/#anthropic-authentication-error","title":"Anthropic: \"Authentication error\"","text":"<pre><code>from anthropic import Anthropic\nimport os\n\napi_key = os.environ.get(\"ANTHROPIC_API_KEY\")\nclient = HeadroomClient(\n    original_client=Anthropic(api_key=api_key),\n    provider=AnthropicProvider(),\n)\n</code></pre>"},{"location":"troubleshooting/#unknown-model-warnings","title":"\"Unknown model\" warnings","text":"<pre><code># For custom/fine-tuned models, specify context limit\nclient = HeadroomClient(\n    original_client=OpenAI(),\n    provider=OpenAIProvider(),\n    model_context_limits={\n        \"ft:gpt-4o-2024-08-06:my-org::abc123\": 128000,\n        \"my-custom-model\": 32000,\n    },\n)\n</code></pre>"},{"location":"troubleshooting/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"troubleshooting/#enable-full-logging","title":"Enable Full Logging","text":"<pre><code>import logging\n\n# See everything\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format=\"%(asctime)s %(name)s %(levelname)s %(message)s\",\n)\n\n# Or just Headroom logs\nlogging.getLogger(\"headroom\").setLevel(logging.DEBUG)\n</code></pre>"},{"location":"troubleshooting/#inspect-transform-results","title":"Inspect Transform Results","text":"<pre><code># Use simulate to see what would happen\nplan = client.chat.completions.simulate(\n    model=\"gpt-4o\",\n    messages=messages,\n)\n\nprint(f\"Tokens: {plan.tokens_before} -&gt; {plan.tokens_after}\")\nprint(f\"Transforms: {plan.transforms}\")\nprint(f\"Waste signals: {plan.waste_signals}\")\n\n# See the actual optimized messages\nimport json\nprint(json.dumps(plan.messages_optimized, indent=2))\n</code></pre>"},{"location":"troubleshooting/#check-storage-contents","title":"Check Storage Contents","text":"<pre><code>from datetime import datetime, timedelta\n\n# Get recent metrics\nmetrics = client.get_metrics(\n    start_time=datetime.utcnow() - timedelta(hours=1),\n    limit=10,\n)\n\nfor m in metrics:\n    print(f\"{m.timestamp}: {m.tokens_input_before} -&gt; {m.tokens_input_after}\")\n    print(f\"  Transforms: {m.transforms_applied}\")\n    if m.error:\n        print(f\"  ERROR: {m.error}\")\n</code></pre>"},{"location":"troubleshooting/#manual-transform-testing","title":"Manual Transform Testing","text":"<pre><code>from headroom import SmartCrusher, Tokenizer\nfrom headroom.config import SmartCrusherConfig\nimport json\n\n# Test compression directly\nconfig = SmartCrusherConfig()\ncrusher = SmartCrusher(config)\ntokenizer = Tokenizer()\n\nmessages = [\n    {\"role\": \"tool\", \"content\": json.dumps({\"items\": list(range(100))}), \"tool_call_id\": \"1\"}\n]\n\nresult = crusher.apply(messages, tokenizer)\nprint(f\"Tokens: {result.tokens_before} -&gt; {result.tokens_after}\")\nprint(f\"Compressed content: {result.messages[0]['content'][:200]}...\")\n</code></pre>"},{"location":"troubleshooting/#error-reference","title":"Error Reference","text":"Exception Meaning Solution <code>ConfigurationError</code> Invalid config values Check config parameters <code>ProviderError</code> Provider issue (unknown model, etc.) Set model_context_limits <code>StorageError</code> Database issue Check path/permissions <code>CompressionError</code> Compression failed Rare - check data format <code>TokenizationError</code> Token counting failed Check model name <code>ValidationError</code> Setup validation failed Run validate_setup()"},{"location":"troubleshooting/#handling-errors","title":"Handling Errors","text":"<pre><code>from headroom import (\n    HeadroomClient,\n    HeadroomError,\n    ConfigurationError,\n    StorageError,\n)\n\ntry:\n    client = HeadroomClient(...)\n    response = client.chat.completions.create(...)\nexcept ConfigurationError as e:\n    print(f\"Config issue: {e}\")\n    print(f\"Details: {e.details}\")\nexcept StorageError as e:\n    print(f\"Storage issue: {e}\")\n    # Headroom continues to work, just without metrics persistence\nexcept HeadroomError as e:\n    print(f\"Headroom error: {e}\")\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<ol> <li>Enable debug logging and check the output</li> <li>Use simulate() to see what transforms would apply</li> <li>Check validate_setup() for configuration issues</li> <li>File an issue at https://github.com/headroom-sdk/headroom/issues</li> </ol> <p>When filing an issue, include: - Headroom version (<code>pip show headroom</code>) - Python version - Provider (OpenAI/Anthropic) - Debug log output - Minimal reproduction code</p>"},{"location":"plans/dynamic-smart-crusher/","title":"Dynamic SmartCrusher Preservation Plan","text":""},{"location":"plans/dynamic-smart-crusher/#problem-statement","title":"Problem Statement","text":"<p>Current SmartCrusher uses static \"First 3 + Last 2\" preservation regardless of: - Array size (100 items vs 10,000 items get same treatment) - Data pattern (time series vs search results vs logs) - Query context (user asking about \"latest\" vs \"oldest\") - Position importance (first items might all be identical/wasteful) - Learned retrieval patterns (which positions do users actually need?)</p> <p>This is too simplistic for a production-grade compression system.</p>"},{"location":"plans/dynamic-smart-crusher/#current-implementation-analysis","title":"Current Implementation Analysis","text":"<p>Location: <code>headroom/transforms/smart_crusher.py</code></p> <p>Current Logic (lines 2273-2279, 2353-2359, 2561-2567): <pre><code># Always keep first 3\nfor i in range(min(3, n)):\n    keep_indices.add(i)\n\n# Always keep last 2\nfor i in range(max(0, n - 2), n):\n    keep_indices.add(i)\n</code></pre></p> <p>Problems: 1. Fixed slots waste budget - If first 3 items are identical, we've wasted 3 slots 2. No size adaptation - 20-item array loses 25% to anchors; 1000-item array loses 0.5% 3. Pattern-agnostic - Search results don't need \"last 2\"; time series might need more recency 4. No learning - Doesn't adapt based on what users actually retrieve</p>"},{"location":"plans/dynamic-smart-crusher/#proposed-solution-adaptive-slot-allocation","title":"Proposed Solution: Adaptive Slot Allocation","text":""},{"location":"plans/dynamic-smart-crusher/#idea-1-size-proportional-anchor-budget","title":"Idea 1: Size-Proportional Anchor Budget","text":"<p>Instead of fixed counts, allocate a percentage budget for position-based anchors:</p> <pre><code>def calculate_anchor_budget(array_size: int, max_items: int) -&gt; AnchorBudget:\n    \"\"\"Allocate slots proportionally, with floors and ceilings.\"\"\"\n\n    # Base: 20% of output budget for position anchors\n    total_anchor_slots = max(3, min(10, int(max_items * 0.20)))\n\n    # Distribution: 60% front, 40% back (front-weighted for context)\n    front_slots = max(1, int(total_anchor_slots * 0.6))\n    back_slots = max(1, total_anchor_slots - front_slots)\n\n    return AnchorBudget(front=front_slots, back=back_slots)\n</code></pre> <p>Example: | Array Size | Max Items | Anchor Budget | Front | Back | |------------|-----------|---------------|-------|------| | 50         | 10        | 3             | 2     | 1    | | 200        | 15        | 3             | 2     | 1    | | 1000       | 20        | 4             | 3     | 1    | | 5000       | 30        | 6             | 4     | 2    |</p>"},{"location":"plans/dynamic-smart-crusher/#idea-2-pattern-aware-anchor-weighting","title":"Idea 2: Pattern-Aware Anchor Weighting","text":"<p>Different data patterns need different position importance:</p> <pre><code>class AnchorStrategy(Enum):\n    FRONT_HEAVY = \"front_heavy\"     # Search results: top items matter most\n    BACK_HEAVY = \"back_heavy\"       # Logs: recent items matter most\n    BALANCED = \"balanced\"           # Time series: both ends matter\n    MIDDLE_AWARE = \"middle_aware\"   # Database: order might be arbitrary\n\ndef get_anchor_strategy(pattern: DataPattern) -&gt; AnchorStrategy:\n    return {\n        DataPattern.SEARCH_RESULTS: AnchorStrategy.FRONT_HEAVY,  # Top N by score\n        DataPattern.LOGS: AnchorStrategy.BACK_HEAVY,             # Recency matters\n        DataPattern.TIME_SERIES: AnchorStrategy.BALANCED,        # Both ends for trend\n        DataPattern.GENERIC: AnchorStrategy.MIDDLE_AWARE,        # Don't assume order\n    }.get(pattern, AnchorStrategy.BALANCED)\n</code></pre> <p>FRONT_HEAVY (Search Results): - Front: 80% of anchor budget - Back: 20% of anchor budget - Rationale: Top search results are ranked by relevance</p> <p>BACK_HEAVY (Logs): - Front: 20% of anchor budget - Back: 80% of anchor budget - Rationale: Most recent logs are usually most relevant</p> <p>BALANCED (Time Series): - Front: 50% of anchor budget - Back: 50% of anchor budget - Rationale: Need both start and end for trend analysis</p> <p>MIDDLE_AWARE (Generic/Database): - Front: 30% of anchor budget - Back: 30% of anchor budget - Middle sample: 40% of anchor budget (stratified) - Rationale: Order might be arbitrary; sample across positions</p>"},{"location":"plans/dynamic-smart-crusher/#idea-3-query-aware-dynamic-weighting","title":"Idea 3: Query-Aware Dynamic Weighting","text":"<p>Adjust anchor strategy based on user's query:</p> <pre><code>def adjust_for_query(base_strategy: AnchorStrategy, query: str) -&gt; AnchorWeights:\n    \"\"\"Shift anchor weights based on query intent.\"\"\"\n\n    weights = base_strategy.default_weights()\n\n    # Recency signals\n    recency_keywords = [\"latest\", \"recent\", \"last\", \"newest\", \"current\"]\n    if any(kw in query.lower() for kw in recency_keywords):\n        weights.back_weight *= 1.5\n        weights.front_weight *= 0.7\n\n    # Historical signals\n    historical_keywords = [\"first\", \"oldest\", \"earliest\", \"original\", \"initial\"]\n    if any(kw in query.lower() for kw in historical_keywords):\n        weights.front_weight *= 1.5\n        weights.back_weight *= 0.7\n\n    # Range signals\n    range_keywords = [\"all\", \"every\", \"complete\", \"full\"]\n    if any(kw in query.lower() for kw in range_keywords):\n        weights.middle_weight *= 1.3  # Better coverage\n\n    return weights.normalize()\n</code></pre>"},{"location":"plans/dynamic-smart-crusher/#idea-4-information-density-anchor-selection","title":"Idea 4: Information-Density Anchor Selection","text":"<p>Don't blindly take first N - select most informative items from anchor regions:</p> <pre><code>def select_informative_anchors(\n    items: list[dict],\n    region: str,  # \"front\", \"back\", \"middle\"\n    slots: int,\n    all_items_hash: set[str]\n) -&gt; list[int]:\n    \"\"\"Select most informative items from a region.\"\"\"\n\n    if region == \"front\":\n        candidates = list(range(min(slots * 3, len(items))))  # Consider 3x candidates\n    elif region == \"back\":\n        start = max(0, len(items) - slots * 3)\n        candidates = list(range(start, len(items)))\n    else:  # middle\n        step = len(items) // (slots * 3 + 1)\n        candidates = [i * step for i in range(1, slots * 3 + 1)]\n\n    # Score each candidate by information content\n    scored = []\n    for idx in candidates:\n        item = items[idx]\n        item_hash = hash_item(item)\n\n        # Skip if we've seen identical item\n        if item_hash in all_items_hash:\n            continue\n\n        score = calculate_information_score(item, items)\n        scored.append((idx, score, item_hash))\n\n    # Select top N by information score\n    scored.sort(key=lambda x: x[1], reverse=True)\n    selected = []\n    for idx, _, item_hash in scored[:slots]:\n        selected.append(idx)\n        all_items_hash.add(item_hash)\n\n    return sorted(selected)\n\n\ndef calculate_information_score(item: dict, all_items: list[dict]) -&gt; float:\n    \"\"\"Score item by how much unique information it contributes.\"\"\"\n\n    score = 0.0\n\n    # 1. Field uniqueness - rare field values score higher\n    for field, value in item.items():\n        field_values = [i.get(field) for i in all_items if field in i]\n        value_frequency = field_values.count(value) / len(field_values)\n        score += (1 - value_frequency)  # Rare values score higher\n\n    # 2. Structural uniqueness - different fields than typical\n    typical_fields = get_typical_fields(all_items)\n    unique_fields = set(item.keys()) - typical_fields\n    score += len(unique_fields) * 0.5\n\n    # 3. Content length - longer items often more informative\n    content_length = len(json.dumps(item))\n    avg_length = sum(len(json.dumps(i)) for i in all_items) / len(all_items)\n    if content_length &gt; avg_length:\n        score += 0.3\n\n    return score\n</code></pre>"},{"location":"plans/dynamic-smart-crusher/#idea-5-toin-learned-position-importance","title":"Idea 5: TOIN-Learned Position Importance","text":"<p>Track which positions users actually retrieve and learn from it:</p> <pre><code>@dataclass\nclass PositionRetrievalPattern:\n    \"\"\"Learned position importance from retrieval data.\"\"\"\n    tool_name: str\n    total_compressions: int\n    position_retrievals: dict[str, int]  # \"front_10%\", \"middle\", \"back_10%\"\n\n    def get_position_weights(self) -&gt; dict[str, float]:\n        \"\"\"Convert retrieval counts to weights.\"\"\"\n        total = sum(self.position_retrievals.values())\n        if total == 0:\n            return {\"front\": 0.5, \"middle\": 0.0, \"back\": 0.5}\n\n        return {\n            position: count / total\n            for position, count in self.position_retrievals.items()\n        }\n\n\nclass TOINPositionLearning:\n    \"\"\"Learn position importance from retrieval patterns.\"\"\"\n\n    def record_retrieval(\n        self,\n        tool_name: str,\n        original_size: int,\n        retrieved_indices: list[int]\n    ):\n        \"\"\"Record which positions were retrieved.\"\"\"\n        for idx in retrieved_indices:\n            position = self._classify_position(idx, original_size)\n            self._increment_position_count(tool_name, position)\n\n    def _classify_position(self, idx: int, size: int) -&gt; str:\n        \"\"\"Classify index into position bucket.\"\"\"\n        relative_pos = idx / size\n        if relative_pos &lt; 0.1:\n            return \"front_10%\"\n        elif relative_pos &lt; 0.3:\n            return \"front_30%\"\n        elif relative_pos &gt; 0.9:\n            return \"back_10%\"\n        elif relative_pos &gt; 0.7:\n            return \"back_30%\"\n        else:\n            return \"middle\"\n\n    def get_anchor_recommendation(self, tool_name: str) -&gt; AnchorWeights:\n        \"\"\"Get learned anchor weights for a tool.\"\"\"\n        pattern = self._get_pattern(tool_name)\n        if pattern.total_compressions &lt; 10:\n            return AnchorWeights.default()  # Not enough data\n\n        weights = pattern.get_position_weights()\n        return AnchorWeights(\n            front=weights.get(\"front_10%\", 0.3) + weights.get(\"front_30%\", 0.1),\n            middle=weights.get(\"middle\", 0.2),\n            back=weights.get(\"back_10%\", 0.3) + weights.get(\"back_30%\", 0.1),\n        )\n</code></pre>"},{"location":"plans/dynamic-smart-crusher/#idea-6-stratified-sampling-for-middle-positions","title":"Idea 6: Stratified Sampling for Middle Positions","text":"<p>For large arrays, sample strategically from middle:</p> <pre><code>def stratified_middle_sample(\n    items: list[dict],\n    num_samples: int,\n    analysis: ArrayAnalysis\n) -&gt; list[int]:\n    \"\"\"Sample middle positions using stratified approach.\"\"\"\n\n    n = len(items)\n    front_boundary = int(n * 0.1)\n    back_boundary = int(n * 0.9)\n    middle_items = list(range(front_boundary, back_boundary))\n\n    if not middle_items or num_samples &lt;= 0:\n        return []\n\n    # Strategy 1: Cluster-based sampling\n    if analysis.has_clusterable_field:\n        clusters = cluster_by_field(items, analysis.cluster_field)\n        return sample_from_clusters(clusters, num_samples, middle_items)\n\n    # Strategy 2: Variance-based sampling (pick high-variance points)\n    if analysis.numeric_fields:\n        variance_scores = calculate_position_variance(items, analysis.numeric_fields)\n        sorted_by_variance = sorted(\n            middle_items,\n            key=lambda i: variance_scores.get(i, 0),\n            reverse=True\n        )\n        return sorted(sorted_by_variance[:num_samples])\n\n    # Strategy 3: Even spacing (fallback)\n    step = len(middle_items) // (num_samples + 1)\n    return [middle_items[i * step] for i in range(1, num_samples + 1)]\n</code></pre>"},{"location":"plans/dynamic-smart-crusher/#testing-strategy","title":"Testing Strategy","text":""},{"location":"plans/dynamic-smart-crusher/#test-category-1-adversarial-position-tests","title":"Test Category 1: Adversarial Position Tests","text":"<p>Test cases where important data is NOT at expected positions:</p> <pre><code>class TestAdversarialPositions:\n    \"\"\"Test scenarios that break 'first 3 + last 2' assumption.\"\"\"\n\n    def test_important_data_in_middle(self):\n        \"\"\"Critical error at position 50 of 100-item array.\"\"\"\n        items = [{\"status\": \"ok\", \"value\": i} for i in range(100)]\n        items[50] = {\"status\": \"error\", \"error_code\": \"CRITICAL\", \"value\": 50}\n\n        result = smart_crusher.crush(items, max_items=10)\n\n        # Error item MUST be preserved regardless of position\n        assert any(item.get(\"error_code\") == \"CRITICAL\" for item in result)\n\n    def test_spike_not_at_boundaries(self):\n        \"\"\"Numeric spike at position 75 of 100-item array.\"\"\"\n        items = [{\"metric\": 10.0 + random.random()} for _ in range(100)]\n        items[75][\"metric\"] = 1000.0  # Spike\n\n        result = smart_crusher.crush(items, max_items=10)\n\n        # Spike MUST be preserved as anomaly\n        assert any(item[\"metric\"] &gt; 500 for item in result)\n\n    def test_first_items_identical(self):\n        \"\"\"First 10 items are identical - shouldn't waste slots.\"\"\"\n        items = [{\"id\": \"same\", \"value\": 0}] * 10 + [\n            {\"id\": f\"unique_{i}\", \"value\": i} for i in range(90)\n        ]\n\n        result = smart_crusher.crush(items, max_items=10)\n\n        # Should NOT have multiple identical items\n        ids = [item[\"id\"] for item in result]\n        # At most 1-2 of the identical items, not 3\n        assert ids.count(\"same\") &lt;= 2\n\n    def test_last_items_identical(self):\n        \"\"\"Last 10 items are identical - shouldn't waste slots.\"\"\"\n        items = [{\"id\": f\"unique_{i}\", \"value\": i} for i in range(90)] + [\n            {\"id\": \"same\", \"value\": 100}\n        ] * 10\n\n        result = smart_crusher.crush(items, max_items=10)\n\n        ids = [item[\"id\"] for item in result]\n        assert ids.count(\"same\") &lt;= 2\n\n    def test_relevant_item_in_middle(self):\n        \"\"\"Item matching user query is in middle of array.\"\"\"\n        items = [{\"name\": f\"item_{i}\", \"status\": \"active\"} for i in range(100)]\n        items[42][\"name\"] = \"target_item\"\n        items[42][\"description\"] = \"This is what user asked about\"\n\n        result = smart_crusher.crush(\n            items,\n            max_items=10,\n            query=\"find target_item\"\n        )\n\n        # Query-matched item MUST be preserved\n        assert any(\"target_item\" in item.get(\"name\", \"\") for item in result)\n</code></pre>"},{"location":"plans/dynamic-smart-crusher/#test-category-2-size-adaptation-tests","title":"Test Category 2: Size Adaptation Tests","text":"<pre><code>class TestSizeAdaptation:\n    \"\"\"Test that anchor allocation scales with array size.\"\"\"\n\n    @pytest.mark.parametrize(\"size,expected_min_anchors\", [\n        (20, 3),    # Small array: at least 3 anchors\n        (100, 4),   # Medium array: at least 4 anchors\n        (500, 5),   # Large array: at least 5 anchors\n        (2000, 6),  # Very large: at least 6 anchors\n    ])\n    def test_anchor_count_scales(self, size, expected_min_anchors):\n        \"\"\"Anchor count should increase with array size.\"\"\"\n        items = [{\"id\": i, \"value\": i * 10} for i in range(size)]\n\n        result = smart_crusher.crush(items, max_items=20)\n\n        # Count items from first 10% and last 10%\n        anchor_count = sum(\n            1 for item in result\n            if item[\"id\"] &lt; size * 0.1 or item[\"id\"] &gt; size * 0.9\n        )\n\n        assert anchor_count &gt;= expected_min_anchors\n\n    def test_small_array_high_preservation(self):\n        \"\"\"Small arrays should preserve higher percentage.\"\"\"\n        items = [{\"id\": i} for i in range(15)]\n\n        result = smart_crusher.crush(items, max_items=20)\n\n        # Should preserve most/all of small array\n        assert len(result) &gt;= 10  # At least 66%\n\n    def test_large_array_efficient_sampling(self):\n        \"\"\"Large arrays should sample efficiently.\"\"\"\n        items = [{\"id\": i, \"value\": i} for i in range(5000)]\n\n        result = smart_crusher.crush(items, max_items=20)\n\n        # Should have good distribution across positions\n        positions = [item[\"id\"] for item in result]\n\n        has_front = any(p &lt; 500 for p in positions)\n        has_middle = any(500 &lt; p &lt; 4500 for p in positions)\n        has_back = any(p &gt; 4500 for p in positions)\n\n        assert has_front and has_back\n        # Middle should be represented if array is large enough\n        assert has_middle\n</code></pre>"},{"location":"plans/dynamic-smart-crusher/#test-category-3-pattern-specific-tests","title":"Test Category 3: Pattern-Specific Tests","text":"<pre><code>class TestPatternAwareAnchoring:\n    \"\"\"Test pattern-specific anchor strategies.\"\"\"\n\n    def test_search_results_front_heavy(self):\n        \"\"\"Search results should preserve more from front.\"\"\"\n        items = [\n            {\"title\": f\"Result {i}\", \"score\": 1.0 - (i * 0.01)}\n            for i in range(100)\n        ]\n\n        result = smart_crusher.crush(items, max_items=10)\n\n        # More items should be from front (high scores)\n        front_count = sum(1 for item in result if item[\"score\"] &gt; 0.9)\n        back_count = sum(1 for item in result if item[\"score\"] &lt; 0.1)\n\n        assert front_count &gt; back_count\n\n    def test_logs_back_heavy(self):\n        \"\"\"Logs should preserve more from back (recent).\"\"\"\n        items = [\n            {\"timestamp\": f\"2024-01-{i:02d}\", \"level\": \"INFO\", \"message\": f\"Log {i}\"}\n            for i in range(1, 31)\n        ]\n\n        result = smart_crusher.crush(items, max_items=10)\n\n        # More items should be from back (recent logs)\n        timestamps = [item[\"timestamp\"] for item in result]\n        recent_count = sum(1 for ts in timestamps if int(ts[-2:]) &gt; 20)\n        old_count = sum(1 for ts in timestamps if int(ts[-2:]) &lt; 10)\n\n        assert recent_count &gt;= old_count\n\n    def test_time_series_balanced(self):\n        \"\"\"Time series should have balanced front/back.\"\"\"\n        items = [\n            {\"timestamp\": f\"2024-01-01T{i:02d}:00:00\", \"value\": 100 + i}\n            for i in range(24)\n        ]\n\n        result = smart_crusher.crush(items, max_items=8)\n\n        hours = [int(item[\"timestamp\"][11:13]) for item in result]\n        front_count = sum(1 for h in hours if h &lt; 8)\n        back_count = sum(1 for h in hours if h &gt; 16)\n\n        # Should be roughly balanced\n        assert abs(front_count - back_count) &lt;= 2\n</code></pre>"},{"location":"plans/dynamic-smart-crusher/#test-category-4-query-aware-tests","title":"Test Category 4: Query-Aware Tests","text":"<pre><code>class TestQueryAwareAnchoring:\n    \"\"\"Test query-based anchor adjustment.\"\"\"\n\n    def test_latest_query_shifts_to_back(self):\n        \"\"\"'Latest' in query should preserve more recent items.\"\"\"\n        items = [{\"id\": i, \"created\": f\"2024-01-{i:02d}\"} for i in range(1, 31)]\n\n        result = smart_crusher.crush(\n            items,\n            max_items=8,\n            query=\"Show me the latest entries\"\n        )\n\n        ids = [item[\"id\"] for item in result]\n        recent_count = sum(1 for id in ids if id &gt; 20)\n\n        assert recent_count &gt;= 3  # At least 3 recent items\n\n    def test_first_query_shifts_to_front(self):\n        \"\"\"'First' in query should preserve earlier items.\"\"\"\n        items = [{\"id\": i, \"created\": f\"2024-01-{i:02d}\"} for i in range(1, 31)]\n\n        result = smart_crusher.crush(\n            items,\n            max_items=8,\n            query=\"Show me the first entries\"\n        )\n\n        ids = [item[\"id\"] for item in result]\n        early_count = sum(1 for id in ids if id &lt; 10)\n\n        assert early_count &gt;= 3\n\n    def test_specific_id_query_finds_item(self):\n        \"\"\"Query for specific ID should find it regardless of position.\"\"\"\n        items = [{\"id\": f\"item_{i:04d}\", \"value\": i} for i in range(1000)]\n\n        result = smart_crusher.crush(\n            items,\n            max_items=10,\n            query=\"Find item_0567\"\n        )\n\n        assert any(item[\"id\"] == \"item_0567\" for item in result)\n</code></pre>"},{"location":"plans/dynamic-smart-crusher/#test-category-5-coverage-metrics-tests","title":"Test Category 5: Coverage Metrics Tests","text":"<pre><code>class TestCoverageMetrics:\n    \"\"\"Test that preserved items represent the full distribution.\"\"\"\n\n    def test_value_range_coverage(self):\n        \"\"\"Preserved items should cover the value range.\"\"\"\n        items = [{\"value\": i} for i in range(100)]\n\n        result = smart_crusher.crush(items, max_items=10)\n\n        values = [item[\"value\"] for item in result]\n\n        # Should cover most of the range\n        assert min(values) &lt; 10  # Has low values\n        assert max(values) &gt; 90  # Has high values\n\n        # Should have some middle values too\n        middle_count = sum(1 for v in values if 30 &lt; v &lt; 70)\n        assert middle_count &gt;= 1\n\n    def test_category_coverage(self):\n        \"\"\"Preserved items should represent all categories.\"\"\"\n        items = [\n            {\"category\": cat, \"id\": i}\n            for i, cat in enumerate([\"A\"] * 30 + [\"B\"] * 30 + [\"C\"] * 40)\n        ]\n\n        result = smart_crusher.crush(items, max_items=10)\n\n        categories = set(item[\"category\"] for item in result)\n\n        # Should have at least 2 of 3 categories\n        assert len(categories) &gt;= 2\n\n    def test_temporal_coverage(self):\n        \"\"\"Preserved items should span the time range.\"\"\"\n        items = [\n            {\"timestamp\": f\"2024-{m:02d}-15\", \"event\": f\"event_{i}\"}\n            for i, m in enumerate(range(1, 13))\n        ]\n\n        result = smart_crusher.crush(items, max_items=5)\n\n        months = [int(item[\"timestamp\"][5:7]) for item in result]\n\n        # Should span at least 6 months of the year\n        assert max(months) - min(months) &gt;= 6\n</code></pre>"},{"location":"plans/dynamic-smart-crusher/#test-category-6-retrieval-simulation-tests","title":"Test Category 6: Retrieval Simulation Tests","text":"<pre><code>class TestRetrievalSimulation:\n    \"\"\"Simulate user retrieval patterns to measure effectiveness.\"\"\"\n\n    def test_retrieval_hit_rate_random_queries(self):\n        \"\"\"Measure how often preserved items satisfy random queries.\"\"\"\n        items = [\n            {\"id\": i, \"name\": f\"Item {i}\", \"category\": f\"cat_{i % 5}\"}\n            for i in range(100)\n        ]\n\n        compressed = smart_crusher.crush(items, max_items=15)\n        compressed_ids = {item[\"id\"] for item in compressed}\n\n        # Simulate 100 random \"queries\" (random item lookups)\n        hits = 0\n        for _ in range(100):\n            target_id = random.randint(0, 99)\n            if target_id in compressed_ids:\n                hits += 1\n\n        # Should hit at least 15% (we keep 15 of 100)\n        assert hits &gt;= 15\n\n    def test_retrieval_hit_rate_weighted_queries(self):\n        \"\"\"Measure hits for queries weighted toward common patterns.\"\"\"\n        items = [{\"id\": i, \"value\": i * 10} for i in range(100)]\n\n        compressed = smart_crusher.crush(items, max_items=15)\n        compressed_ids = {item[\"id\"] for item in compressed}\n\n        # Weight queries toward front (30%), back (30%), anomalies (40%)\n        hits = 0\n        queries = (\n            list(range(10)) * 3 +  # Front queries\n            list(range(90, 100)) * 3 +  # Back queries\n            [50] * 4  # Middle anomaly queries\n        )\n\n        for target_id in queries:\n            if target_id in compressed_ids:\n                hits += 1\n\n        # Should hit more often than random due to anchor strategy\n        assert hits &gt;= 20  # At least 20% hit rate\n</code></pre>"},{"location":"plans/dynamic-smart-crusher/#implementation-phases","title":"Implementation Phases","text":""},{"location":"plans/dynamic-smart-crusher/#phase-1-refactor-anchor-logic-foundation","title":"Phase 1: Refactor Anchor Logic (Foundation)","text":"<ol> <li>Extract anchor selection into <code>AnchorSelector</code> class</li> <li>Make slot counts configurable via <code>AnchorConfig</code></li> <li>Add size-proportional allocation</li> <li>Maintain backward compatibility with current defaults</li> </ol>"},{"location":"plans/dynamic-smart-crusher/#phase-2-pattern-aware-anchoring","title":"Phase 2: Pattern-Aware Anchoring","text":"<ol> <li>Map <code>DataPattern</code> to <code>AnchorStrategy</code></li> <li>Implement front-heavy, back-heavy, balanced, middle-aware strategies</li> <li>Add pattern-specific anchor weight configs</li> </ol>"},{"location":"plans/dynamic-smart-crusher/#phase-3-information-density-selection","title":"Phase 3: Information-Density Selection","text":"<ol> <li>Add <code>calculate_information_score()</code> for items</li> <li>Select from candidate region instead of fixed positions</li> <li>Deduplicate identical items across regions</li> </ol>"},{"location":"plans/dynamic-smart-crusher/#phase-4-query-aware-adjustment","title":"Phase 4: Query-Aware Adjustment","text":"<ol> <li>Parse query for position intent keywords</li> <li>Adjust anchor weights dynamically</li> <li>Add query-position relevance scoring</li> </ol>"},{"location":"plans/dynamic-smart-crusher/#phase-5-toin-position-learning","title":"Phase 5: TOIN Position Learning","text":"<ol> <li>Track retrieval positions in TOIN</li> <li>Learn per-tool position importance</li> <li>Use learned weights to adjust anchor strategy</li> </ol>"},{"location":"plans/dynamic-smart-crusher/#phase-6-comprehensive-testing","title":"Phase 6: Comprehensive Testing","text":"<ol> <li>Implement all adversarial tests</li> <li>Add coverage metric tests</li> <li>Add retrieval simulation tests</li> <li>Performance benchmarks</li> </ol>"},{"location":"plans/dynamic-smart-crusher/#configuration-schema","title":"Configuration Schema","text":"<pre><code>@dataclass\nclass AnchorConfig:\n    \"\"\"Configuration for dynamic anchor allocation.\"\"\"\n\n    # Base anchor budget as percentage of max_items\n    anchor_budget_pct: float = 0.20  # 20% of slots for position anchors\n\n    # Minimum and maximum anchor slots\n    min_anchor_slots: int = 3\n    max_anchor_slots: int = 10\n\n    # Default distribution (overridden by pattern)\n    default_front_weight: float = 0.5\n    default_back_weight: float = 0.5\n    default_middle_weight: float = 0.0\n\n    # Pattern-specific overrides\n    search_front_weight: float = 0.8\n    logs_back_weight: float = 0.8\n    time_series_balance: float = 0.5\n\n    # Query keyword detection\n    recency_keywords: list[str] = field(default_factory=lambda: [\n        \"latest\", \"recent\", \"last\", \"newest\", \"current\"\n    ])\n    historical_keywords: list[str] = field(default_factory=lambda: [\n        \"first\", \"oldest\", \"earliest\", \"original\", \"initial\"\n    ])\n\n    # Information density selection\n    use_information_density: bool = True\n    candidate_multiplier: int = 3  # Consider 3x candidates per slot\n\n    # TOIN learning\n    use_learned_positions: bool = True\n    min_samples_for_learning: int = 10\n</code></pre>"},{"location":"plans/dynamic-smart-crusher/#success-metrics","title":"Success Metrics","text":"<ol> <li>Retrieval Coverage: % of user retrievals that hit preserved items (target: &gt;80%)</li> <li>Information Density: Unique information per preserved slot (target: no duplicate items)</li> <li>Distribution Coverage: Preserved items span full value/time/category ranges</li> <li>Adversarial Robustness: All adversarial tests pass</li> <li>Backward Compatibility: Existing tests still pass</li> <li>Performance: &lt;5ms additional latency for anchor selection</li> </ol>"},{"location":"plans/dynamic-smart-crusher/#risks-and-mitigations","title":"Risks and Mitigations","text":"Risk Mitigation Information density calculation is expensive Cache scores, limit candidate pool Query keyword detection is brittle Use as soft signal, not hard rule TOIN learning needs cold start Fall back to pattern-based defaults Breaking existing behavior Feature flag, A/B testing Middle sampling misses important items Always include anomalies/errors regardless"},{"location":"plans/dynamic-smart-crusher/#next-steps","title":"Next Steps","text":"<ol> <li>Review and approve this plan</li> <li>Write failing tests first (TDD approach)</li> <li>Implement Phase 1 (refactor foundation)</li> <li>Iterate through phases with test validation</li> <li>Benchmark against current implementation</li> <li>A/B test in production with telemetry</li> </ol>"}]}